{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   id      conversation_id           created_at  \\\n0   1  1435703745304612870  2021-09-08 20:37:01   \n1   2  1435664738353098756  2021-09-08 18:02:01   \n2   3  1435663883595706370  2021-09-08 17:58:37   \n\n                                                text  author_id  \\\n0  RT @OregonOEM: ðŸš©ðŸŒ©ðŸ”¥ Red Flag and #FireWeather w...   14838508   \n1  It's #NationalPreparednessMonth. Help ensure t...   14838508   \n2  RT @OregonOEM: Oregon is still recovering from...   14838508   \n\n   in_reply_to_user_id  \n0                  NaN  \n1                  NaN  \n2                  NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>conversation_id</th>\n      <th>created_at</th>\n      <th>text</th>\n      <th>author_id</th>\n      <th>in_reply_to_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1435703745304612870</td>\n      <td>2021-09-08 20:37:01</td>\n      <td>RT @OregonOEM: ðŸš©ðŸŒ©ðŸ”¥ Red Flag and #FireWeather w...</td>\n      <td>14838508</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1435664738353098756</td>\n      <td>2021-09-08 18:02:01</td>\n      <td>It's #NationalPreparednessMonth. Help ensure t...</td>\n      <td>14838508</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1435663883595706370</td>\n      <td>2021-09-08 17:58:37</td>\n      <td>RT @OregonOEM: Oregon is still recovering from...</td>\n      <td>14838508</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Create your connection.\n",
    "from twitter.nlp_util import process_tweet\n",
    "\n",
    "cnx = sqlite3.connect('db.sqlite3')\n",
    "\n",
    "df = pd.read_sql_query(\n",
    "    \"SELECT id, conversation_id, created_at, text, author_id,in_reply_to_user_id FROM delab_timeline WHERE lang='en'\",\n",
    "    cnx)\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are importing the libraries.\n",
    "\n",
    "The first idea was to follow lda https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
    "\n",
    "but turns out it is to little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Showing some descriptive statistics for the author corpus downloaded from twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "69"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author_id.nunique() # the number of unique authors in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "author_id\n16558158               478\n18616003               946\n26998226               469\n382814447              446\n1005470991668084736    445\n1106611172462219265    491\n1162371171805011968    447\n1239172010363826183    427\n1292908140975943681    414\n1402252385427222528    414\n1403930956428460035    441\ndtype: int64"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df[[\"author_id\", \"text\", \"id\"]]\n",
    "#df_reduced = df_reduced.groupby('author_id')\n",
    "# df_reduced.count()\n",
    "\n",
    "df_reshaped = df_reduced.pivot(index=\"id\",columns=\"author_id\", values=\"text\")\n",
    "mask = 400 > df_reshaped.nunique()\n",
    "mask = mask[mask == True]\n",
    "df_reshaped.drop(columns=mask.index,inplace=True)\n",
    "df_reshaped.nunique() # the number of tweets of the authors that have more then 400 tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following takes the pandas dataframe and converts it to a dictionary with the author ids as keys and the twitter\n",
    "corpora as values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "id\n8247                                   @ncreen_same ouch!\n8248    That last tweet got some responses from spambo...\n8249    'virtually' virtually means: real\\n\\nSo it sho...\n8250                               Sounds great on paper!\n8251    Any .com.au registrar recommendations? So far ...\n                              ...                        \n8742    RT @paydirtapp: Check out our Free Invoice Cre...\n8743    @taitems @mmilo yeah nice site, suggestion: ht...\n8744    RT @MichaelFHansen: Zendesk eyes Southeast Asi...\n8745    While LinkedIn has been changing drastically o...\n8746    Another LinkedIn email failâ€¦ now they want me ...\nName: 16558158, Length: 478, dtype: object"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_reshaped.shape\n",
    "author_corpora = df_reshaped.to_dict(orient=\"series\")\n",
    "for author_id, tweets in author_corpora.items():\n",
    "    author_corpora[author_id] = tweets.dropna()\n",
    "\n",
    "example_corpus = author_corpora[next(iter(author_corpora))]\n",
    "example_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are ready to analyze the corpora of the authors tweets. We do one example with the first corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 9\n",
      "Number of documents: 478\n",
      "Average topic coherence: -5.7811.\n",
      "[([(0.3684394, 'like'),\n",
      "   (0.31433037, 'know'),\n",
      "   (0.28787804, 'use'),\n",
      "   (0.0053321696, 'look'),\n",
      "   (0.0049920008, 'new'),\n",
      "   (0.0049395612, 'get'),\n",
      "   (0.0047678538, 'work'),\n",
      "   (0.004673165, 'â€¦'),\n",
      "   (0.0046474542, '...')],\n",
      "  -5.77066551539591),\n",
      " ([(0.2638249, 'â€¦'),\n",
      "   (0.2637392, 'work'),\n",
      "   (0.24274658, 'get'),\n",
      "   (0.21125293, 'new'),\n",
      "   (0.0038828384, 'know'),\n",
      "   (0.0037253946, 'use'),\n",
      "   (0.0036451528, 'like'),\n",
      "   (0.0036288158, 'look'),\n",
      "   (0.00355417, '...')],\n",
      "  -5.7780273587207835),\n",
      " ([(0.52524346, '...'),\n",
      "   (0.42429098, 'look'),\n",
      "   (0.0077454015, 'like'),\n",
      "   (0.0073940223, 'new'),\n",
      "   (0.0073653413, 'know'),\n",
      "   (0.007064507, 'get'),\n",
      "   (0.0070528523, 'work'),\n",
      "   (0.0070286235, 'â€¦'),\n",
      "   (0.006814815, 'use')],\n",
      "  -5.794644230029977)]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for tweet in example_corpus:\n",
    "    docs.append(process_tweet(tweet))\n",
    "\n",
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "\n",
    "###############################################################################\n",
    "# We remove rare words and common words based on their *document frequency*.\n",
    "# Below we remove words that appear in less than 20 documents or in more than\n",
    "# 50% of the documents. Consider trying to remove words only based on their\n",
    "# frequency, or maybe combining that with this approach.\n",
    "#\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "###############################################################################\n",
    "# Finally, we transform the documents to a vectorized form. We simply compute\n",
    "# the frequency of each word, including the bigrams.\n",
    "#\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "###############################################################################\n",
    "# Let's see how many tokens and documents we have to train on.\n",
    "#\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 3\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# Note that we use the \"Umass\" topic coherence measure here (see\n",
    "# :py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
    "# obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
    "# accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As this resulted in disappointed non-nouns as topics the alternative would be to use the named entities directly\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[\"Thinking with Type]',\n",
      " '[#Melbourne #, 2013]',\n",
      " '[#]',\n",
      " '[#susanalbumparty #craftbeer #, melbourne]',\n",
      " '[#wuws #]',\n",
      " '[0, http://t.co/K8cPzW6qPk]',\n",
      " '[1 to 10]',\n",
      " '[1.30, 1.26]',\n",
      " '[100%, first]',\n",
      " '[100, a few seconds]',\n",
      " '[1000th]',\n",
      " '[10B+]',\n",
      " '[11000 hours]',\n",
      " '[140]',\n",
      " '[16 days]',\n",
      " '[1]',\n",
      " '[1st]',\n",
      " '[20, first]',\n",
      " '[234272]',\n",
      " '[2]',\n",
      " '[3, Tiiim]',\n",
      " '[35%]',\n",
      " '[3]',\n",
      " '[3rd]',\n",
      " '[4]',\n",
      " '[7 minutes]',\n",
      " '[700m]',\n",
      " '[7487944]',\n",
      " '[75]',\n",
      " '[8]',\n",
      " '[@ArondeParon, Redmine]',\n",
      " '[@BrewSmithAu]',\n",
      " '[@BugHerd]',\n",
      " '[@FLTBTG http://t.co/y9LJeAehCV]',\n",
      " '[@GVRV &lt;/genuinelywondering&gt]',\n",
      " '[@GVRV]',\n",
      " '[@KeyPayApp]',\n",
      " '[@Moocar]',\n",
      " '[@Panviva]',\n",
      " '[@Rackspace]',\n",
      " '[@TomPisel]',\n",
      " '[@UXMastery]',\n",
      " '[@Uber_Sydney, today]',\n",
      " '[@alandownie @bigyahu, 10%, the last month]',\n",
      " '[@alandownie @ivanmelvin]',\n",
      " '[@alandownie @mmilo]',\n",
      " '[@alandownie @ncreen_same @therealdevgeeks]',\n",
      " '[@alandownie @ncreen_same]',\n",
      " '[@alandownie @rioter @ncreen_same]',\n",
      " '[@alandownie @rioter]',\n",
      " '[@alandownie @skwashd]',\n",
      " '[@alandownie @thuydownie, Alan]',\n",
      " '[@alandownie @vbrendel]',\n",
      " '[@alandownie, alan]',\n",
      " '[@alandownie, half, one day]',\n",
      " '[@alandownie]',\n",
      " '[@amir_nissen, Melbourne, http://t.co/q7sIHGgx]',\n",
      " '[@annejan88]',\n",
      " '[@bengrubb]',\n",
      " '[@bigyahu, http://t.co/bB7Uzzqo]',\n",
      " '[@bigyahu]',\n",
      " '[@bugcrowd, $$ http://t.co/drfPmCCD]',\n",
      " '[@bugherd, Pearlstar]',\n",
      " '[@bugherd, http://t.co/v5aIrquJ]',\n",
      " '[@bugherd]',\n",
      " '[@carsalescomau, @thissink: Quality Assurance Engineer]',\n",
      " '[@caseyjohnellis, NFP]',\n",
      " '[@caseyjohnellis]',\n",
      " '[@darwindavem]',\n",
      " '[@fuzebox]',\n",
      " '[@garyvee]',\n",
      " '[@googlemaps do Water View]',\n",
      " '[@gpdawson @runkeeper, SOS]',\n",
      " '[@hughker, @bugherd]',\n",
      " '[@inspiredworlds, Loic Le Meur]',\n",
      " '[@jakeisonline]',\n",
      " '[@jamesarosen @therealdevgeeks, Melbourne]',\n",
      " '[@jamesarosen]',\n",
      " '[@jessemcnelis]',\n",
      " '[@katsambarcus @mmilo]',\n",
      " '[@katsambarcus, Los Angeles]',\n",
      " '[@katsambarcus]',\n",
      " '[@kealey]',\n",
      " '[@lomaxx]',\n",
      " '[@longzheng]',\n",
      " '[@lukcha]',\n",
      " '[@mipearson]',\n",
      " '[@mmilo, @alandownie]',\n",
      " '[@mmilo]',\n",
      " '[@nedwin @tweakyinc]',\n",
      " '[@nikiscevak]',\n",
      " '[@omnyapp]',\n",
      " '[@paydirtapp, 500, 1000, https://t.co/0fhM7pTg]',\n",
      " '[@paydirtapp]',\n",
      " '[@rioter @alandownie @ncreen_same]',\n",
      " '[@rioter @alandownie]',\n",
      " '[@rioter, @keithpitt, @buzzswarbrick, @bensmithett]',\n",
      " '[@rioter]',\n",
      " '[@siganakis @bugherd]',\n",
      " '[@theheraldsun]',\n",
      " '[@thehubhu, @medhved, Araknode]',\n",
      " '[@thehubhu, http://t.co/AoLGmKn9j7]',\n",
      " '[@tristangamilis @alandownie]',\n",
      " '[@tristangamilis, @nbfm]',\n",
      " '[@untappd]',\n",
      " '[@vbrendel @ncreen_same, Alkido]',\n",
      " '[@veroapp]',\n",
      " '[@zujiaustralia, 28th Mar 2013, 12-18 weeks]',\n",
      " '[ALLCAPS]',\n",
      " '[API]',\n",
      " '[AirPlay]',\n",
      " '[Alan]',\n",
      " '[Alfa Beer, Athenian Brewery, Heineken Group]',\n",
      " '[Almost 2 years]',\n",
      " '[Anheuser-Busch]',\n",
      " '[Apple, iPad]',\n",
      " '[Australia, @thehubhu, http://t.co/y2LBrd5YTj]',\n",
      " '[Australia, iOS 6, October]',\n",
      " '[Australia]',\n",
      " '[Australians, Australian, Europe]',\n",
      " \"[Beer Connoisseur', @untappd, http://t.co/usZWTWV06w]\",\n",
      " '[BitBucket, GitHub]',\n",
      " \"[Bottle Share', @untappd]\",\n",
      " '[BrewSmith]',\n",
      " '[Budapest, a few months, Holland]',\n",
      " '[Buddy &amp]',\n",
      " '[BugHerd Classic]',\n",
      " '[BugHerd https://t.co/9IK6XRd5Qw]',\n",
      " '[BugHerd, 500,000]',\n",
      " '[BugHerd, Game Changer]',\n",
      " '[BugHerd, Rackspace, Amazon]',\n",
      " '[Camberwell Station]',\n",
      " '[Chicago, a weekend, NY]',\n",
      " '[Citroen]',\n",
      " '[Clearing Cache, Rebooting Windows]',\n",
      " '[Daily, US]',\n",
      " '[Dogbolter, @matildabaybeer, #]',\n",
      " '[England]',\n",
      " '[English]',\n",
      " '[Europe]',\n",
      " '[Facebook, Zendesk]',\n",
      " '[Flash, @alandownie]',\n",
      " '[Flightfox, @veroapp]',\n",
      " '[Flightfox, https://t.co/3jwA9VLd, @flightfox]',\n",
      " '[Game Changer, the Year for @netmag Awards]',\n",
      " '[Gibraltar]',\n",
      " '[Google, Facebook, Twitter, Yahoo &, Microsoft, 10, AngelList]',\n",
      " '[Google, Facebook]',\n",
      " '[Hartley]',\n",
      " '[HeadStartAus]',\n",
      " '[HeadStart]',\n",
      " '[Heineken International]',\n",
      " '[Holden]',\n",
      " '[I Knew You Were Going To Say That, @lukcha]',\n",
      " '[IPA, 4]',\n",
      " '[IP]',\n",
      " '[Independence Day, 2013, @untappd]',\n",
      " '[Jared M. Spool, first]',\n",
      " '[July, Steptember, Movember]',\n",
      " '[Kickstarter]',\n",
      " \"[Lager Jack', @untappd]\",\n",
      " '[Lager Jack, 2, @untappd]',\n",
      " '[Lager Jack, 3, @untappd]',\n",
      " '[Lance Armstrong]',\n",
      " '[LeanStartup, Melbourne]',\n",
      " '[LinkedIn Fails, 2]',\n",
      " '[LinkedIn, 6 years]',\n",
      " '[LinkedIn, the past 5 years]',\n",
      " '[LinkedIn]',\n",
      " '[Macbook Pro]',\n",
      " '[Married With Children]',\n",
      " '[Melb]',\n",
      " '[Melbourne Accelerator Program]',\n",
      " '[Melbourne Accelerator]',\n",
      " '[Melbourne, 3121, 3122, 3123, 3124]',\n",
      " '[Melbourne, the Melbourne Cup]',\n",
      " '[Melbourne]',\n",
      " '[Mexx, Australia]',\n",
      " '[Mooroolbark]',\n",
      " '[Mythos Hellenic Lager, Mythos Brewery, Carlsberg Group]',\n",
      " '[NBN, Australia]',\n",
      " '[NR]',\n",
      " '[New Year, BugHerd, Australia]',\n",
      " '[New Year]',\n",
      " '[New iTunes]',\n",
      " '[NoSQL, NoArray]',\n",
      " '[OSX]',\n",
      " '[Obama, http://t.co/wmC0xu5X]',\n",
      " '[One Million]',\n",
      " '[One, 24 hours]',\n",
      " '[Paydirt, @paydirtapp]',\n",
      " '[Perth, Melbourne]',\n",
      " '[Peter Mac, @charityvideoau]',\n",
      " '[Phil Schiller, iPad Mini]',\n",
      " '[Postbox &amp, Sparrow]',\n",
      " '[RT @37signals, Basecamp]',\n",
      " '[RT @Araknode:]',\n",
      " '[RT @BrewSmithAu:, 4, IPA, Sunday]',\n",
      " '[RT @BrewSmithAu:]',\n",
      " '[RT @HeadStartAus, tomorrow]',\n",
      " '[RT @HeadStartAus:]',\n",
      " '[RT @HeadStartAus]',\n",
      " '[RT @Melair]',\n",
      " '[RT @MichaelFHansen, Zendesk, Southeast Asian]',\n",
      " '[RT @Oatmeal, Disney, Lucasfilm]',\n",
      " '[RT @RhettLuciani]',\n",
      " '[RT @Scobleizer]',\n",
      " '[RT @UXMastery, #ux #uxdesign]',\n",
      " '[RT @Uber_Melbourne]',\n",
      " '[RT @alandownie]',\n",
      " '[RT @amir_nissen, 2, tomorrow]',\n",
      " '[RT @amir_nissen, Melbourne]',\n",
      " '[RT @amir_nissen]',\n",
      " '[RT @bigyahu, @gartner, @bugherd]',\n",
      " '[RT @bigyahu]',\n",
      " '[RT @bugherd, tomorrow, Budapest, @thehubhu, tomorrow morning]',\n",
      " '[RT @bugherd]',\n",
      " '[RT @caseyjohnellis, @bugcrowd, 10k]',\n",
      " '[RT @damonagle]',\n",
      " '[RT @gbissett]',\n",
      " '[RT @longzheng]',\n",
      " '[RT @medhved]',\n",
      " '[RT @melbjs, 1 week]',\n",
      " '[RT @merryuxmas:, 50]',\n",
      " '[RT @paydirtapp:]',\n",
      " '[RT @paydirtapp]',\n",
      " '[RT @rubygems]',\n",
      " '[RT @thissink]',\n",
      " '[RT @uxmastery]',\n",
      " '[RT @veroapp]',\n",
      " '[RT @webdirections]',\n",
      " '[RT, Charity, Thursday]',\n",
      " '[RT, tonight, http://t.co/weIZAiJ1, 7pm]',\n",
      " '[RT]',\n",
      " '[Rails]',\n",
      " '[RequestBin, LocalTunnel]',\n",
      " '[Rioli]',\n",
      " '[SAB Miller]',\n",
      " '[SQL]',\n",
      " '[Samsung Galaxy, @GoPollGo]',\n",
      " '[Sapporo Breweries]',\n",
      " '[Shopify, Bigcommerce]',\n",
      " '[Shrimp]',\n",
      " '[Social Media]',\n",
      " '[Sony, PS4, PS4]',\n",
      " '[Soproni Srgyr]',\n",
      " '[Sri Lankan]',\n",
      " '[Street View]',\n",
      " '[SupportPoint VX]',\n",
      " '[The House of Commons, 4-6]',\n",
      " '[This day, BugHerd]',\n",
      " '[Timisoreana, SAB Miller]',\n",
      " '[Today]',\n",
      " '[Tomorrow, @bugherd]',\n",
      " '[Twitter iOS App]',\n",
      " '[Twitter, Australia]',\n",
      " '[Twitter, Mac]',\n",
      " '[Twitter]',\n",
      " '[ULR]',\n",
      " '[Uber, yesterday]',\n",
      " '[Ubers, Melbourne]',\n",
      " '[Uncategorized]',\n",
      " '[Virtual Reality]',\n",
      " '[WebEx, years]',\n",
      " '[World, First, WobbleWorks LLC.]',\n",
      " '[Years]',\n",
      " '[Zapier BugHerd]',\n",
      " '[Zapier]',\n",
      " '[Zen]',\n",
      " '[Zendesk]',\n",
      " '[a few seconds]',\n",
      " '[afternoon, 25%, 0%]',\n",
      " '[alan]',\n",
      " '[americans]',\n",
      " '[another 6 years]',\n",
      " '[at least a week]',\n",
      " '[bush, @Zendesk]',\n",
      " '[epicfail]',\n",
      " '[first]',\n",
      " '[five]',\n",
      " '[french]',\n",
      " '[http://t.co/19ZJHeaSqx]',\n",
      " '[http://t.co/2rSLN6I4]',\n",
      " '[http://t.co/56BRLKDT]',\n",
      " '[http://t.co/AAl2behHoQ -, Australia]',\n",
      " '[http://t.co/FGUyTtD5]',\n",
      " '[http://t.co/IjcTTtPOKX]',\n",
      " '[http://t.co/UbLUTw27]',\n",
      " '[http://t.co/Xl0Vt]',\n",
      " '[http://t.co/YCtErRux]',\n",
      " '[http://t.co/qr4PpqcE]',\n",
      " '[http://t.co/qsvud4CHF6]',\n",
      " '[https://t.co/SSloY8548U]',\n",
      " '[https://t.co/eAylnAi2]',\n",
      " '[iOS]',\n",
      " '[last holiday, Heineken, @heineken]',\n",
      " '[last minute]',\n",
      " '[last monday]',\n",
      " '[mojito]',\n",
      " '[nxt week, Google Analytics, @TweakyInc]',\n",
      " '[one]',\n",
      " '[steve]',\n",
      " '[susanalbumparty]',\n",
      " '[the day]',\n",
      " '[the half million]',\n",
      " '[the last minute]',\n",
      " '[the next week]',\n",
      " '[this morning]',\n",
      " '[this week]',\n",
      " '[this weekend, Melbourne]',\n",
      " '[this year]',\n",
      " '[today]',\n",
      " '[tomorrow, @bugherd at \"Wake]',\n",
      " '[tonight]',\n",
      " '[tshirt]',\n",
      " '[weeks ago]',\n",
      " '[zero]'}\n"
     ]
    }
   ],
   "source": [
    "found_entities = set()\n",
    "\n",
    "for tweet in example_corpus:\n",
    "    doc = nlp(tweet)\n",
    "    sentences = list(doc.sents)\n",
    "    for sentence in sentences:\n",
    "        if sentence.ents:\n",
    "            found_entities.add(str(sentence.ents))\n",
    "\n",
    "pprint(found_entities)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% alternatively preprocess including NER\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}