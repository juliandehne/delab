{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Topic Model in Python: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "In the previous article, I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using gensim implementation.\n",
    "\n",
    "Pursuing on that understanding, in this article, I'll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.\n",
    "\n",
    "### Why evaluate topic models?\n",
    "\n",
    "![img](https://tinyurl.com/y3xznjwq)\n",
    "\n",
    "We know probabilistic topic models, such as LDA, are popular tools for analysis of the text, providing both a predictive and latent topic representation of the corpus. There is a longstanding assumption that the latent space discovered by these models is meaningful and useful, and evaluating such assumptions is challenging due to its unsupervised training process. There is a no-gold standard list of topics to compare against every corpus.\n",
    "\n",
    "However, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods and to do so, we require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if \"the correct thing\" has been learned about the corpus, we use implicit knowledge and \"eyeballing\" approaches. Ideally, we'd like to capture this information in a single metric that can be maximized, and compared. Let's take a look at roughly what approaches are commonly used for the evaluation:\n",
    "\n",
    "**Eye Balling Models**\n",
    "- Top N words\n",
    "- Topics / Documents\n",
    "\n",
    "**Intrinsic Evaluation Metrics**\n",
    "- Capturing model semantics\n",
    "- Topics interpretability\n",
    "\n",
    "**Human Judgements**\n",
    "- What is a topic\n",
    "\n",
    "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
    "- Is model good at performing predefined tasks, such as classification\n",
    "\n",
    "Natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to cleanse ambiguity reduces the language to an unnatural form. Nevertheless, in this article, we'll explore more about topic coherence, and how we can use it to quantitatively justify the model selection.\n",
    "\n",
    "### What is Topic Coherence?\n",
    "\n",
    "Perplexity is often used as an example of an intrinsic evaluation measure. It comes from the language modeling community and aims to capture how surprised a model is of new data it has not seen before. It is measured as the normalized log-likelihood of a held-out test set.\n",
    "\n",
    "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
    "\n",
    "However, past research has shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated. And that served as a motivation for more work trying to model the human judgment, and thus `Topic Coherence`.\n",
    "\n",
    "The topic coherence concept combines a number of papers into one framework that allows evaluating the coherence of topics inferred by a topic model. But,\n",
    "\n",
    "#### What is topic coherence?\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
    "\n",
    "#### What is coherence?\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is \"the game is a team sport\", \"the game is played with a ball\", \"the game demands great physical efforts\"\n",
    "\n",
    "### Coherence Measures\n",
    "\n",
    "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
    "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
    "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
    "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
    "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
    "6. `C_a` is baseed on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "1. Loading Data\n",
    "2. Data Cleaning\n",
    "3. Phrase Modeling: Bi-grams and Tri-grams\n",
    "4. Data Transformation: Corpus and Dictionary\n",
    "5. Base Model\n",
    "6. Hyper-parameter Tuning\n",
    "7. Final model\n",
    "8. Visualize Results\n",
    "\n",
    "** **\n",
    "\n",
    "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
    "\n",
    "Let’s start by looking at the content of the file\n",
    "\n",
    "** **\n",
    "#### Step 1: Loading Data\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "# Read data into papers\n",
    "papers = pd.read_csv('./data/NIPS Papers/papers.csv')\n",
    "\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 2: Data Cleaning\n",
    "** **\n",
    "\n",
    "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>Interpolating Convex and Non-Convex Tensor\\nDe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>Learning Multiple Models via Regularized Weigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>Towards social robots: Automatic evaluation of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6261</th>\n",
       "      <td>The Power of Amnesia\\n\\nDana Ron\\nYoram Singer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>Matrix Completion from Noisy Entries\\n\\nRaghun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             paper_text\n",
       "5328  Interpolating Convex and Non-Convex Tensor\\nDe...\n",
       "4528  Learning Multiple Models via Regularized Weigh...\n",
       "1543  Towards social robots: Automatic evaluation of...\n",
       "6261  The Power of Amnesia\\n\\nDana Ron\\nYoram Singer...\n",
       "3064  Matrix Completion from Noisy Entries\\n\\nRaghun..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['id', 'title', 'abstract', \n",
    "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
    "\n",
    "# sample only 100 papers\n",
    "papers = papers.sample(100)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation/lower casing\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5328    interpolating convex and non-convex tensor\\nde...\n",
       "4528    learning multiple models via regularized weigh...\n",
       "1543    towards social robots: automatic evaluation of...\n",
       "6261    the power of amnesia\\n\\ndana ron\\nyoram singer...\n",
       "3064    matrix completion from noisy entries\\n\\nraghun...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize words and further clean-up text\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interpolating', 'convex', 'and', 'non', 'convex', 'tensor', 'decompositions', 'via', 'the', 'subspace', 'norm', 'ryota', 'tomioka', 'toyota', 'technological', 'institute', 'at', 'chicago', 'tomioka', 'tticedu', 'qinqing', 'zheng', 'university', 'of', 'chicago', 'qinqing', 'csuchicagoedu', 'abstract', 'we', 'consider']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
    "** **\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
    "\n",
    "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shashank/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the functions in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 526 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: setuptools in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/shashank/Documents/python/git/mediumposts/topic_model/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/Users/shashank/Documents/python/git/mediumposts/topic_model/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interpolate', 'decomposition', 'tticedu', 'qinqe', 'csuchicagoedu', 'abstract', 'consider', 'problem', 'recover', 'previous', 'work', 'show', 'recovery', 'guarantee', 'signal', 'noise', 'ratio', 'recover', 'tensor', 'size', 'recursive', 'unfold', 'paper', 'first', 'improve', 'bind', 'much', 'simple', 'approach', 'careful']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 4: Data transformation: Corpus and Dictionary\n",
    "** **\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 4), (2, 2), (3, 1), (4, 2), (5, 2), (6, 2), (7, 1), (8, 1), (9, 1), (10, 15), (11, 1), (12, 1), (13, 1), (14, 4), (15, 3), (16, 1), (17, 1), (18, 5), (19, 11), (20, 1), (21, 1), (22, 3), (23, 1), (24, 2), (25, 1), (26, 1), (27, 1), (28, 4), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 5: Base Model \n",
    "** **\n",
    "\n",
    "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
    "\n",
    "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
    "\n",
    "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.023*\"model\" + 0.015*\"image\" + 0.011*\"use\" + 0.011*\"learn\" + '\n",
      "  '0.010*\"training\" + 0.009*\"set\" + 0.008*\"figure\" + 0.007*\"segment\" + '\n",
      "  '0.007*\"parameter\" + 0.007*\"local\"'),\n",
      " (1,\n",
      "  '0.019*\"policy\" + 0.016*\"value\" + 0.016*\"action\" + 0.015*\"feedback\" + '\n",
      "  '0.014*\"state\" + 0.012*\"model\" + 0.011*\"learn\" + 0.011*\"use\" + 0.010*\"noise\" '\n",
      "  '+ 0.010*\"reward\"'),\n",
      " (2,\n",
      "  '0.012*\"worker\" + 0.009*\"question\" + 0.009*\"expression\" + 0.008*\"sum\" + '\n",
      "  '0.007*\"use\" + 0.007*\"network\" + 0.007*\"user\" + 0.006*\"result\" + '\n",
      "  '0.006*\"mechanism\" + 0.006*\"learn\"'),\n",
      " (3,\n",
      "  '0.017*\"model\" + 0.013*\"network\" + 0.008*\"evidence\" + 0.008*\"input\" + '\n",
      "  '0.007*\"variable\" + 0.006*\"show\" + 0.006*\"relation\" + 0.006*\"inference\" + '\n",
      "  '0.005*\"feature\" + 0.005*\"set\"'),\n",
      " (4,\n",
      "  '0.013*\"function\" + 0.011*\"use\" + 0.008*\"value\" + 0.008*\"point\" + '\n",
      "  '0.007*\"method\" + 0.007*\"algorithm\" + 0.007*\"set\" + 0.006*\"give\" + '\n",
      "  '0.006*\"problem\" + 0.006*\"result\"'),\n",
      " (5,\n",
      "  '0.018*\"model\" + 0.009*\"use\" + 0.007*\"image\" + 0.006*\"result\" + '\n",
      "  '0.006*\"datum\" + 0.005*\"distribution\" + 0.005*\"show\" + 0.005*\"figure\" + '\n",
      "  '0.005*\"time\" + 0.005*\"set\"'),\n",
      " (6,\n",
      "  '0.017*\"model\" + 0.011*\"problem\" + 0.009*\"method\" + 0.008*\"set\" + '\n",
      "  '0.007*\"use\" + 0.007*\"matrix\" + 0.006*\"function\" + 0.006*\"also\" + '\n",
      "  '0.005*\"datum\" + 0.005*\"show\"'),\n",
      " (7,\n",
      "  '0.019*\"model\" + 0.011*\"learn\" + 0.011*\"use\" + 0.009*\"task\" + '\n",
      "  '0.007*\"problem\" + 0.007*\"datum\" + 0.007*\"input\" + 0.007*\"function\" + '\n",
      "  '0.007*\"set\" + 0.006*\"result\"'),\n",
      " (8,\n",
      "  '0.013*\"network\" + 0.012*\"variable\" + 0.010*\"unit\" + 0.010*\"number\" + '\n",
      "  '0.009*\"input\" + 0.009*\"function\" + 0.009*\"set\" + 0.009*\"matrix\" + '\n",
      "  '0.008*\"product\" + 0.007*\"use\"'),\n",
      " (9,\n",
      "  '0.013*\"image\" + 0.013*\"model\" + 0.009*\"cell\" + 0.008*\"figure\" + '\n",
      "  '0.007*\"block\" + 0.006*\"noise\" + 0.006*\"time\" + 0.006*\"use\" + '\n",
      "  '0.005*\"parameter\" + 0.005*\"function\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Let's calculate the baseline coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.28484369372941065\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 6: Hyperparameter tuning\n",
    "** **\n",
    "First, let's differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
    "\n",
    "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters: \n",
    "- Number of Topics (K)\n",
    "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "- Dirichlet hyperparameter beta: Word-Topic Density\n",
    "\n",
    "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 7: Final Model\n",
    "** **\n",
    "\n",
    "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"model\" + 0.008*\"local\" + 0.006*\"network\" + 0.006*\"prediction\" + '\n",
      "  '0.005*\"learn\" + 0.005*\"input\" + 0.005*\"test\" + 0.004*\"training\" + '\n",
      "  '0.004*\"unit\" + 0.004*\"deep\"'),\n",
      " (1,\n",
      "  '0.009*\"policy\" + 0.009*\"feedback\" + 0.006*\"option\" + 0.006*\"reward\" + '\n",
      "  '0.006*\"action\" + 0.005*\"state\" + 0.004*\"value\" + 0.003*\"advise\" + '\n",
      "  '0.003*\"human\" + 0.003*\"reinforcement\"'),\n",
      " (2,\n",
      "  '0.001*\"finding\" + 0.001*\"disease\" + 0.001*\"network\" + 0.001*\"variational\" + '\n",
      "  '0.001*\"kearn\" + 0.000*\"expansion\" + 0.000*\"treat\" + 0.000*\"model\" + '\n",
      "  '0.000*\"node\" + 0.000*\"result\"'),\n",
      " (3,\n",
      "  '0.007*\"input\" + 0.005*\"synapse\" + 0.004*\"voltage\" + 0.004*\"weak\" + '\n",
      "  '0.003*\"weight\" + 0.003*\"phase\" + 0.003*\"current\" + 0.003*\"transistor\" + '\n",
      "  '0.003*\"synaptic\" + 0.003*\"circuit\"'),\n",
      " (4,\n",
      "  '0.010*\"function\" + 0.009*\"use\" + 0.007*\"value\" + 0.006*\"set\" + '\n",
      "  '0.006*\"learn\" + 0.006*\"problem\" + 0.006*\"show\" + 0.005*\"result\" + '\n",
      "  '0.005*\"give\" + 0.005*\"point\"'),\n",
      " (5,\n",
      "  '0.019*\"model\" + 0.009*\"image\" + 0.009*\"use\" + 0.006*\"datum\" + '\n",
      "  '0.006*\"feature\" + 0.005*\"figure\" + 0.005*\"result\" + 0.005*\"set\" + '\n",
      "  '0.005*\"learn\" + 0.004*\"show\"'),\n",
      " (6,\n",
      "  '0.011*\"model\" + 0.010*\"problem\" + 0.009*\"method\" + 0.007*\"set\" + '\n",
      "  '0.006*\"use\" + 0.005*\"optimization\" + 0.005*\"function\" + 0.004*\"matrix\" + '\n",
      "  '0.004*\"step\" + 0.004*\"algorithm\"'),\n",
      " (7,\n",
      "  '0.012*\"model\" + 0.008*\"input\" + 0.008*\"task\" + 0.007*\"use\" + 0.006*\"noise\" '\n",
      "  '+ 0.006*\"function\" + 0.005*\"network\" + 0.005*\"set\" + 0.005*\"learn\" + '\n",
      "  '0.004*\"variable\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 8: Visualize Results\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el87021406516059978246743289617\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el87021406516059978246743289617_data = {\"mdsDat\": {\"x\": [0.13042202389973626, 0.11477437356271412, 0.08003749524416055, 0.035669701436050784, -0.037009210554951, -0.10260615656953939, -0.09553327193825659, -0.12575495507991483], \"y\": [0.03219055239586739, -0.05532060963332659, 0.05029786742813291, -0.027385588547497574, -0.012337254948018659, -0.01909096626599847, 0.022048748095686535, 0.009597251475154405], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [35.35583322286264, 35.19381583645087, 14.901284667802686, 9.096265635749548, 2.9257816401859276, 1.358199686195775, 1.1682434874343834, 0.0005758233181621561]}, \"tinfo\": {\"Term\": [\"model\", \"input\", \"network\", \"learn\", \"local\", \"state\", \"action\", \"prediction\", \"policy\", \"value\", \"task\", \"problem\", \"system\", \"test\", \"image\", \"weight\", \"training\", \"method\", \"set\", \"human\", \"show\", \"feedback\", \"learning\", \"use\", \"case\", \"noise\", \"function\", \"optimal\", \"layer\", \"signal\", \"player\", \"game\", \"analogy_question\", \"payment\", \"amp\", \"temperature\", \"tensor\", \"mmse\", \"payoff\", \"activity_shape\", \"bandit\", \"univariate\", \"regret\", \"privacy\", \"analogy\", \"ad\", \"lprobit\", \"confusion\", \"private\", \"mistake\", \"spin\", \"mse\", \"grid\", \"structured_prediction\", \"exogenous_event\", \"quadruple\", \"wsat\", \"clause\", \"confidence\", \"alexnet\", \"theorem\", \"question\", \"worker\", \"transition\", \"program\", \"strategy\", \"intensity\", \"user\", \"answer\", \"bind\", \"margin\", \"policy\", \"action\", \"let\", \"value\", \"sum\", \"function\", \"proof\", \"guarantee\", \"algorithm\", \"random\", \"expression\", \"bound\", \"point\", \"follow\", \"give\", \"matrix\", \"sample\", \"use\", \"learn\", \"choose\", \"state\", \"time\", \"show\", \"problem\", \"distribution\", \"consider\", \"set\", \"number\", \"result\", \"test\", \"probability\", \"learning\", \"example\", \"large\", \"method\", \"also\", \"see\", \"case\", \"base\", \"may\", \"model\", \"subject\", \"rois\", \"scene\", \"rcnn\", \"rod\", \"cue\", \"redundancy\", \"orientation\", \"perceptual\", \"localization\", \"mwis\", \"roi\", \"spike\", \"fixation\", \"movement\", \"face\", \"spatial\", \"stimulus\", \"sensory\", \"variability\", \"cortical\", \"legislation\", \"connectivity\", \"sem\", \"disparity\", \"sound\", \"unity\", \"item\", \"bipolar\", \"interference\", \"brain\", \"cell\", \"auditory\", \"behavioral\", \"topic\", \"human\", \"image\", \"visual\", \"segment\", \"source\", \"segmentation\", \"feature\", \"pattern\", \"filter\", \"model\", \"response\", \"context\", \"code\", \"target\", \"embed\", \"datum\", \"fig\", \"object\", \"location\", \"map\", \"performance\", \"activity\", \"trial\", \"use\", \"figure\", \"classification\", \"result\", \"parameter\", \"different\", \"estimate\", \"base\", \"learn\", \"set\", \"time\", \"prediction\", \"network\", \"show\", \"information\", \"distribution\", \"also\", \"number\", \"function\", \"approach\", \"space\", \"method\", \"give\", \"well\", \"first\", \"system\", \"large\", \"biomarker\", \"svrg\", \"smo\", \"submodular\", \"ialm\", \"gene\", \"instrument\", \"altproj\", \"spiv\", \"sgd\", \"outlier\", \"hdl\", \"sage\", \"recovery\", \"confounder\", \"submodular_function\", \"rpca\", \"modular\", \"best_tune\", \"orthant\", \"supergradient\", \"sparseness\", \"block_diagonal\", \"clmw\", \"instrumental_variable\", \"trait\", \"pleiotropic\", \"association\", \"tonga\", \"step_size\", \"sparsity\", \"string\", \"evidence\", \"variational\", \"sparse\", \"curvature\", \"gradient\", \"marginal\", \"optimization\", \"convergence\", \"problem\", \"method\", \"rank\", \"causal\", \"moreover\", \"approximation\", \"stochastic\", \"minimization\", \"iteration\", \"step\", \"inference\", \"set\", \"model\", \"size\", \"matrix\", \"algorithm\", \"bound\", \"use\", \"also\", \"error\", \"compute\", \"solve\", \"loss\", \"function\", \"parameter\", \"show\", \"consider\", \"datum\", \"result\", \"follow\", \"figure\", \"number\", \"give\", \"variable\", \"treermn\", \"entity\", \"quadrant\", \"spiral\", \"cvi\", \"umli\", \"analog\", \"ssflogreg\", \"unfaithful\", \"neurocomputer\", \"nigp\", \"remote_sense\", \"spam\", \"swnp\", \"fmli\", \"com\", \"mlhgp\", \"collision\", \"adiabatic\", \"heteroscedastic\", \"task_relationship\", \"eij\", \"autoencoder\", \"decoding\", \"climate\", \"component_score\", \"ordinary_logistic\", \"anticipatory\", \"untangling\", \"neuron_perturbation\", \"modulation\", \"hardware\", \"sin\", \"digital\", \"relational\", \"hide\", \"decode\", \"noise\", \"contribution\", \"task\", \"input\", \"rmn\", \"perturbation\", \"net\", \"neuron\", \"variance\", \"correlation\", \"network\", \"output\", \"model\", \"component\", \"relation\", \"weight\", \"variable\", \"system\", \"degree\", \"use\", \"function\", \"learn\", \"training\", \"set\", \"application\", \"show\", \"problem\", \"result\", \"datum\", \"feature\", \"figure\", \"represent\", \"point\", \"time\", \"distribution\", \"method\", \"value\", \"shallow\", \"tiling\", \"tangent\", \"brick\", \"bridging_test\", \"chart\", \"tile\", \"ball\", \"gin\", \"macroscopic\", \"psr\", \"cae\", \"teacher\", \"hit\", \"abstraction\", \"auto_encoder\", \"formalism\", \"observable\", \"pomdp\", \"accurate_refinement\", \"cpmc\", \"episode\", \"extractor\", \"history\", \"bengio\", \"layered\", \"cifar\", \"simpler\", \"amr\", \"multisent\", \"deep\", \"labeling\", \"nips_page\", \"depth\", \"union\", \"local\", \"interest\", \"product\", \"supervise\", \"unit\", \"prediction\", \"layer\", \"network\", \"model\", \"training\", \"input\", \"test\", \"system\", \"learn\", \"make\", \"collection\", \"abstract\", \"set\", \"variable\", \"representation\", \"output\", \"segment\", \"result\", \"compute\", \"number\", \"observation\", \"use\", \"synapse\", \"postsynaptic\", \"synapsis\", \"injection\", \"tunnel\", \"epsp\", \"transistor\", \"synaptic\", \"presynaptic\", \"drain\", \"associative\", \"potentiation\", \"depression\", \"stimulation\", \"burst\", \"plasticity\", \"silicon\", \"calibration\", \"tunneling\", \"shock\", \"associative_ltp\", \"vcal\", \"mismatch\", \"long_lasting\", \"vtun\", \"ical\", \"pathway\", \"gate\", \"depolarize\", \"depolarization\", \"voltage\", \"charge\", \"weak\", \"phase\", \"circuit\", \"input\", \"strong\", \"current\", \"pre\", \"adaptation\", \"weight\", \"signal\", \"implement\", \"stimulus\", \"learn\", \"show\", \"conditional\", \"probability\", \"fig\", \"change\", \"learning\", \"correlation\", \"bql\", \"advise\", \"frogger\", \"reward_shape\", \"action_biase\", \"control_sharing\", \"interruption\", \"interrupt\", \"option\", \"episode\", \"landmark\", \"fuel\", \"advice\", \"agent\", \"reward\", \"controller\", \"ghost\", \"control_share\", \"reward_shaping\", \"weather\", \"pellet\", \"shaping\", \"food\", \"rh\", \"inconsistent\", \"action_biasing\", \"feedback\", \"termination\", \"policy\", \"mission\", \"moderate\", \"car\", \"reinforcement\", \"action\", \"smdp\", \"human\", \"consistency\", \"state\", \"optimal\", \"value\", \"learn\", \"case\", \"domain\", \"learning\", \"use\", \"information\", \"estimate\", \"parameter\", \"show\", \"kearn\", \"ljn\", \"disease\", \"zi\", \"finding\", \"lowerbounde\", \"unsurprise\", \"variationally\", \"loglog\", \"quantitie\", \"reformulation\", \"ofmf\", \"vlog\", \"lio\", \"symptom\", \"offinding\", \"constantt\", \"jlog\", \"diagnose\", \"phys\", \"befixed\", \"perturbational\", \"lmin\", \"factorizability\", \"factorizable\", \"ei\", \"binaryvalue\", \"internist\", \"moral\", \"moralize\", \"jj\", \"loopy\", \"expansion\", \"variational\", \"parent\", \"treat\", \"odd\", \"network\", \"formalism\", \"node\", \"taylor\", \"layer\", \"inference\", \"case\", \"result\", \"model\", \"function\", \"diagnosis\", \"division\", \"algorithm\", \"rate\", \"error\", \"use\", \"number\", \"product\", \"sum\", \"also\", \"input\", \"set\", \"show\", \"figure\", \"datum\", \"give\", \"learn\"], \"Freq\": [1922.0, 438.0, 477.0, 861.0, 228.0, 342.0, 165.0, 286.0, 104.0, 688.0, 393.0, 812.0, 370.0, 426.0, 710.0, 303.0, 364.0, 750.0, 983.0, 121.0, 781.0, 83.0, 462.0, 1394.0, 442.0, 249.0, 1108.0, 202.0, 154.0, 188.0, 72.95510492694444, 90.24000288624423, 42.2712478611642, 41.162360324516655, 50.00469103556388, 42.06319937291973, 46.03601192291853, 28.470889437573152, 54.77336190671892, 23.86345480394338, 40.84415000978214, 25.309786011315243, 61.2730169387674, 22.900040305363756, 75.71463204201825, 22.005531923772345, 21.1596504291085, 29.970857000499045, 27.509740105570877, 25.195465718006798, 27.853117547305622, 29.91040905484266, 31.623584751429124, 20.03520707999885, 18.35765546330204, 19.14641077987123, 21.40864712199979, 32.246965029662825, 34.45987008420756, 17.311550763826286, 229.1274311638614, 133.3382814793323, 104.64162805579178, 44.38799328703559, 66.06856230399558, 116.68701515197932, 52.922683268829964, 83.79726128183839, 83.45846244436929, 119.37271964802252, 65.09272067607172, 82.58714386204761, 124.74363520208578, 213.43747507221389, 427.02690045045915, 158.50343472747414, 638.547819333054, 79.1522860264319, 93.80366949973258, 304.2885258617506, 177.6709989808601, 105.43609902946261, 150.54568106389272, 313.30325237604984, 276.83620871221586, 335.82665286858844, 262.9508055302051, 221.31260289638047, 560.1808220422967, 389.05382672972627, 168.17310367530027, 192.53634147176965, 311.69678980324807, 342.0492925095608, 351.25521553092216, 291.4573351380382, 213.05732755865887, 394.23318106166687, 288.13962950090735, 337.2245270318306, 220.31191058540603, 213.16109256903707, 226.6487560992757, 226.80033453897076, 219.92948896617904, 269.03891537274643, 238.81772439080058, 193.56360792323792, 203.04408272119738, 212.60871373157107, 198.75025655639047, 202.94272053945375, 103.75298807929947, 48.04364529320238, 49.58789108347364, 47.17294533912866, 39.239293076408146, 42.15093963418932, 41.49179809310741, 47.00888871082011, 32.909432899309635, 37.73730629179734, 32.74929144881853, 30.216367159823744, 77.34670112728838, 27.61028955293631, 47.00938700617869, 61.274198573182595, 88.7328568485019, 98.1110815927667, 42.932106590083244, 34.29110982884298, 36.50722860660796, 24.811298677599495, 29.232027389972455, 24.66738030028285, 23.1176230074204, 23.877745882545568, 23.10717838077923, 49.215301406850394, 24.144456693601192, 22.22832417908754, 117.73782528057802, 126.58212654812043, 49.113517461354554, 35.06247816243561, 92.86401722987048, 103.35432331841206, 552.1178063354106, 155.85074305442316, 103.93403302008316, 132.70036780493356, 97.44734505662814, 353.01680578220623, 147.01197998386615, 123.96489424370584, 1191.5264810551384, 144.36181747475433, 112.78941780010149, 123.06969951350517, 204.62679710268208, 72.90956604023405, 393.53123668123, 121.84770845013509, 141.18898584551897, 93.99168098186068, 129.86560822098187, 227.79898345186737, 137.45232589954105, 135.07018934530626, 545.5479955876388, 315.38650841866473, 137.96065835034761, 309.8497066602675, 230.36808042464278, 214.08757515085392, 188.32512139465854, 231.69931206762465, 296.61780086431395, 307.9486386822196, 254.15406699653144, 161.6285346852417, 206.63532603406108, 254.80672002019406, 177.89027557952733, 218.15968753223035, 213.01045990373947, 214.42752648148993, 249.36596113291128, 179.0620811817829, 162.9869305437715, 200.82273915020315, 193.98837627805406, 174.06258419004132, 170.85085704881593, 163.42594299169397, 163.32339380689746, 35.47105411522259, 24.367647897357408, 22.39399815918425, 19.12282629539894, 18.38508758927086, 16.95444975527448, 16.20280888105316, 16.167228372794998, 15.471066568078797, 23.153363967134133, 30.040440774144788, 13.990498475733862, 13.263503244547438, 41.887755397452274, 12.513563784719315, 12.481714633447481, 12.440742796587305, 12.302694644173894, 10.285857764306185, 13.107336309805019, 10.272255491221129, 9.513917464153584, 12.455512248988157, 9.51249409312225, 8.810747309007287, 8.807327149604834, 8.806593235690318, 16.216310184462092, 8.768501295114824, 14.001121389868203, 33.0171112188569, 26.124375588953004, 75.9499208483177, 30.183941289068915, 79.90521350618886, 16.14123546273838, 93.18561937592153, 39.988069714565455, 125.6881258308247, 58.67221893544612, 253.3297225346986, 234.95075792742912, 65.5493811230654, 28.08669863927914, 37.95828385048732, 98.92837670321265, 44.33246032195763, 31.225589678270858, 75.72994559929494, 105.93949400383707, 81.2540960427263, 185.53533039434043, 275.3321904300743, 92.46912047002176, 109.99254339231773, 105.61848879828626, 65.38999370775764, 163.47700667693675, 105.01623934143639, 86.55121842990711, 72.74039926036183, 61.47920607190596, 66.98619468441636, 120.7437221695583, 86.8774198062531, 101.92166042717245, 78.30725234174335, 94.61597087800054, 98.61298963128262, 83.01479018488249, 87.38265398145845, 85.77791729964197, 85.37540696030742, 75.72743190646247, 25.2141852877036, 22.789208534441077, 15.459404348866302, 14.836162582566258, 12.282222444420968, 11.070910660431482, 27.93804416992774, 11.401476157004787, 9.82944627517582, 9.27318841553579, 9.211801373326564, 8.651743275841989, 10.851218091899277, 7.8814973441292056, 7.356810620433703, 7.353858099648891, 6.736103725927569, 6.737242601369724, 6.136220865303782, 6.11388906336837, 6.060542224199885, 10.454294242220616, 9.451638606182126, 9.143093514664452, 4.916273682464932, 4.907855847462152, 5.322365111480263, 4.876264932986115, 6.11977294650894, 4.835279676009844, 16.06206632314982, 9.863503066568414, 8.411805770431341, 9.913643023837583, 16.6273404333622, 41.5015005668417, 38.740307176584906, 93.0678487310669, 39.79538788023046, 121.30345952762809, 129.27058319427778, 13.490880225461654, 18.83898134612587, 28.933288836206938, 46.17988313195824, 47.1473789784982, 40.24212143169869, 86.36166440372007, 59.584020237817995, 189.57831737865905, 43.062764488008675, 30.489730475412866, 54.62027853970081, 64.3456235040647, 56.55740079459337, 29.848280501013807, 108.17098561164877, 88.82558738535019, 74.09173316671641, 50.093644227763725, 75.87514219432252, 36.521244254881566, 64.21278536686985, 62.845295858574914, 58.195596763802804, 54.94748352578123, 47.60985072145626, 51.495311038503694, 39.01950276720878, 42.89424036038403, 42.128868226807384, 41.59711826753445, 40.68432523948586, 40.05331464999599, 11.559084357608754, 13.438603543267119, 20.404083960397703, 6.701019740697127, 5.9871571190616155, 7.328672819046886, 6.851159284955005, 11.266283071278371, 3.8257201750873677, 4.917183697270744, 3.153482269439017, 3.312675836689546, 3.0948191598446866, 4.546659664199438, 3.1300783345840646, 2.6558773200974883, 4.519794079781068, 5.101971397689621, 2.0891747504317255, 2.087483871174511, 2.516244632352765, 4.213301538903382, 2.2293372341883555, 14.332462473631583, 4.89587261200653, 1.9585971213043616, 1.9487132670269343, 1.7332154473766697, 2.1211126767289645, 2.1073354653308045, 21.711222246110214, 15.464096102924971, 3.761864739094298, 8.973996424771038, 3.830219520609321, 39.69370554096473, 20.77279239540754, 19.905935456030274, 10.717129754094147, 21.8740342860944, 30.35916672996163, 18.2497809912054, 32.80678465534003, 60.032614975382714, 22.67945204987135, 23.86866404584946, 23.441252851613573, 21.178830662789437, 26.47133688624799, 17.663741680810162, 7.888756415303908, 9.266745042187642, 17.380079930484126, 13.17802853074227, 11.241662425336594, 11.508925798125695, 9.839350228856212, 12.676821678662433, 11.083112424869588, 11.923305874593147, 10.147072204210007, 11.102816516478025, 10.847739376374044, 5.972310987601955, 5.5257392728379955, 5.965773813811303, 4.19488843291252, 3.310413709005881, 6.56741687303031, 6.4118032365487565, 2.8625859865695866, 2.8613665163032937, 3.087109445562211, 2.6440030153080545, 2.6420758731773843, 4.417575131778891, 3.085199061189199, 2.19900982637201, 1.9732601665382956, 2.8586613746695506, 1.7513768785288224, 1.5323893859861195, 1.5322079351839375, 1.5290396201172207, 2.3496005310335812, 1.3100936098926548, 1.3075061846385254, 1.3067856010592316, 2.415180504682806, 1.9589054988001857, 1.0886211616631412, 1.0885185547371534, 9.930370273054104, 2.1071269037745206, 9.49597732734743, 7.253334970228743, 6.299885590519111, 17.324088550509327, 5.921820687930414, 6.7839089706273334, 3.0653908422027643, 4.5523811275959805, 7.619739770095536, 6.2761181452479695, 4.303956710705628, 4.295797605747552, 6.0401755106394415, 5.2555531219660265, 3.519753540691002, 3.930216133744711, 3.5198002426385733, 3.366525549578335, 3.384903040665051, 3.0933136619384127, 5.133489994824742, 7.038613183799376, 3.218986339263146, 3.216398665083242, 2.651060500674319, 2.2679762623235638, 2.059611189134749, 1.8690462708150914, 12.728052606096679, 3.612429072091523, 1.6858444445676546, 1.4930976899280135, 1.6902128431215657, 4.845248469341349, 12.383746145724324, 2.7747733492326936, 1.3111464586579433, 1.3110724981047235, 1.310755271131249, 1.4898570125889314, 1.1209255868733914, 1.1198503233033381, 1.3091362911619666, 1.2923004590330325, 1.3109278851380446, 0.9284741427813107, 17.42616116257336, 1.4827097766360917, 19.1163805038266, 0.9176637918567414, 2.3448035549416306, 1.9948307237507112, 6.005112515267228, 11.177279862399065, 2.141649747058595, 6.732166492621846, 3.366035278284394, 9.202060500036456, 5.114434164620729, 7.1106209893776, 5.715568055309541, 4.448531150741343, 3.083151048847643, 4.045959369947812, 4.130322604940592, 2.9249565143814404, 2.5925385631259976, 2.3919660494390422, 2.4177062123251045, 0.000613856076283295, 0.0002707495099560658, 0.0010385130332846832, 0.00020814939768423274, 0.0010463192757333769, 0.00015126165624850234, 0.0001512612631644091, 0.0001511525535746271, 0.00015113370009682238, 0.00015112132522722085, 0.00015109708504147196, 0.00015106563831401393, 0.00015106619154347845, 0.00015097052652212348, 0.00015097051196345335, 0.00015096857566032748, 0.0001509666539158717, 0.00015096010251431795, 0.00015094132182986384, 0.00015093618261931166, 0.00015093404249480412, 0.00015092242467604878, 0.00015091122905872692, 0.0001508646121970044, 0.00015085520729610723, 0.00015083591705819895, 0.00015082770596825157, 0.0001507893438724868, 0.00015078002632361033, 0.00015078014279297128, 0.0001900811417524243, 0.0002034486360413808, 0.00041848486703892324, 0.0006472584407811369, 0.0002901043592423414, 0.0004103960990378911, 0.0002460445823396104, 0.0007151696287058998, 0.00020368479222938857, 0.0003674752177812653, 0.00017513707451756127, 0.0003164477778287214, 0.00034244665092805424, 0.0003267133873033555, 0.0003509103338020027, 0.0003932633104546086, 0.00032982279895012965, 0.00018436478184070889, 0.00016764799269741986, 0.00028417237086748664, 0.0002529895047466967, 0.0002558195937485589, 0.0002868888731250768, 0.00026141981914873136, 0.00021758970526713486, 0.0002315236774104396, 0.000249562189979194, 0.00023836058904390964, 0.0002519339720456956, 0.0002478778100810529, 0.00024182448974942257, 0.000241368264703889, 0.0002378395633576744, 0.00023554850911698523], \"Total\": [1922.0, 438.0, 477.0, 861.0, 228.0, 342.0, 165.0, 286.0, 104.0, 688.0, 393.0, 812.0, 370.0, 426.0, 710.0, 303.0, 364.0, 750.0, 983.0, 121.0, 781.0, 83.0, 462.0, 1394.0, 442.0, 249.0, 1108.0, 202.0, 154.0, 188.0, 75.9352890846276, 95.53823580650493, 45.308346878749866, 44.18231637743971, 54.27578169365971, 45.873871170814695, 50.568850653328646, 31.389727904771043, 61.145423901069506, 26.67286715537763, 45.76255672121716, 28.508793232982413, 69.06339479820475, 25.82214843802586, 85.50082856973945, 24.875005186306073, 23.988413568922564, 34.06463605262547, 31.338443966622457, 28.875762692218785, 31.987302972397238, 34.35091484117875, 36.32176166111829, 23.019346624937246, 21.154100061504277, 22.110660131250416, 24.879418503291866, 37.642945332444626, 40.27780453226317, 20.252582961056586, 273.3956283602676, 158.3356492118811, 124.93330210176953, 52.09585358951469, 78.86976516855407, 145.73391256572842, 63.48883933027207, 103.76411857214637, 104.62026807418175, 154.51448027125073, 80.13661008716564, 104.9947412548957, 165.7647556119366, 308.09402693536896, 688.9330598920558, 225.73541204640586, 1108.6561775040432, 103.13964762539979, 125.73648232309233, 518.9373670739159, 273.9116753252107, 147.30035846446896, 230.1296174380152, 592.8341205023459, 511.84908949941683, 661.2487466907611, 486.04800979945054, 397.23116250279014, 1394.980991617944, 861.2112691622337, 281.72329998293105, 342.01058725139603, 680.7070250992375, 781.1598290407126, 812.8236387344458, 623.861556080864, 400.20799893775387, 983.2814736906602, 632.8793431203244, 819.2514788082412, 426.4092839162599, 412.01379366393854, 462.8116221546406, 465.134776904401, 473.4141717580339, 750.2194362713441, 596.534766382661, 383.9789138091797, 442.532075990013, 542.4520057017257, 419.7699853738168, 1922.2534463672453, 108.53634989972556, 51.02091280772094, 52.715727847784386, 50.162056411642475, 42.12372944650131, 45.47187660937428, 44.80174807339868, 50.89676550063317, 35.82317904704567, 41.18943891446405, 35.8204374768023, 33.11984136578109, 84.93732116692615, 30.440678993777265, 51.8328867946072, 67.98116525552943, 98.45935856519465, 108.92417159369288, 47.706900170007565, 38.18139825788186, 40.79363807334264, 27.727262132734168, 32.73518490578263, 27.685834007862642, 25.96606463425242, 26.836659364000962, 25.97706422752908, 55.394587809439564, 27.18824423560439, 25.068429355902854, 132.84317991754915, 142.9812137558309, 55.412023895317844, 39.5710391771176, 107.40109788358623, 121.68846204306823, 710.8960999735776, 188.06129933754883, 123.35465749233202, 160.06154904326266, 115.63121164492183, 472.71663139321606, 182.89452701258813, 152.18218586601878, 1922.2534463672453, 188.5122260712214, 143.53412439337785, 159.06838048780955, 290.63099929709676, 88.06392100369581, 691.2574446882909, 166.48223374281963, 202.7380591690204, 122.24759120429705, 184.94257720053565, 390.06779102379556, 203.75012748925877, 204.5520947111469, 1394.980991617944, 654.1907650232196, 216.38519751155025, 819.2514788082412, 509.2748998426451, 454.1947699054602, 368.9108275868753, 542.4520057017257, 861.2112691622337, 983.2814736906602, 680.7070250992375, 286.93234181902193, 477.0060962952257, 781.1598290407126, 361.7426743701774, 623.861556080864, 596.534766382661, 632.8793431203244, 1108.6561775040432, 442.92730065758303, 343.649060834961, 750.2194362713441, 661.2487466907611, 444.758660817787, 435.0990866993852, 370.7593273364824, 473.4141717580339, 38.518431865136314, 27.366184030717047, 25.842910626052273, 22.14508089961332, 21.43859499431582, 19.93504711288711, 19.178740080594174, 19.216988349079934, 18.446521037788415, 27.64810773999408, 36.074141563268284, 16.95825006378816, 16.22774523275523, 51.42132249436357, 15.467689247960164, 15.46615286780366, 15.486859629197632, 15.516824529756601, 13.235632825207244, 16.883844122081342, 13.236553036677929, 12.48994390288578, 16.36509243365319, 12.511986599881478, 11.753343050008645, 11.749663276557506, 11.75134322741171, 21.69682664916675, 11.756157389448322, 18.806131352507627, 44.904676426610855, 35.942643952768435, 109.97544713856097, 43.36767595647437, 125.3882235042699, 22.503346392250204, 163.42902312381898, 63.46205469581041, 289.300748685982, 112.18721169195288, 812.8236387344458, 750.2194362713441, 140.6754473363241, 47.53995477727433, 73.94879754943844, 288.2682862258679, 94.20039283077395, 56.79175023651715, 219.98482423133754, 368.86282868630906, 249.72263023642168, 983.2814736906602, 1922.2534463672453, 324.9024831500737, 486.04800979945054, 518.9373670739159, 230.1296174380152, 1394.980991617944, 596.534766382661, 440.67132441832223, 307.3921796603335, 215.40086834539417, 266.90568613425694, 1108.6561775040432, 509.2748998426451, 781.1598290407126, 400.20799893775387, 691.2574446882909, 819.2514788082412, 511.84908949941683, 654.1907650232196, 632.8793431203244, 661.2487466907611, 440.4413941040071, 28.453293611244302, 26.880190068280047, 18.55844804775557, 17.931116753206368, 15.405296298375657, 14.20295854544726, 36.62496924547498, 14.954185106292513, 12.95016312781637, 12.323381259340842, 12.313889633284951, 11.699987149642945, 15.228149778684822, 11.127493221938032, 10.454675587271154, 10.453915938310313, 9.819517188029877, 9.83998263856598, 9.198922713342052, 9.195610138808561, 9.236811919712695, 16.104102237835825, 14.583847938917668, 14.143596314636358, 7.948713770038288, 7.947771837114275, 8.651167744930197, 7.961531445518157, 10.090838104962241, 7.978801197952231, 27.248826844774165, 16.767154687380334, 14.366603857099415, 17.3365174485122, 31.00627373377903, 93.13975787475, 87.88006393008499, 249.74485161047818, 93.88337296843144, 393.91353529175944, 438.36833708829926, 25.368490016066936, 40.1185488298722, 74.63453702953417, 151.78762301585886, 164.26696245436173, 136.39272142292862, 477.0060962952257, 287.7783875973443, 1922.2534463672453, 180.92942187501993, 102.95647679108956, 303.28521512021393, 440.4413941040071, 370.7593273364824, 109.51269577713357, 1394.980991617944, 1108.6561775040432, 861.2112691622337, 364.955837016417, 983.2814736906602, 186.48034553291015, 781.1598290407126, 812.8236387344458, 819.2514788082412, 691.2574446882909, 472.71663139321606, 654.1907650232196, 260.32223166433164, 592.8341205023459, 680.7070250992375, 623.861556080864, 750.2194362713441, 688.9330598920558, 16.12389477228298, 19.458857089701194, 30.128027684909085, 10.067540067432446, 9.351797920111883, 11.815778101230125, 12.365320851869193, 20.73260948056813, 7.212098730572224, 9.795074680498994, 6.4658630090585465, 7.121589279227434, 6.790261552036549, 10.428713861688816, 7.415945243409295, 6.3189800919874735, 10.756205974553097, 12.542314053997469, 5.386090271584486, 5.387196729922806, 6.530155506330578, 11.164787898936558, 6.055644313852107, 39.57928909945836, 13.723002030690752, 5.5661160720782, 5.597381967158107, 5.027754788535985, 6.162997937028331, 6.181483532296676, 65.52609907876428, 54.566198327261105, 11.649332563314642, 33.243337861176336, 12.26049001176136, 228.3311619078902, 105.51735365906313, 113.5865572530524, 52.29624009828717, 154.89663591064152, 286.93234181902193, 154.16302935656145, 477.0060962952257, 1922.2534463672453, 364.955837016417, 438.36833708829926, 426.4092839162599, 370.7593273364824, 861.2112691622337, 343.61044154104803, 46.89990359915073, 75.69509721689882, 983.2814736906602, 440.4413941040071, 221.90332031494168, 287.7783875973443, 123.35465749233202, 819.2514788082412, 307.3921796603335, 632.8793431203244, 198.49768986298164, 1394.980991617944, 15.782335706559687, 9.463244460070294, 9.027354194082811, 10.387221165386677, 7.674206590489485, 6.736746863539382, 14.089792117199268, 13.950278616007255, 6.300877518732421, 6.3114084947037, 7.010901846739872, 6.060919031770341, 6.061798321883586, 10.146254673805394, 7.407632748016223, 5.610814098053309, 5.403398445627546, 8.1044193783489, 5.1763940038350675, 4.932388406034571, 4.933075792581452, 4.950323418987721, 8.190763219612021, 4.707664616236629, 4.722761368661678, 4.725052409009959, 9.42712787767239, 7.794609221550495, 4.481180494227517, 4.481353179097475, 43.72502602201904, 10.542788041548745, 75.0778905768798, 53.568712764862546, 45.07868529279318, 438.36833708829926, 90.68645438014377, 150.92880430659108, 23.866358107595033, 63.52437813510109, 303.28521512021393, 188.26559381063745, 73.72312604066941, 108.92417159369288, 861.2112691622337, 781.1598290407126, 98.77459106374053, 412.01379366393854, 166.48223374281963, 137.7823612828463, 462.8116221546406, 136.39272142292862, 8.801448478905, 13.310248735660304, 6.818878391450909, 6.826967819428814, 6.197511773883251, 5.802974622808733, 5.67733939419943, 5.471201172655415, 38.16368537358506, 11.164787898936558, 5.245404015403655, 5.049950447814748, 5.8670340103364635, 17.073713345872555, 43.829847159893475, 10.110312147825901, 4.808060164281493, 4.807835245814181, 4.811138944183457, 5.686862682665518, 4.6092525856633415, 4.610860364224473, 5.694124811929502, 5.804932299724545, 6.070373945186806, 4.410693493684005, 83.98651556868418, 7.618423517963449, 104.9947412548957, 5.079686336212115, 14.101556618920746, 12.013933852047298, 59.20394644448933, 165.7647556119366, 14.627436948884213, 121.68846204306823, 33.75476067233348, 342.01058725139603, 202.9142025013922, 688.9330598920558, 861.2112691622337, 442.532075990013, 108.41083442512631, 462.8116221546406, 1394.980991617944, 361.7426743701774, 368.9108275868753, 509.2748998426451, 781.1598290407126, 7.99141862976883, 4.8773467342097785, 20.106967790792744, 5.126219639467802, 26.840786093045402, 4.087395605101639, 4.087478331885498, 4.088083677519679, 4.088232292622304, 4.088348078853371, 4.0887483430889375, 4.088437671144111, 4.088685620459932, 4.089251493633113, 4.0892755366104545, 4.089338640520894, 4.089289181213163, 4.089302689925097, 4.089558651135186, 4.089478387413957, 4.089426799960679, 4.089719620732076, 4.0897002623463115, 4.089848278300549, 4.0900200590110085, 4.090265411879509, 4.09033878892629, 4.090689156113784, 4.090570660707842, 4.090575909747405, 5.833532756491091, 6.540243048431204, 21.203314774389593, 43.36767595647437, 12.714727179165491, 49.874277971545354, 14.337922647511743, 477.0060962952257, 10.756205974553097, 142.00192457734445, 7.819471322473843, 154.16302935656145, 249.72263023642168, 442.532075990013, 819.2514788082412, 1922.2534463672453, 1108.6561775040432, 18.170258804156962, 8.786497617426118, 518.9373670739159, 222.1262661422177, 440.67132441832223, 1394.980991617944, 632.8793431203244, 113.5865572530524, 225.73541204640586, 596.534766382661, 438.36833708829926, 983.2814736906602, 781.1598290407126, 654.1907650232196, 691.2574446882909, 661.2487466907611, 861.2112691622337], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.735799789428711, -6.523200035095215, -7.281499862670898, -7.30810022354126, -7.113500118255615, -7.286499977111816, -7.196199893951416, -7.676799774169922, -7.02239990234375, -7.853300094604492, -7.315899848937988, -7.794400215148926, -6.910299777984619, -7.894499778747559, -6.698699951171875, -7.934299945831299, -7.973499774932861, -7.625400066375732, -7.711100101470947, -7.798999786376953, -7.698699951171875, -7.627399921417236, -7.571700096130371, -8.02810001373291, -8.115599632263184, -8.07349967956543, -7.9618000984191895, -7.552199840545654, -7.485799789428711, -8.174300193786621, -5.591400146484375, -6.132699966430664, -6.375100135803223, -7.232699871063232, -6.83489990234375, -6.26609992980957, -7.05679988861084, -6.5971999168396, -6.60129976272583, -6.2434000968933105, -6.849800109863281, -6.611800193786621, -6.199399948120117, -5.662300109863281, -4.968800067901611, -5.95989990234375, -4.566400051116943, -6.654300212860107, -6.484399795532227, -5.307700157165527, -5.845699787139893, -6.367499828338623, -6.01140022277832, -5.278500080108643, -5.402200222015381, -5.209000110626221, -5.453700065612793, -5.626100063323975, -4.697400093078613, -5.0619001388549805, -5.900599956512451, -5.765399932861328, -5.283599853515625, -5.190700054168701, -5.164100170135498, -5.3506999015808105, -5.664100170135498, -5.048699855804443, -5.362199783325195, -5.204899787902832, -5.6305999755859375, -5.663599967956543, -5.602200031280518, -5.601600170135498, -5.632299900054932, -5.430799961090088, -5.549900054931641, -5.760000228881836, -5.712200164794922, -5.666200160980225, -5.73360013961792, -5.712699890136719, -6.379000186920166, -7.148900032043457, -7.117300033569336, -7.167200088500977, -7.351399898529053, -7.279799938201904, -7.295499801635742, -7.1707000732421875, -7.527299880981445, -7.390399932861328, -7.532199859619141, -7.61269998550415, -6.672699928283691, -7.702899932861328, -7.1707000732421875, -6.905700206756592, -6.535399913787842, -6.434899806976318, -7.26140022277832, -7.486199855804443, -7.423500061035156, -7.809700012207031, -7.6458001136779785, -7.8155999183654785, -7.88040018081665, -7.848100185394287, -7.880899906158447, -7.124800205230713, -7.836999893188477, -7.9197001457214355, -6.252600193023682, -6.180200099945068, -7.1269001960754395, -7.463900089263916, -6.4899001121521, -6.382900238037109, -4.707300186157227, -5.972099781036377, -6.377299785614014, -6.132999897003174, -6.441699981689453, -5.1545000076293945, -6.0304999351501465, -6.201000213623047, -3.9381000995635986, -6.048699855804443, -6.295499801635742, -6.2083001136779785, -5.699900150299072, -6.731800079345703, -5.045899868011475, -6.218299865722656, -6.070899963378906, -6.477799892425537, -6.1545000076293945, -5.592599868774414, -6.097799777984619, -6.115200042724609, -4.719299793243408, -5.267199993133545, -6.094099998474121, -5.284999847412109, -5.581399917602539, -5.654699802398682, -5.782899856567383, -5.5756001472473145, -5.32859992980957, -5.291100025177002, -5.483099937438965, -5.935699939727783, -5.690100193023682, -5.480500221252441, -5.839900016784668, -5.635799884796143, -5.6596999168396, -5.65310001373291, -5.502099990844727, -5.8333001136779785, -5.9274001121521, -5.718599796295166, -5.753200054168701, -5.861599922180176, -5.880300045013428, -5.924699783325195, -5.925300121307373, -6.592899799346924, -6.968400001525879, -7.052800178527832, -7.210700035095215, -7.250100135803223, -7.331099987030029, -7.376399993896484, -7.378600120544434, -7.422599792480469, -7.019499778747559, -6.759099960327148, -7.523200035095215, -7.576600074768066, -6.426599979400635, -7.634799957275391, -7.637400150299072, -7.640600204467773, -7.651800155639648, -7.830900192260742, -7.588399887084961, -7.832200050354004, -7.908899784088135, -7.639500141143799, -7.908999919891357, -7.985599994659424, -7.986000061035156, -7.986100196838379, -7.3755998611450195, -7.990499973297119, -7.522500038146973, -6.664599895477295, -6.898799896240234, -5.831500053405762, -6.754300117492676, -5.780799865722656, -7.380199909210205, -5.626999855041504, -6.4730000495910645, -5.3277997970581055, -6.089700222015381, -4.6269001960754395, -4.702199935913086, -5.978799819946289, -6.826300144195557, -6.525100231170654, -5.567200183868408, -6.369900226593018, -6.720399856567383, -5.834400177001953, -5.498799800872803, -5.763999938964844, -4.938399791717529, -4.543600082397461, -5.634699821472168, -5.46120023727417, -5.501800060272217, -5.981299877166748, -5.064899921417236, -5.507500171661377, -5.700900077819824, -5.87470006942749, -6.042900085449219, -5.957099914550781, -5.368000030517578, -5.6971001625061035, -5.537399768829346, -5.801000118255615, -5.611800193786621, -5.570400238037109, -5.742599964141846, -5.691299915313721, -5.70989990234375, -5.714600086212158, -5.834499835968018, -6.4405999183654785, -6.541800022125244, -6.929800033569336, -6.9710001945495605, -7.159900188446045, -7.263700008392334, -6.3379998207092285, -7.234300136566162, -7.382699966430664, -7.440899848937988, -7.4475998878479, -7.510300159454346, -7.28380012512207, -7.603499889373779, -7.672399997711182, -7.672800064086914, -7.7606000900268555, -7.76039981842041, -7.853799819946289, -7.857500076293945, -7.866199970245361, -7.321000099182129, -7.421800136566162, -7.454999923706055, -8.075499534606934, -8.077199935913086, -7.996099948883057, -8.083700180053711, -7.856500148773193, -8.092100143432617, -6.891600131988525, -7.379199981689453, -7.538400173187256, -7.374100208282471, -6.85699987411499, -5.942299842834473, -6.011199951171875, -5.134699821472168, -5.984300136566162, -4.869699954986572, -4.806099891662598, -7.065999984741211, -6.732100009918213, -6.302999973297119, -5.8354997634887695, -5.814799785614014, -5.973100185394287, -5.209499835968018, -5.580699920654297, -4.4232001304626465, -5.905399799346924, -6.2505998611450195, -5.667600154876709, -5.503799915313721, -5.632800102233887, -6.271900177001953, -4.984300136566162, -5.181399822235107, -5.36269998550415, -5.7540998458862305, -5.338900089263916, -6.070099830627441, -5.505799770355225, -5.527400016784668, -5.6041998863220215, -5.6616997718811035, -5.804999828338623, -5.726500034332275, -6.004000186920166, -5.909299850463867, -5.927299976348877, -5.940000057220459, -5.962200164794922, -5.977799892425537, -6.086299896240234, -5.9355998039245605, -5.51800012588501, -6.631499767303467, -6.744100093841553, -6.541900157928467, -6.609300136566162, -6.1118998527526855, -7.191999912261963, -6.940999984741211, -7.385200023651123, -7.335999965667725, -7.4039998054504395, -7.0192999839782715, -7.3927001953125, -7.557000160217285, -7.025300025939941, -6.904099941253662, -7.796999931335449, -7.797800064086914, -7.611000061035156, -7.0954999923706055, -7.73199987411499, -5.871200084686279, -6.945300102233887, -7.861499786376953, -7.866600036621094, -7.983799934387207, -7.781799793243408, -7.788300037384033, -5.455900192260742, -5.795199871063232, -7.208799839019775, -6.339399814605713, -7.190800189971924, -4.852499961853027, -5.500100135803223, -5.542699813842773, -6.161900043487549, -5.448400020599365, -5.12060022354126, -5.6296000480651855, -5.043099880218506, -4.438799858093262, -5.412300109863281, -5.361199855804443, -5.379199981689453, -5.4807000160217285, -5.257699966430664, -5.662199974060059, -6.468299865722656, -6.307300090789795, -5.678400039672852, -5.9552001953125, -6.114099979400635, -6.09060001373291, -6.247300148010254, -5.99399995803833, -6.128300189971924, -6.055200099945068, -6.216599941253662, -6.126500129699707, -5.382400035858154, -5.9791998863220215, -6.0569000244140625, -5.980299949645996, -6.332499980926514, -6.569300174713135, -5.884200096130371, -5.908199787139893, -6.714600086212158, -6.715000152587891, -6.639100074768066, -6.794000148773193, -6.7947998046875, -6.280700206756592, -6.639699935913086, -6.978300094604492, -7.086599826812744, -6.716000080108643, -7.205900192260742, -7.3394999504089355, -7.339600086212158, -7.341700077056885, -6.912099838256836, -7.496200084686279, -7.498199939727783, -7.498799800872803, -6.8846001625061035, -7.093900203704834, -7.681399822235107, -7.68149995803833, -5.470699787139893, -7.020999908447266, -5.515500068664551, -5.784900188446045, -5.92579984664917, -4.9141998291015625, -5.98769998550415, -5.851799964904785, -6.646200180053711, -6.250699996948242, -5.735599994659424, -5.929599761962891, -6.30679988861084, -6.308700084686279, -5.967899799346924, -6.10699987411499, -6.507900238037109, -6.397600173950195, -6.507900238037109, -6.552499771118164, -6.546999931335449, -6.6371002197265625, -5.979899883270264, -5.664299964904785, -6.446599960327148, -6.447400093078613, -6.640699863433838, -6.796800136566162, -6.893199920654297, -6.990200042724609, -5.071899890899658, -6.331299781799316, -7.093400001525879, -7.214799880981445, -7.090799808502197, -6.037700176239014, -5.099299907684326, -6.595099925994873, -7.344799995422363, -7.344799995422363, -7.345099925994873, -7.2170000076293945, -7.501500129699707, -7.502500057220459, -7.34630012512207, -7.359300136566162, -7.344900131225586, -7.689899921417236, -4.757699966430664, -7.221799850463867, -4.66510009765625, -7.701600074768066, -6.763500213623047, -6.925099849700928, -5.8231000900268555, -5.2017998695373535, -6.854100227355957, -5.708799839019775, -6.401899814605713, -5.396200180053711, -5.98360013961792, -5.654099941253662, -5.872499942779541, -6.1230998039245605, -6.489699840545654, -6.2179999351501465, -6.197299957275391, -6.542399883270264, -6.663000106811523, -6.743599891662598, -6.732900142669678, -7.396200180053711, -8.214799880981445, -6.8703999519348145, -8.477700233459473, -6.8628997802734375, -8.79699993133545, -8.79699993133545, -8.797699928283691, -8.797800064086914, -8.797900199890137, -8.798100471496582, -8.798299789428711, -8.798299789428711, -8.79889965057373, -8.79889965057373, -8.79889965057373, -8.79889965057373, -8.798999786376953, -8.799099922180176, -8.799099922180176, -8.799099922180176, -8.799200057983398, -8.799300193786621, -8.799599647521973, -8.799699783325195, -8.799799919128418, -8.799799919128418, -8.800100326538086, -8.800200462341309, -8.800200462341309, -8.568499565124512, -8.50059986114502, -7.779300212860107, -7.343200206756592, -8.145700454711914, -7.798900127410889, -8.310500144958496, -7.243500232696533, -8.49940013885498, -7.909299850463867, -8.650400161743164, -8.058799743652344, -7.979899883270264, -8.026900291442871, -7.955399990081787, -7.8414998054504395, -8.017399787902832, -8.599100112915039, -8.694100379943848, -8.166399955749512, -8.282600402832031, -8.271499633789062, -8.156900405883789, -8.249799728393555, -8.43340015411377, -8.371299743652344, -8.296299934387207, -8.34220027923584, -8.286800384521484, -8.303000450134277, -8.327799797058105, -8.329700469970703, -8.344400405883789, -8.354100227355957], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9997, 0.9827, 0.9703, 0.9689, 0.9577, 0.953, 0.9458, 0.9421, 0.9297, 0.9284, 0.926, 0.9207, 0.92, 0.9196, 0.9182, 0.9171, 0.9142, 0.9117, 0.9094, 0.9034, 0.9013, 0.9013, 0.9012, 0.9009, 0.8979, 0.8958, 0.8895, 0.885, 0.8837, 0.8828, 0.8631, 0.8679, 0.8625, 0.8796, 0.8626, 0.8174, 0.8577, 0.826, 0.8137, 0.7817, 0.8318, 0.7997, 0.7554, 0.6726, 0.5614, 0.6861, 0.488, 0.775, 0.7467, 0.5059, 0.6068, 0.7053, 0.6153, 0.402, 0.4251, 0.3622, 0.4254, 0.4548, 0.1273, 0.2451, 0.5238, 0.4652, 0.2586, 0.2139, 0.2007, 0.2787, 0.4093, 0.1258, 0.2529, 0.1521, 0.3794, 0.3807, 0.3258, 0.3214, 0.273, 0.0142, 0.1243, 0.3547, 0.2606, 0.1031, 0.292, -1.2086, 0.9992, 0.9842, 0.9831, 0.9829, 0.9734, 0.9685, 0.9675, 0.9648, 0.9595, 0.9568, 0.9547, 0.9526, 0.9507, 0.9467, 0.9466, 0.9404, 0.9403, 0.9397, 0.9388, 0.9368, 0.9333, 0.9332, 0.9311, 0.9289, 0.9281, 0.9275, 0.9272, 0.926, 0.9256, 0.9241, 0.9236, 0.9225, 0.9236, 0.9233, 0.8989, 0.881, 0.7915, 0.8564, 0.873, 0.8568, 0.8732, 0.7523, 0.8259, 0.8392, 0.566, 0.7775, 0.8032, 0.7877, 0.6934, 0.8555, 0.4809, 0.7322, 0.6825, 0.7815, 0.6908, 0.5064, 0.6507, 0.6293, 0.1055, 0.3147, 0.5942, 0.072, 0.251, 0.2922, 0.3719, 0.1936, -0.0216, -0.1167, 0.0591, 0.4704, 0.2077, -0.076, 0.3345, -0.0064, 0.0145, -0.038, -0.4477, 0.1386, 0.2983, -0.2736, -0.182, 0.1062, 0.1095, 0.2251, -0.0199, 1.8213, 1.7877, 1.7605, 1.757, 1.7501, 1.7418, 1.7351, 1.7309, 1.7278, 1.7263, 1.7207, 1.7113, 1.702, 1.6987, 1.6918, 1.6893, 1.6847, 1.6716, 1.6516, 1.6505, 1.6502, 1.6316, 1.6307, 1.6296, 1.6156, 1.6155, 1.6153, 1.6126, 1.6105, 1.6087, 1.5962, 1.5847, 1.5335, 1.5413, 1.4531, 1.5714, 1.3419, 1.4419, 1.0701, 1.2555, 0.7379, 0.7427, 1.1401, 1.3774, 1.2368, 0.8342, 1.15, 1.3056, 0.8373, 0.6562, 0.781, 0.2361, -0.0396, 0.6471, 0.4178, 0.3118, 0.6454, -0.2402, 0.1667, 0.2762, 0.4625, 0.6499, 0.5213, -0.3135, 0.1352, -0.1329, 0.2724, -0.085, -0.2135, 0.0847, -0.1094, -0.0948, -0.1433, 0.1431, 2.2764, 2.2322, 2.2146, 2.2078, 2.1707, 2.1482, 2.1266, 2.1261, 2.1216, 2.1129, 2.1071, 2.0955, 2.0584, 2.0524, 2.0459, 2.0456, 2.0204, 2.0185, 1.9924, 1.9891, 1.9759, 1.9652, 1.9636, 1.961, 1.9168, 1.9153, 1.9115, 1.9071, 1.8972, 1.8965, 1.8688, 1.8667, 1.862, 1.8384, 1.7742, 1.5889, 1.5782, 1.4102, 1.539, 1.2195, 1.1762, 1.7658, 1.6414, 1.4497, 1.2074, 1.1491, 1.1767, 0.6883, 0.8225, 0.0809, 0.9619, 1.1804, 0.683, 0.4738, 0.517, 1.0974, -0.1596, -0.1269, -0.0557, 0.4114, -0.1645, 0.7669, -0.1013, -0.1625, -0.2473, -0.1348, 0.1018, -0.1446, 0.4994, -0.2289, -0.3851, -0.3106, -0.5172, -0.4476, 3.1988, 3.1614, 3.1419, 3.1246, 3.0857, 3.054, 2.9411, 2.9217, 2.8976, 2.8425, 2.8136, 2.7662, 2.7458, 2.7014, 2.669, 2.6648, 2.6646, 2.6321, 2.5846, 2.5835, 2.5779, 2.5571, 2.5323, 2.5158, 2.5009, 2.4871, 2.4765, 2.4666, 2.465, 2.4555, 2.427, 2.2707, 2.4013, 2.2221, 2.3681, 1.782, 1.9064, 1.7901, 1.9465, 1.5742, 1.2855, 1.3978, 0.8547, 0.0652, 0.7533, 0.6211, 0.6307, 0.6691, 0.0493, 0.5636, 1.749, 1.4313, -0.504, 0.0224, 0.549, 0.3125, 1.0029, -0.637, 0.2089, -0.4402, 0.558, -1.3018, 3.9241, 3.8387, 3.8082, 3.7445, 3.695, 3.5885, 3.5357, 3.5217, 3.51, 3.508, 3.4788, 3.4694, 3.4686, 3.4675, 3.4231, 3.3623, 3.2917, 3.257, 3.2153, 3.13, 3.1298, 3.1242, 3.0502, 3.0199, 3.0147, 3.0137, 2.9372, 2.918, 2.884, 2.8839, 2.8167, 2.6889, 2.2314, 2.2995, 2.3311, 1.068, 1.5702, 1.1968, 2.2467, 1.6632, 0.6151, 0.8979, 1.4582, 1.066, -0.6609, -0.7025, 0.9646, -0.3534, 0.4425, 0.5872, -0.619, 0.5127, 3.9105, 3.8125, 3.699, 3.6971, 3.6005, 3.5102, 3.4357, 3.3756, 3.3516, 3.3213, 3.3146, 3.2311, 3.2052, 3.1901, 3.1857, 3.1567, 3.1503, 3.1503, 3.1493, 3.1102, 3.0358, 3.0344, 2.9796, 2.9474, 2.917, 2.8914, 2.877, 2.813, 2.7463, 2.7385, 2.6556, 2.6542, 2.1613, 1.753, 2.5283, 1.5551, 2.1443, 0.8343, 0.769, -0.1239, -0.5655, -0.1503, 0.8897, -0.2899, -1.3726, -0.368, -0.5082, -0.9112, -1.3283, 2.5908, 2.266, 2.1938, 1.9533, 1.9125, 1.8605, 1.8604, 1.8596, 1.8594, 1.8593, 1.8591, 1.8589, 1.8589, 1.8581, 1.8581, 1.8581, 1.8581, 1.858, 1.8578, 1.8578, 1.8578, 1.8577, 1.8576, 1.8572, 1.8571, 1.857, 1.8569, 1.8565, 1.8565, 1.8565, 1.7332, 1.6868, 1.2319, 0.9524, 1.3768, 0.357, 1.092, -1.3456, 1.1905, -0.7998, 1.3583, -1.0315, -1.4349, -2.0541, -2.5985, -3.3374, -2.963, 0.5665, 1.198, -2.3528, -1.6205, -2.2945, -3.3322, -2.6348, -1.1006, -1.7253, -2.6221, -2.3599, -3.1124, -2.8985, -2.7458, -2.8028, -2.7732, -3.0471]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], \"Freq\": [0.4095377526390084, 0.27742880017481214, 0.10568716197135701, 0.06605447623209813, 0.11889805721777663, 0.013210895246419626, 0.26968915416108846, 0.13484457708054423, 0.13484457708054423, 0.13484457708054423, 0.40453373124163267, 0.18562529830135405, 0.18562529830135405, 0.18562529830135405, 0.18562529830135405, 0.3712505966027081, 0.7540806822207196, 0.07842439095095483, 0.07239174549318907, 0.018097936373297267, 0.012065290915531513, 0.06635910003542332, 0.16135507869691673, 0.16135507869691673, 0.16135507869691673, 0.16135507869691673, 0.4840652360907502, 0.2267217165355002, 0.2267217165355002, 0.2267217165355002, 0.2267217165355002, 0.2267217165355002, 0.20122686795453135, 0.6723922173114828, 0.004907972389134911, 0.10306742017183312, 0.009815944778269822, 0.8997907821529887, 0.03749128258970786, 0.03749128258970786, 0.03749128258970786, 0.8844219261554651, 0.04020099664343023, 0.04020099664343023, 0.04020099664343023, 0.11019391618620307, 0.7713574133034214, 0.01574198802660044, 0.03148397605320088, 0.07870994013300218, 0.1087083815314164, 0.1087083815314164, 0.1087083815314164, 0.6522502891884985, 0.1704438730435537, 0.1704438730435537, 0.1704438730435537, 0.1704438730435537, 0.3408877460871074, 0.0751300760684388, 0.0751300760684388, 0.0751300760684388, 0.2253902282053164, 0.5259105324790716, 0.29284783565894373, 0.23427826852715497, 0.05856956713178874, 0.05856956713178874, 0.05856956713178874, 0.29284783565894373, 0.8393991044346821, 0.04937641790792248, 0.04937641790792248, 0.04937641790792248, 0.5858125070355535, 0.1753583491455111, 0.20426357153213381, 0.02890522238662271, 0.0038540296515496945, 0.40064722706654105, 0.35706217307603866, 0.17601656419241343, 0.046937750451310246, 0.015087134073635437, 0.0016763482304039375, 0.0016763482304039375, 0.05203729022648222, 0.05203729022648222, 0.8325966436237155, 0.05203729022648222, 0.9212211863148682, 0.01842442372629736, 0.03684884745259472, 0.01842442372629736, 0.16225869458625505, 0.16225869458625505, 0.16225869458625505, 0.16225869458625505, 0.3245173891725101, 0.10921510877430159, 0.0819113315807262, 0.027303777193575397, 0.7645057614201111, 0.027303777193575397, 0.8888802748620146, 0.08187055163202767, 0.011695793090289666, 0.011695793090289666, 0.9269815142979423, 0.022070988435665295, 0.022070988435665295, 0.022070988435665295, 0.7933453194857831, 0.07646701874561765, 0.038233509372808824, 0.057350264059213237, 0.019116754686404412, 0.12560397542145452, 0.12560397542145452, 0.12560397542145452, 0.6280198771072727, 0.35392469813046484, 0.25739978045851986, 0.1823248444914516, 0.19841233077010906, 0.005362495426219164, 0.005362495426219164, 0.005362495426219164, 0.3747793368201767, 0.4041295258482629, 0.1557817725336879, 0.05644267120785794, 0.004515413696628635, 0.0022577068483143176, 0.47872071467435845, 0.12488366469765873, 0.3434300779185615, 0.04162788823255291, 0.010406972058138227, 0.04608968934350611, 0.13826906803051833, 0.7374350294960977, 0.04608968934350611, 0.14263500215239905, 0.14263500215239905, 0.14263500215239905, 0.14263500215239905, 0.14263500215239905, 0.42790500645719715, 0.2027132851888954, 0.2027132851888954, 0.2027132851888954, 0.2027132851888954, 0.4054265703777908, 0.036093249432258945, 0.8842846110903442, 0.018046624716129472, 0.054139874148388424, 0.15825338669257868, 0.15825338669257868, 0.15825338669257868, 0.15825338669257868, 0.474760160077736, 0.13713801792069624, 0.13713801792069624, 0.06856900896034812, 0.6171210806431331, 0.06856900896034812, 0.19293278078425413, 0.09646639039212707, 0.1446995855881906, 0.04823319519606353, 0.5305651471566989, 0.8959289632738315, 0.021851925933508085, 0.06555577780052425, 0.021851925933508085, 0.39266146638071575, 0.4276876065743007, 0.11429582589485622, 0.058991394010248374, 0.0036869621256405234, 0.0036869621256405234, 0.0018434810628202617, 0.24453304800800332, 0.24453304800800332, 0.24453304800800332, 0.24453304800800332, 0.02527100679676518, 0.8844852378867812, 0.02527100679676518, 0.05054201359353036, 0.2186110585198969, 0.29148141135986255, 0.07287035283996564, 0.07287035283996564, 0.3643517641998282, 0.07555362204484106, 0.07555362204484106, 0.7555362204484106, 0.07555362204484106, 0.2444785264993903, 0.2444785264993903, 0.2444785264993903, 0.2444785264993903, 0.770154355702424, 0.03883131205222305, 0.1359095921827807, 0.04530319739426023, 0.025961596866177643, 0.025961596866177643, 0.9086558903162175, 0.025961596866177643, 0.036780602356457026, 0.8827344565549686, 0.036780602356457026, 0.036780602356457026, 0.06110567380258722, 0.12221134760517444, 0.7332680856310466, 0.06110567380258722, 0.6561519620162383, 0.05214452678274741, 0.28244952007321517, 0.004345377231895617, 0.004345377231895617, 0.11361766218330592, 0.11361766218330592, 0.11361766218330592, 0.11361766218330592, 0.5680883109165296, 0.0075276728592364546, 0.8882653973899016, 0.0075276728592364546, 0.082804401451601, 0.0075276728592364546, 0.0075276728592364546, 0.09932913038358862, 0.09932913038358862, 0.09932913038358862, 0.09932913038358862, 0.6953039126851203, 0.10693130973771471, 0.10693130973771471, 0.10693130973771471, 0.10693130973771471, 0.6415878584262883, 0.1349958932923344, 0.2699917865846688, 0.1349958932923344, 0.1349958932923344, 0.40498767987700324, 0.14041809500540056, 0.14041809500540056, 0.14041809500540056, 0.14041809500540056, 0.4212542850162017, 0.12338946854990225, 0.37016840564970677, 0.12338946854990225, 0.12338946854990225, 0.37016840564970677, 0.41618341349098975, 0.24971004809459385, 0.08323668269819795, 0.08323668269819795, 0.08323668269819795, 0.1664733653963959, 0.4587238101234977, 0.3028029091455601, 0.16044034738309526, 0.06101252646962778, 0.006779169607736419, 0.009038892810315226, 0.021034937973437723, 0.33655900757500357, 0.5889782632562562, 0.042069875946875446, 0.03496962900691627, 0.8882285767756732, 0.013987851602766508, 0.048957480609682774, 0.020981777404149762, 0.4645006763138404, 0.27579727656134273, 0.08709387680884507, 0.1161251690784601, 0.02177346920221127, 0.02177346920221127, 0.007257823067403756, 0.5691094211848154, 0.09485157019746925, 0.09485157019746925, 0.09485157019746925, 0.1897031403949385, 0.08463259816091936, 0.16926519632183873, 0.08463259816091936, 0.08463259816091936, 0.5924281871264356, 0.5963298030733657, 0.2342724226359651, 0.12423537564028451, 0.03904540377266085, 0.003549582161150986, 0.003549582161150986, 0.1786549508086757, 0.1786549508086757, 0.1786549508086757, 0.1786549508086757, 0.3573099016173514, 0.3327514966901237, 0.022183433112674917, 0.08873373245069967, 0.26620119735209896, 0.1552840317887244, 0.13310059867604948, 0.22644802215449358, 0.6377515725983697, 0.06469943490128388, 0.055456658486814754, 0.013864164621703689, 0.8500928850649488, 0.07969620797483895, 0.02656540265827965, 0.02656540265827965, 0.1258065177500011, 0.1258065177500011, 0.1258065177500011, 0.6290325887500055, 0.07992335925371537, 0.07992335925371537, 0.7992335925371536, 0.07992335925371537, 0.10058567234376403, 0.7732523561426861, 0.044006231650396764, 0.07543925425782302, 0.006286604521485252, 0.23454205991585853, 0.5330501361724057, 0.04264401089379246, 0.02132200544689623, 0.17057604357516984, 0.10162619556671637, 0.10162619556671637, 0.10162619556671637, 0.7113833689670146, 0.0956579339169272, 0.0956579339169272, 0.0956579339169272, 0.6696055374184904, 0.1381754263121958, 0.46426943240897783, 0.14922946041717144, 0.23766173325697676, 0.005527017052487831, 0.12582142775289912, 0.12582142775289912, 0.12582142775289912, 0.6291071387644955, 0.46195059404864863, 0.22446895062927294, 0.23748164341937572, 0.039038078370308336, 0.03578490517278264, 0.003253173197525695, 0.003253173197525695, 0.2935977733513057, 0.4960789963522062, 0.10124061150045024, 0.05062030575022512, 0.030372183450135076, 0.0404962446001801, 0.8441373703168318, 0.09931027886080374, 0.024827569715200934, 0.024827569715200934, 0.06465089800869106, 0.06465089800869106, 0.8404616741129839, 0.06465089800869106, 0.8806787177662451, 0.05871191451774967, 0.029355957258874837, 0.029355957258874837, 0.030548170199073816, 0.8858969357731407, 0.030548170199073816, 0.030548170199073816, 0.5322232453258108, 0.20739215662930655, 0.19489865321790253, 0.05747011569245844, 0.004997401364561603, 0.6517599165806537, 0.1185018030146643, 0.08887635226099823, 0.029625450753666075, 0.029625450753666075, 0.08887635226099823, 0.24454127739220724, 0.24454127739220724, 0.24454127739220724, 0.24454127739220724, 0.11843873414665369, 0.7872692328571687, 0.04876889053097505, 0.027867937446271456, 0.013933968723135728, 0.18107572685651482, 0.28759086030152353, 0.09586362010050785, 0.4260605337800349, 0.010651513344500872, 0.20799381610894932, 0.20799381610894932, 0.20799381610894932, 0.20799381610894932, 0.20799381610894932, 0.17232541325779294, 0.17232541325779294, 0.17232541325779294, 0.17232541325779294, 0.3446508265155859, 0.1978178290400337, 0.09890891452001685, 0.09890891452001685, 0.2967267435600505, 0.2967267435600505, 0.3298058614879898, 0.11587773511740182, 0.525906643994362, 0.026741015796323497, 0.25661193379573, 0.359256707314022, 0.06598592583318771, 0.29327078148083424, 0.02199530861106257, 0.024513626320900966, 0.9070041738733358, 0.024513626320900966, 0.024513626320900966, 0.024513626320900966, 0.1531357100195489, 0.3062714200390978, 0.1531357100195489, 0.1531357100195489, 0.45940713005864675, 0.02199161491817218, 0.9236478265632316, 0.02199161491817218, 0.02199161491817218, 0.32465638500960525, 0.4571691952176074, 0.05963076459360096, 0.10601024816640171, 0.006625640510400107, 0.04637948357280075, 0.006625640510400107, 0.13331350580965826, 0.08887567053977218, 0.7110053643181774, 0.04443783526988609, 0.04443783526988609, 0.06491274043884768, 0.06491274043884768, 0.06491274043884768, 0.7789528852661722, 0.20976265950435433, 0.5699757782394179, 0.1374307079511287, 0.0795651467085482, 0.002893278062129025, 0.0014466390310645125, 0.011379145112998246, 0.5120615300849211, 0.022758290225996493, 0.44378665940693157, 0.07070337541839766, 0.21211012625519296, 0.07070337541839766, 0.636330378765579, 0.35100517691970845, 0.2594386090276106, 0.04578328394604893, 0.015261094648682976, 0.3357440822710255, 0.34699173214887163, 0.36525445489354913, 0.009131361372338727, 0.2739408411701618, 0.009131361372338727, 0.22314688444203268, 0.22314688444203268, 0.22314688444203268, 0.22314688444203268, 0.22314688444203268, 0.2231554835356802, 0.2231554835356802, 0.2231554835356802, 0.2231554835356802, 0.2231554835356802, 0.16496754707096054, 0.16496754707096054, 0.16496754707096054, 0.16496754707096054, 0.4949026412128816, 0.33089336714429307, 0.21056850636455013, 0.06016243038987147, 0.0902436455848072, 0.2707309367544216, 0.24452516403510155, 0.24452516403510155, 0.24452516403510155, 0.24452516403510155, 0.11006997872492841, 0.7154548617120347, 0.055034989362464204, 0.055034989362464204, 0.3478683826167514, 0.4711635055695241, 0.0968747394628928, 0.06825265734885629, 0.011008493120783273, 0.004403397248313309, 0.0022016986241566545, 0.1153634232445939, 0.2307268464891878, 0.05768171162229695, 0.5768171162229695, 0.14920200953292126, 0.34813802224348295, 0.3978720254211234, 0.04973400317764042, 0.03851180431403831, 0.8857714992228812, 0.03851180431403831, 0.03851180431403831, 0.46644964281511364, 0.34943650217764527, 0.10899854196366916, 0.0673226288599133, 0.004808759204279521, 0.0016029197347598406, 0.34143297257033894, 0.22762198171355927, 0.11381099085677963, 0.11381099085677963, 0.11381099085677963, 0.35974264202288064, 0.34129430140632266, 0.08301753277451092, 0.15681089524074285, 0.027672510924836972, 0.027672510924836972, 0.1584432382786129, 0.1584432382786129, 0.1584432382786129, 0.1584432382786129, 0.47532971483583875, 0.2444829123058037, 0.2444829123058037, 0.2444829123058037, 0.2444829123058037, 0.12419195869864107, 0.12419195869864107, 0.12419195869864107, 0.6209597934932054, 0.12490926902446643, 0.8289433307987318, 0.011355388093133313, 0.022710776186266626, 0.011355388093133313, 0.03720211789648204, 0.07440423579296408, 0.03720211789648204, 0.855648711619087, 0.08956730831359991, 0.08956730831359991, 0.08956730831359991, 0.08956730831359991, 0.35826923325439963, 0.35826923325439963, 0.1484395985564189, 0.1484395985564189, 0.1484395985564189, 0.1484395985564189, 0.4453187956692567, 0.3653516602481837, 0.3426590105433276, 0.19742605243224834, 0.0885013338489389, 0.004538529940971226, 0.29275367358136684, 0.5096082466046016, 0.13824479030231213, 0.05150296109301824, 0.0027106821627904338, 0.008132046488371302, 0.04546469353018741, 0.245509345063012, 0.6910633416588486, 0.009092938706037482, 0.009092938706037482, 0.4880305908552936, 0.2923883716137442, 0.15264392929835174, 0.049448033434677324, 0.015049401480119186, 0.0021499144971598837, 0.850898877648592, 0.04727215986936622, 0.04727215986936622, 0.04727215986936622, 0.23581218565123818, 0.3301370599117334, 0.18864974852099053, 0.14148731139074291, 0.09432487426049527, 0.7128292225122288, 0.10862159581138724, 0.12219929528781065, 0.04752194816748192, 0.006788849738211702, 0.16513519423730513, 0.16513519423730513, 0.16513519423730513, 0.16513519423730513, 0.33027038847461027, 0.029419913478716437, 0.8973073611008514, 0.04412987021807466, 0.014709956739358218, 0.24450784771300346, 0.24450784771300346, 0.24450784771300346, 0.24450784771300346, 0.2444975783912918, 0.2444975783912918, 0.2444975783912918, 0.2444975783912918, 0.11846420515172856, 0.7467475789028605, 0.02538518681822755, 0.1015407472729102, 0.006346296704556887, 0.002115432234852296, 0.3929202179248953, 0.20241344559767332, 0.09525338616361098, 0.0833467128931596, 0.011906673270451373, 0.20241344559767332, 0.14415952657792303, 0.7328109267711087, 0.018019940822240378, 0.07207976328896151, 0.006006646940746793, 0.024026587762987172, 0.006006646940746793, 0.287377948530544, 0.4815109243995817, 0.13298873150083684, 0.07795891156945609, 0.015286061092050213, 0.0015286061092050214, 0.0015286061092050214, 0.019713214019945805, 0.81481284615776, 0.02628428535992774, 0.1314214267996387, 0.1490269318541465, 0.298053863708293, 0.4843375285259761, 0.037256732963536626, 0.037256732963536626, 0.3769256360524365, 0.3930139253961381, 0.16547954753521604, 0.04596654098200446, 0.01608828934370156, 0.0022983270491002228, 0.03285077840098185, 0.9198217952274917, 0.03285077840098185, 0.03285077840098185, 0.09565098329952262, 0.09565098329952262, 0.09565098329952262, 0.6695568830966583, 0.5411751347861206, 0.2578885118836387, 0.16215717035107585, 0.031259213561653176, 0.0058611025428099706, 0.0019537008476033235, 0.1756196137297422, 0.3512392274594844, 0.1756196137297422, 0.1756196137297422, 0.1756196137297422, 0.09296958447670005, 0.27890875343010013, 0.09296958447670005, 0.09296958447670005, 0.46484792238350026, 0.14665168413235505, 0.14665168413235505, 0.14665168413235505, 0.14665168413235505, 0.43995505239706517, 0.19802174503172154, 0.19802174503172154, 0.19802174503172154, 0.19802174503172154, 0.19802174503172154, 0.5763734627254802, 0.22459623195406037, 0.10914114082908154, 0.08027736804783683, 0.008117936094725074, 0.0009019928994138971, 0.0009019928994138971, 0.9420312112762727, 0.020934026917250503, 0.010467013458625251, 0.010467013458625251, 0.38488138593344945, 0.12829379531114982, 0.12829379531114982, 0.12829379531114982, 0.12829379531114982, 0.25658759062229963, 0.05016291129573228, 0.05016291129573228, 0.8527694920274488, 0.05016291129573228, 0.20798408627015133, 0.20798408627015133, 0.20798408627015133, 0.20798408627015133, 0.20798408627015133, 0.1386558944015812, 0.1386558944015812, 0.1386558944015812, 0.1386558944015812, 0.5546235776063247, 0.5081295075136579, 0.29338429898110013, 0.128544667079348, 0.055954737434539714, 0.010586031406534541, 0.0015122902009335058, 0.0015122902009335058, 0.20804138304272018, 0.13461501255705424, 0.5690543712639111, 0.07342637048566596, 0.006118864207138829, 0.881014536094359, 0.055063408505897436, 0.027531704252948718, 0.027531704252948718, 0.027531704252948718, 0.7475952743648235, 0.10339083581641177, 0.14315654189964708, 0.00795314121664706, 0.05964041118751305, 0.2385616447500522, 0.05964041118751305, 0.5964041118751305, 0.05896834851700603, 0.05896834851700603, 0.8255568792380844, 0.05896834851700603, 0.10874754202330353, 0.10874754202330353, 0.10874754202330353, 0.6524852521398211, 0.1932579642756379, 0.22546762498824421, 0.06441932142521263, 0.45093524997648843, 0.06441932142521263, 0.37898608947489343, 0.1768601750882836, 0.02526573929832623, 0.02526573929832623, 0.3537203501765672, 0.05053147859665246, 0.2876673039252592, 0.09588910130841972, 0.09588910130841972, 0.09588910130841972, 0.4794455065420986, 0.04108852980844141, 0.846423714053893, 0.024653117885064846, 0.016435411923376563, 0.008217705961688282, 0.057523941731817976, 0.046644847774079304, 0.046644847774079304, 0.8396072599334274, 0.046644847774079304, 0.2116378641839298, 0.2116378641839298, 0.2116378641839298, 0.2116378641839298, 0.2116378641839298, 0.19974789565631013, 0.7764847774808675, 0.014066753215233107, 0.005626701286093243, 0.004220025964569932, 0.20346397128799504, 0.325542354060792, 0.23059250079306104, 0.162771177030396, 0.05425705901013201, 0.1647344972533198, 0.1647344972533198, 0.1647344972533198, 0.1647344972533198, 0.1647344972533198, 0.1647344972533198, 0.07608441406376344, 0.4925464699917318, 0.3243598704823599, 0.08809774260014715, 0.016017771381844934, 0.3455494992887828, 0.4920624869872267, 0.08016748383499761, 0.06910989985775656, 0.0027643959943102624, 0.0027643959943102624, 0.008293187982930788, 0.09627213901368527, 0.19254427802737054, 0.09627213901368527, 0.09627213901368527, 0.5776328340821115, 0.25549290522193935, 0.2897107050284491, 0.06387322630548484, 0.2942730783359837, 0.054748479690415575, 0.038780173114044365, 0.05214106848508993, 0.05214106848508993, 0.8342570957614389, 0.05214106848508993, 0.08508217583245513, 0.08508217583245513, 0.7657395824920962, 0.08508217583245513, 0.8347923912152715, 0.11025559883975285, 0.015750799834250408, 0.031501599668500815, 0.28431342295536455, 0.3316989934479253, 0.10424825508363367, 0.07581691278809721, 0.19901939606875518, 0.039890811897416716, 0.8775978617431677, 0.039890811897416716, 0.039890811897416716, 0.2444575869338395, 0.2444575869338395, 0.2444575869338395, 0.2444575869338395, 0.1827752203662173, 0.1827752203662173, 0.1827752203662173, 0.1827752203662173, 0.3655504407324346, 0.17613884437166213, 0.17613884437166213, 0.17613884437166213, 0.17613884437166213, 0.35227768874332427, 0.036104610199106636, 0.8845629498781127, 0.036104610199106636, 0.018052305099553318, 0.018052305099553318, 0.2545630144973547, 0.381844521746032, 0.3454783768178385, 0.013637304348072572, 0.004545768116024191, 0.342845422059996, 0.171422711029998, 0.171422711029998, 0.171422711029998, 0.24454046956800768, 0.24454046956800768, 0.24454046956800768, 0.24454046956800768, 0.12513422789226689, 0.12513422789226689, 0.6256711394613345, 0.12513422789226689, 0.2199163652199101, 0.40318000290316847, 0.05497909130497752, 0.01832636376832584, 0.2748954565248876, 0.19064308431979685, 0.19064308431979685, 0.19064308431979685, 0.19064308431979685, 0.3812861686395937, 0.4647093668172737, 0.34430739450552555, 0.12673891822289285, 0.05914482850401666, 0.0021123153037148805, 0.13621943009065685, 0.5513643598907538, 0.025946558112506065, 0.1621659882031629, 0.11675951150627728, 0.006486639528126516, 0.17965848844158827, 0.17965848844158827, 0.17965848844158827, 0.17965848844158827, 0.35931697688317654, 0.4516893983266268, 0.3448631138894811, 0.07315278173413237, 0.085925489655965, 0.030190036906149863, 0.006966931593726891, 0.006966931593726891, 0.49048033613155856, 0.3068202983730455, 0.10371390367539565, 0.06266048347055153, 0.021607063265707428, 0.006482118979712228, 0.008642825306282971, 0.036065587551084716, 0.9016396887771179, 0.036065587551084716, 0.036065587551084716, 0.6913473854677569, 0.1557965939082269, 0.1330762572966105, 0.012983049492352242, 0.0032457623730880606, 0.2445435311467101, 0.2445435311467101, 0.2445435311467101, 0.2445435311467101, 0.20502950774157308, 0.20502950774157308, 0.41005901548314616, 0.20502950774157308, 0.24451669703204304, 0.24451669703204304, 0.24451669703204304, 0.24451669703204304, 0.5824872912163115, 0.21460058097443055, 0.013138811080067175, 0.0175184147734229, 0.17518414773422902, 0.02427806802798765, 0.9225665850635307, 0.02427806802798765, 0.02427806802798765, 0.057260841960491304, 0.7689313063265975, 0.10634156364091242, 0.049080721680421116, 0.008180120280070187, 0.008180120280070187, 0.24460449613017773, 0.24460449613017773, 0.24460449613017773, 0.24460449613017773, 0.2124195501419159, 0.2124195501419159, 0.2124195501419159, 0.2124195501419159, 0.2124195501419159, 0.15289951651565428, 0.15289951651565428, 0.15289951651565428, 0.15289951651565428, 0.15289951651565428, 0.5882227624068939, 0.12738582115818084, 0.25102500051759163, 0.03371977618893022, 0.24465456652932266, 0.24465456652932266, 0.24465456652932266, 0.24465456652932266, 0.8754226259966558, 0.04168679171412647, 0.04168679171412647, 0.04168679171412647, 0.10209212615712866, 0.20418425231425733, 0.10209212615712866, 0.10209212615712866, 0.5104606307856433, 0.4278100494872163, 0.3608737832409171, 0.08730817336473802, 0.06402599380080788, 0.05238490401884281, 0.0029102724454912673, 0.0029102724454912673, 0.1405841769567635, 0.7029208847838175, 0.09732750712391319, 0.04325666983285031, 0.005407083729106288, 0.005407083729106288, 0.8111149190026712, 0.13726560167737514, 0.024957382123159115, 0.012478691061579558, 0.012478691061579558, 0.1733319233473572, 0.07878723788516237, 0.630297903081299, 0.11030213303922731, 0.5410988106062137, 0.17076502387952752, 0.22631509188853047, 0.057607477935262295, 0.002057409926259368, 0.47406914961484203, 0.2787240728891282, 0.16437573529358845, 0.059556425831010305, 0.019058056265923298, 0.002382257033240412, 0.002382257033240412, 0.35856175805968643, 0.2679216110408809, 0.31324168455028367, 0.05465067687898566, 0.0026658866770236907, 0.0013329433385118454, 0.0026658866770236907, 0.3697720164027799, 0.0528245737718257, 0.5458539289755322, 0.017608191257275235, 0.017608191257275235, 0.12208874474671577, 0.12208874474671577, 0.24417748949343154, 0.12208874474671577, 0.24417748949343154, 0.1968625489474007, 0.1968625489474007, 0.1968625489474007, 0.1968625489474007, 0.1968625489474007, 0.8657779975015796, 0.034631119900063186, 0.034631119900063186, 0.034631119900063186, 0.034631119900063186, 0.1018380008763581, 0.1018380008763581, 0.1018380008763581, 0.7128660061345067, 0.8920115550203344, 0.03185755553644051, 0.03185755553644051, 0.03185755553644051, 0.1056052209887504, 0.6201055340817265, 0.14306125996012986, 0.09884232506336245, 0.031213365809482876, 0.0005202227634913813, 0.0010404455269827626, 0.4254849419921137, 0.14182831399737122, 0.14182831399737122, 0.07091415699868561, 0.14182831399737122, 0.06444617570317308, 0.06444617570317308, 0.7733541084380771, 0.06444617570317308, 0.03669882764849313, 0.3302894488364382, 0.03669882764849313, 0.5871812423758901, 0.24446466836657887, 0.24446466836657887, 0.24446466836657887, 0.24446466836657887, 0.24446435466876607, 0.24446435466876607, 0.24446435466876607, 0.24446435466876607, 0.3245488878159893, 0.1352287032566622, 0.5138690723753164, 0.01352287032566622, 0.01352287032566622, 0.019292770706802347, 0.9067602232197104, 0.019292770706802347, 0.03858554141360469, 0.019292770706802347, 0.8733391858325993, 0.029111306194419977, 0.029111306194419977, 0.05822261238883995, 0.1617734634049019, 0.1617734634049019, 0.1617734634049019, 0.1617734634049019, 0.3235469268098038, 0.027917023644605424, 0.921261780271979, 0.027917023644605424, 0.027917023644605424, 0.4287559255219477, 0.1473848493981695, 0.013398622672560865, 0.3885600575042651, 0.013398622672560865, 0.013398622672560865, 0.2851116601156135, 0.4339567179700882, 0.027253320452227763, 0.18029119683781444, 0.0691815057633474, 0.0041928185311119635, 0.08114656026259212, 0.08114656026259212, 0.08114656026259212, 0.7303190423633291, 0.01317630489404949, 0.6522270922554497, 0.006588152447024745, 0.30305501256313827, 0.006588152447024745, 0.019764457341074234, 0.125332111327282, 0.125332111327282, 0.125332111327282, 0.62666055663641, 0.08120910855794572, 0.08120910855794572, 0.08120910855794572, 0.7308819770215115, 0.17168365561974566, 0.3433673112394913, 0.17168365561974566, 0.08584182780987283, 0.3433673112394913, 0.4506981168775709, 0.30281279727711796, 0.17605395190530113, 0.02816863230484818, 0.02816863230484818, 0.27628197160043105, 0.23223701960615944, 0.11211442325814594, 0.37238004867884184, 0.0040040865449337834, 0.45506304342318343, 0.3381371225436155, 0.13588688102220062, 0.04898248036846766, 0.018960960142632644, 0.0015800800118860538, 0.24169117629339273, 0.6954786909667016, 0.03945978388463555, 0.004932472985579444, 0.01479741895673833, 0.23919031106096758, 0.1594602073739784, 0.1594602073739784, 0.0797301036869892, 0.398650518434946, 0.26196778428955214, 0.4685193065178529, 0.146097418161481, 0.06549194607238804, 0.05037842005568311, 0.48821577379724573, 0.20923533162739105, 0.06974511054246368, 0.06974511054246368, 0.06974511054246368, 0.24453831973979576, 0.24453831973979576, 0.24453831973979576, 0.24453831973979576, 0.24459220867127943, 0.24459220867127943, 0.24459220867127943, 0.24459220867127943, 0.6160239079329224, 0.1872712680116084, 0.14784573790390138, 0.019712765053853516, 0.004928191263463379, 0.024640956317316897, 0.3975102052875271, 0.14517764019196644, 0.4355329205758993, 0.017283052403805527, 0.0034566104807611056, 0.36684088192620096, 0.18342044096310048, 0.07860876041275736, 0.026202920137585784, 0.34063796178861516, 0.11559133165416025, 0.11559133165416025, 0.11559133165416025, 0.5779566582708012, 0.019647613952748326, 0.9234378557791714, 0.019647613952748326, 0.019647613952748326, 0.05922821797982377, 0.05922821797982377, 0.769966833737709, 0.05922821797982377, 0.0554413747169096, 0.0831620620753644, 0.831620620753644, 0.0277206873584548, 0.33011513058069786, 0.3787636761399586, 0.04169875333650921, 0.20849376668254602, 0.04169875333650921, 0.003474896111375767, 0.29649998467754, 0.4516224932174451, 0.17083111699964226, 0.056943705666547426, 0.019635760574671524, 0.003927152114934305, 0.0786489545476534, 0.1572979090953068, 0.5505426818335738, 0.1572979090953068, 0.10607684683777786, 0.42430738735111145, 0.10607684683777786, 0.10607684683777786, 0.21215369367555573, 0.049208689549144105, 0.8037419293026871, 0.021870528688508493, 0.11482027561466958, 0.010935264344254247, 0.9279730752400148, 0.02263348964000036, 0.02263348964000036, 0.02263348964000036, 0.8994949497608763, 0.016354453632015934, 0.06541781452806374, 0.016354453632015934, 0.21695491436299424, 0.21695491436299424, 0.21695491436299424, 0.21695491436299424, 0.21695491436299424, 0.02791488713736783, 0.9211912755331384, 0.02791488713736783, 0.02791488713736783, 0.24867472329721949, 0.5845137825955262, 0.07690970823625345, 0.0794733651774619, 0.005127313882416897, 0.0025636569412084485, 0.09970450369385263, 0.12463062961731577, 0.2741873851580947, 0.4735963925458, 0.24451553963031725, 0.24451553963031725, 0.24451553963031725, 0.24451553963031725, 0.46669032555880846, 0.22401135626822805, 0.11200567813411402, 0.05600283906705701, 0.13067329115646636, 0.2445299633023284, 0.2445299633023284, 0.2445299633023284, 0.2445299633023284, 0.1782272558891148, 0.1782272558891148, 0.1782272558891148, 0.1782272558891148, 0.3564545117782296, 0.9613448619210982, 0.013169107697549292, 0.013169107697549292, 0.013169107697549292, 0.08509665496514095, 0.08509665496514095, 0.7658698946862684, 0.08509665496514095, 0.5279723099182876, 0.27495043615552994, 0.11976368691437195, 0.07253293714532386, 0.003373624983503435, 0.7905157821047525, 0.009524286531382561, 0.009524286531382561, 0.009524286531382561, 0.18096144409626866, 0.18566343109318495, 0.18566343109318495, 0.18566343109318495, 0.18566343109318495, 0.3713268621863699, 0.10567200331973374, 0.10567200331973374, 0.10567200331973374, 0.10567200331973374, 0.6340320199184024, 0.16499147980003764, 0.16499147980003764, 0.16499147980003764, 0.16499147980003764, 0.49497443940011293, 0.16759993217092778, 0.3770998473845875, 0.12569994912819585, 0.08379996608546389, 0.12569994912819585, 0.12569994912819585, 0.2369896665147979, 0.5645930290499597, 0.024395995082405662, 0.06621770093795823, 0.10455426463888141, 0.15870805249380168, 0.15870805249380168, 0.15870805249380168, 0.15870805249380168, 0.476124157481405, 0.8907082249643509, 0.03872644456366743, 0.03872644456366743, 0.03872644456366743, 0.8934712913577291, 0.0638193779541235, 0.03190968897706175, 0.03190968897706175, 0.5169729831271976, 0.28639817844605314, 0.13106357318717687, 0.043687857729058956, 0.009708412828679768, 0.009708412828679768, 0.002427103207169942, 0.4318279922893258, 0.1734693644239172, 0.31126063261880177, 0.07750758835962258, 0.004921116721245878, 0.0012302791803114695, 0.4489967935763761, 0.15846945655636804, 0.16727331525394404, 0.044019293487880015, 0.17607717395152006, 0.8368225752790077, 0.11411216935622832, 0.038037389785409444, 0.012679129928469814, 0.7659518121190961, 0.067869147909287, 0.12604270326010442, 0.009695592558469572, 0.019391185116939144, 0.1546583957314004, 0.1546583957314004, 0.1546583957314004, 0.1546583957314004, 0.46397518719420117, 0.05388381600803837, 0.05388381600803837, 0.05388381600803837, 0.8082572401205755, 0.8593140090442655, 0.04522705310759292, 0.04522705310759292, 0.04522705310759292, 0.2445975686787566, 0.2445975686787566, 0.2445975686787566, 0.2445975686787566, 0.8399877138345672, 0.10105115354400808, 0.03157848548250253, 0.006315697096500505, 0.01263139419300101, 0.6498445157135548, 0.1752389705294979, 0.12777841601109224, 0.04380974263237448, 0.0036508118860312066, 0.32699380646022824, 0.184822586260129, 0.4691650266603275, 0.007108561010004962, 0.007108561010004962, 0.3421477402016947, 0.526727442152609, 0.10354471085051288, 0.018007775800089196, 0.004501943950022299, 0.019935386854832027, 0.9369631821771053, 0.019935386854832027, 0.019935386854832027, 0.019447185554389672, 0.11668311332633803, 0.8167817932843663, 0.019447185554389672, 0.022320557634530256, 0.9151428630157404, 0.022320557634530256, 0.022320557634530256, 0.24457362402610658, 0.24457362402610658, 0.24457362402610658, 0.24457362402610658, 0.8832464748979534, 0.01447945040816317, 0.05791780163265268, 0.02895890081632634, 0.7600844656900161, 0.06756306361689031, 0.016890765904222577, 0.033781531808445155, 0.016890765904222577, 0.10134459542533547, 0.3010981044242854, 0.11655410493843306, 0.2816724202678799, 0.29138526234608264, 0.009712842078202754, 0.03225153749805719, 0.03225153749805719, 0.32251537498057187, 0.5482761374669722, 0.06450307499611438, 0.08547017934378821, 0.08547017934378821, 0.08547017934378821, 0.7692316140940939, 0.3380425845206728, 0.36109094255617324, 0.12676596919525232, 0.14981432723075272, 0.02304835803550042, 0.31545269309468105, 0.5137372430399091, 0.049571137486307024, 0.07210347270735566, 0.049571137486307024, 0.1273126974317974, 0.7638761845907844, 0.010609391452649783, 0.07957043589487338, 0.010609391452649783, 0.41135110368092503, 0.3783941903296343, 0.12084201562139935, 0.07079633238425416, 0.015868143465436278, 0.0012206264204181751, 0.0012206264204181751, 0.5703875696576981, 0.045631005572615846, 0.022815502786307923, 0.06844650835892377, 0.2737860334356951, 0.1464779132478269, 0.1464779132478269, 0.1464779132478269, 0.1464779132478269, 0.43943373974348077, 0.20785099154306783, 0.20785099154306783, 0.20785099154306783, 0.20785099154306783, 0.20785099154306783, 0.3445345951915587, 0.17226729759577936, 0.17226729759577936, 0.17226729759577936, 0.17226729759577936, 0.2759328598417409, 0.03941897997739155, 0.11825693993217465, 0.5124467397060901, 0.023739588425332494, 0.9258439485879673, 0.023739588425332494, 0.023739588425332494, 0.030193381331626322, 0.9058014399487896, 0.030193381331626322, 0.030193381331626322, 0.019599806137703422, 0.9407906946097643, 0.019599806137703422, 0.019599806137703422, 0.06457087001128903, 0.06457087001128903, 0.7748504401354683, 0.06457087001128903, 0.06162285552656627, 0.06162285552656627, 0.8010971218453615, 0.06162285552656627, 0.5563511145690835, 0.32726536151122554, 0.09566218259558901, 0.01762198100445061, 0.0025174258577786582, 0.018969670738256332, 0.9484835369128167, 0.018969670738256332, 0.018969670738256332, 0.5052360768341808, 0.2864740641843293, 0.15104996111537364, 0.0416689547904479, 0.01302154837201497, 0.002604309674402994, 0.0486402388201108, 0.8430974728819206, 0.0243201194100554, 0.008106706470018467, 0.08106706470018467, 0.04324092024006379, 0.8388738526572375, 0.04324092024006379, 0.008648184048012757, 0.060537288336089304, 0.036119554849458564, 0.9029888712364642, 0.036119554849458564, 0.036119554849458564, 0.02096132837045408, 0.9013371199295254, 0.02096132837045408, 0.04192265674090816, 0.4006990984190476, 0.31323685866260575, 0.18916251854300217, 0.07729221187778583, 0.017289047393715253, 0.002034005575731206, 0.07233768107417061, 0.03616884053708531, 0.8318833323529621, 0.03616884053708531, 0.12403950957544113, 0.062019754787720566, 0.062019754787720566, 0.062019754787720566, 0.7442370574526468, 0.21687926352291426, 0.21687926352291426, 0.21687926352291426, 0.21687926352291426, 0.21687926352291426, 0.2027415356780382, 0.2027415356780382, 0.2027415356780382, 0.2027415356780382, 0.4054830713560764, 0.4378105315783918, 0.326437677054064, 0.13057507082162562, 0.08192945620180431, 0.012801477531531923, 0.0064007387657659615, 0.0025602955063063846, 0.09560960999652905, 0.6480206899764747, 0.08498631999691471, 0.12747947999537207, 0.03186986999884302, 0.005311644999807169, 0.18506871371094324, 0.18506871371094324, 0.18506871371094324, 0.18506871371094324, 0.3701374274218865, 0.19889593706521766, 0.19889593706521766, 0.19889593706521766, 0.19889593706521766, 0.3977918741304353, 0.13921174551017346, 0.13921174551017346, 0.06960587275508673, 0.5568469820406938, 0.38473082380925366, 0.2893175795045587, 0.28316188632361067, 0.021544926133318203, 0.018467079542844173, 0.6836467683945685, 0.06836467683945685, 0.06836467683945685, 0.06836467683945685, 0.1367293536789137, 0.03869533174765147, 0.03869533174765147, 0.8512972984483324, 0.03869533174765147, 0.5013907363958374, 0.1717727522837591, 0.2831929159272785, 0.03249754772935983, 0.004642506818479976, 0.037262462009016395, 0.8942990882163936, 0.037262462009016395, 0.037262462009016395, 0.11245673997028995, 0.8309303564471424, 0.018742789995048324, 0.012495193330032217, 0.012495193330032217, 0.006247596665016108, 0.45395148067906316, 0.4743210984018416, 0.02327956311174683, 0.043649180834525306, 0.0029099453889683538, 0.13133571898533886, 0.06566785949266943, 0.06566785949266943, 0.7223464544193637, 0.15950461248315662, 0.16747984310731445, 0.6380184499326265, 0.015950461248315662, 0.007975230624157831, 0.0800644108392634, 0.0800644108392634, 0.8006441083926339, 0.0800644108392634, 0.11134697759534376, 0.13361637311441252, 0.7348900521292688, 0.02226939551906875, 0.02226939551906875, 0.060938849160053314, 0.9039262625407908, 0.010156474860008886, 0.010156474860008886, 0.010156474860008886, 0.023546775110430282, 0.9065508417515659, 0.011773387555215141, 0.03532016266564542, 0.011773387555215141, 0.8753473221597333, 0.03126240436284762, 0.03126240436284762, 0.03126240436284762, 0.055768974892274024, 0.055768974892274024, 0.055768974892274024, 0.8365346233841103, 0.05421076407586348, 0.05421076407586348, 0.8131614611379521, 0.05421076407586348, 0.06687091224912108, 0.06687091224912108, 0.06687091224912108, 0.7355800347403318, 0.5643100161052462, 0.22806311531714613, 0.0847926967204774, 0.07602103843904871, 0.017543316562857396, 0.02631497484428609, 0.3659897108114617, 0.2792365941746708, 0.28736969885936997, 0.03795448852859603, 0.024399314054097447, 0.0027110348948997166, 0.15952244211034808, 0.053174147370116026, 0.7444380631816244, 0.053174147370116026, 0.09855853535607596, 0.19711707071215193, 0.09855853535607596, 0.19711707071215193, 0.39423414142430385, 0.009180698694961695, 0.8997084721062462, 0.009180698694961695, 0.03672279477984678, 0.03672279477984678, 0.22292900665207835, 0.18046633871834913, 0.4670893472710213, 0.10615666983432302, 0.021231333966864604, 0.8028330396141053, 0.15782187958226002, 0.027447283405610437, 0.006861820851402609, 0.006861820851402609, 0.02782210460961307, 0.19475473226729148, 0.7233747198499398, 0.02782210460961307, 0.1874590876465254, 0.49621523200550843, 0.20951309795788134, 0.02205401031135593, 0.06616203093406779, 0.8688343907352115, 0.04344171953676058, 0.04344171953676058, 0.04344171953676058, 0.009213503134423434, 0.9582043259800371, 0.009213503134423434, 0.018427006268846868, 0.04515675533239805, 0.04515675533239805, 0.857978351315563, 0.04515675533239805, 0.06465732031407301, 0.06465732031407301, 0.7758878437688761, 0.06465732031407301, 0.7043644528724335, 0.18162856960861493, 0.03986968601164718, 0.031009755786836696, 0.044299651124052424, 0.075548369521056, 0.075548369521056, 0.7554836952105599, 0.075548369521056, 0.07648733431853373, 0.5927768409686364, 0.038243667159266866, 0.09560916789816716, 0.21034016937596775, 0.03654144833921875, 0.03654144833921875, 0.8769947601412499, 0.03654144833921875, 0.08986750025859229, 0.08986750025859229, 0.08986750025859229, 0.7189400020687383, 0.24454209334812554, 0.24454209334812554, 0.24454209334812554, 0.24454209334812554, 0.06336197750402466, 0.06336197750402466, 0.06336197750402466, 0.1267239550080493, 0.6969817525442712, 0.11077442830984445, 0.11077442830984445, 0.11077442830984445, 0.11077442830984445, 0.6646465698590667, 0.07168315612367408, 0.14336631224734817, 0.07168315612367408, 0.28673262449469633, 0.43009893674204447, 0.3155685949710895, 0.439638298976817, 0.03236600974062456, 0.15373854626796668, 0.056640517046092985, 0.033191684847690604, 0.16595842423845303, 0.09957505454307182, 0.033191684847690604, 0.6638336969538121, 0.2408552431409517, 0.7053617834842156, 0.006881578375455762, 0.04128947025273458, 0.003440789187727881, 0.3097126375960613, 0.317328522127112, 0.05077256354033792, 0.30717400941904444, 0.010154512708067585, 0.002538628177016896, 0.10826246205856538, 0.10826246205856538, 0.10826246205856538, 0.6495747723513923, 0.1278858836819201, 0.2557717673638402, 0.2557717673638402, 0.1278858836819201, 0.14726973215046157, 0.14726973215046157, 0.14726973215046157, 0.14726973215046157, 0.44180919645138467, 0.9155538638457161, 0.0217989015201361, 0.0217989015201361, 0.0217989015201361, 0.9096508899391427, 0.019775019346503102, 0.039550038693006204, 0.019775019346503102, 0.131260751997064, 0.39378225599119193, 0.131260751997064, 0.131260751997064, 0.131260751997064, 0.5159362337973967, 0.2837649285885682, 0.06800977627329322, 0.07035494097237228, 0.05393878807881875, 0.002345164699079076, 0.8376139785901581, 0.0329193266694822, 0.11338879186154978, 0.010973108889827398, 0.08087133459612865, 0.1617426691922573, 0.08087133459612865, 0.08087133459612865, 0.5660993421729006, 0.05139047968697301, 0.15417143906091904, 0.05139047968697301, 0.05139047968697301, 0.6680762359306491, 0.4583469664566996, 0.3731414406410311, 0.09548895134514575, 0.06170055317686341, 0.005876243159701277, 0.0029381215798506386, 0.0029381215798506386, 0.08506180777211649, 0.08506180777211649, 0.7655562699490484, 0.08506180777211649, 0.018621783570293034, 0.8659129360186261, 0.02793267535543955, 0.009310891785146517, 0.07448713428117214, 0.30962650419238774, 0.33428702222540974, 0.15618328087580619, 0.1370028779612335, 0.06302132386216741, 0.08510882196898042, 0.08510882196898042, 0.7659793977208238, 0.08510882196898042, 0.35486683965312377, 0.07097336793062475, 0.07097336793062475, 0.07097336793062475, 0.4968135755143733, 0.8445969682480807, 0.019195385642001836, 0.07678154256800734, 0.03839077128400367, 0.019195385642001836, 0.18045374020521654, 0.4210587271455053, 0.12030249347014436, 0.22055457136193135, 0.020050415578357395, 0.020050415578357395, 0.035145316168417685, 0.035145316168417685, 0.035145316168417685, 0.8786329042104422, 0.27865762059533594, 0.6599785750942168, 0.024443650929415434, 0.029332381115298522, 0.004888730185883087, 0.13030663016542754, 0.13030663016542754, 0.13030663016542754, 0.13030663016542754, 0.5212265206617102, 0.19318467629379132, 0.19318467629379132, 0.19318467629379132, 0.19318467629379132, 0.38636935258758265, 0.07040786585415676, 0.07040786585415676, 0.07040786585415676, 0.7744865243957243, 0.07721910451089568, 0.07721910451089568, 0.07721910451089568, 0.7721910451089568, 0.24468842575803504, 0.16312561717202337, 0.08156280858601168, 0.08156280858601168, 0.32625123434404674, 0.2840603977053653, 0.33570774274270443, 0.08392693568567611, 0.15494203511201743, 0.14203019885268264, 0.038495497075464534, 0.8853964327356842, 0.038495497075464534, 0.038495497075464534, 0.8769224216434732, 0.03507689686573893, 0.03507689686573893, 0.03507689686573893, 0.24464961494700174, 0.24464961494700174, 0.24464961494700174, 0.24464961494700174, 0.09909979623082477, 0.19819959246164953, 0.09909979623082477, 0.5945987773849486, 0.4014391617985374, 0.391403182753574, 0.11684747030921715, 0.07742040977543221, 0.007885412106756985, 0.0014337112921376336, 0.002867422584275267, 0.8095283914698843, 0.12528415582272018, 0.02891172826678158, 0.02891172826678158, 0.6197989686645372, 0.2322431732700842, 0.07402751147983934, 0.05806079331752105, 0.0029030396658760523, 0.0014515198329380261, 0.010160638830566183, 0.02619076423670702, 0.8904859840480387, 0.02619076423670702, 0.02619076423670702, 0.3768946384744216, 0.27472440515304225, 0.1725541718316629, 0.1453087762792951, 0.029515845181731813, 0.2617690091636453, 0.2982949174190377, 0.14610363302156948, 0.28611961466724023, 0.09223459435581857, 0.11529324294477322, 0.6917594576686393, 0.06917594576686394, 0.023058648588954643, 0.24461338829706142, 0.24461338829706142, 0.24461338829706142, 0.24461338829706142, 0.20200700345443035, 0.20200700345443035, 0.20200700345443035, 0.20200700345443035, 0.4040140069088607, 0.14357020873038873, 0.8295167615533571, 0.010634830276325091, 0.0053174151381625455, 0.0053174151381625455, 0.0053174151381625455, 0.2445773759165937, 0.2445773759165937, 0.2445773759165937, 0.2445773759165937, 0.29731257320358057, 0.38879336495852845, 0.022870197938736965, 0.04574039587747393, 0.22870197938736966, 0.21174053100281393, 0.21174053100281393, 0.21174053100281393, 0.21174053100281393, 0.21174053100281393, 0.35962651311243204, 0.3463070126267864, 0.03995850145693689, 0.11987550437081068, 0.11987550437081068, 0.17584388014997487, 0.17584388014997487, 0.17584388014997487, 0.17584388014997487, 0.17584388014997487, 0.2538864282239387, 0.34950599210048705, 0.16486131702853163, 0.18134744873138478, 0.02637781072456506, 0.02637781072456506, 0.42494956624898944, 0.3912234101974823, 0.11017210976825652, 0.060707080892712775, 0.0067452312103014195, 0.0022484104034338065, 0.8404484491610408, 0.14407687699903557, 0.008004270944390865, 0.008004270944390865, 0.8440711746225673, 0.04019386545821749, 0.04019386545821749, 0.04019386545821749, 0.19507552745122306, 0.19507552745122306, 0.19507552745122306, 0.19507552745122306], \"Term\": [\"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstract\", \"abstraction\", \"abstraction\", \"abstraction\", \"abstraction\", \"abstraction\", \"accurate_refinement\", \"accurate_refinement\", \"accurate_refinement\", \"accurate_refinement\", \"accurate_refinement\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action_biase\", \"action_biase\", \"action_biase\", \"action_biase\", \"action_biase\", \"action_biasing\", \"action_biasing\", \"action_biasing\", \"action_biasing\", \"action_biasing\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity_shape\", \"activity_shape\", \"activity_shape\", \"activity_shape\", \"ad\", \"ad\", \"ad\", \"ad\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adiabatic\", \"adiabatic\", \"adiabatic\", \"adiabatic\", \"advice\", \"advice\", \"advice\", \"advice\", \"advice\", \"advise\", \"advise\", \"advise\", \"advise\", \"advise\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"alexnet\", \"alexnet\", \"alexnet\", \"alexnet\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"altproj\", \"altproj\", \"altproj\", \"altproj\", \"amp\", \"amp\", \"amp\", \"amp\", \"amr\", \"amr\", \"amr\", \"amr\", \"amr\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"analogy\", \"analogy\", \"analogy\", \"analogy\", \"analogy_question\", \"analogy_question\", \"analogy_question\", \"analogy_question\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"association\", \"association\", \"association\", \"association\", \"associative\", \"associative\", \"associative\", \"associative\", \"associative\", \"associative\", \"associative_ltp\", \"associative_ltp\", \"associative_ltp\", \"associative_ltp\", \"associative_ltp\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"auto_encoder\", \"auto_encoder\", \"auto_encoder\", \"auto_encoder\", \"auto_encoder\", \"autoencoder\", \"autoencoder\", \"autoencoder\", \"autoencoder\", \"autoencoder\", \"ball\", \"ball\", \"ball\", \"ball\", \"ball\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"befixed\", \"befixed\", \"befixed\", \"befixed\", \"behavioral\", \"behavioral\", \"behavioral\", \"behavioral\", \"bengio\", \"bengio\", \"bengio\", \"bengio\", \"bengio\", \"best_tune\", \"best_tune\", \"best_tune\", \"best_tune\", \"binaryvalue\", \"binaryvalue\", \"binaryvalue\", \"binaryvalue\", \"bind\", \"bind\", \"bind\", \"bind\", \"biomarker\", \"biomarker\", \"biomarker\", \"biomarker\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"block_diagonal\", \"block_diagonal\", \"block_diagonal\", \"block_diagonal\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bql\", \"bql\", \"bql\", \"bql\", \"bql\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brick\", \"brick\", \"brick\", \"brick\", \"brick\", \"bridging_test\", \"bridging_test\", \"bridging_test\", \"bridging_test\", \"bridging_test\", \"burst\", \"burst\", \"burst\", \"burst\", \"burst\", \"cae\", \"cae\", \"cae\", \"cae\", \"cae\", \"calibration\", \"calibration\", \"calibration\", \"calibration\", \"calibration\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"causal\", \"causal\", \"causal\", \"causal\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"chart\", \"chart\", \"chart\", \"chart\", \"chart\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"cifar\", \"cifar\", \"cifar\", \"cifar\", \"cifar\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"clause\", \"clause\", \"clause\", \"clause\", \"climate\", \"climate\", \"climate\", \"climate\", \"clmw\", \"clmw\", \"clmw\", \"clmw\", \"code\", \"code\", \"code\", \"code\", \"code\", \"collection\", \"collection\", \"collection\", \"collection\", \"collection\", \"collision\", \"collision\", \"collision\", \"collision\", \"com\", \"com\", \"com\", \"com\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component_score\", \"component_score\", \"component_score\", \"component_score\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confounder\", \"confounder\", \"confounder\", \"confounder\", \"confusion\", \"confusion\", \"confusion\", \"confusion\", \"connectivity\", \"connectivity\", \"connectivity\", \"connectivity\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"consistency\", \"constantt\", \"constantt\", \"constantt\", \"constantt\", \"context\", \"context\", \"context\", \"context\", \"context\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"contribution\", \"control_share\", \"control_share\", \"control_share\", \"control_share\", \"control_share\", \"control_sharing\", \"control_sharing\", \"control_sharing\", \"control_sharing\", \"control_sharing\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cpmc\", \"cpmc\", \"cpmc\", \"cpmc\", \"cpmc\", \"cue\", \"cue\", \"cue\", \"cue\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"curvature\", \"curvature\", \"curvature\", \"curvature\", \"curvature\", \"cvi\", \"cvi\", \"cvi\", \"cvi\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"decode\", \"decode\", \"decode\", \"decode\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"depolarization\", \"depolarization\", \"depolarization\", \"depolarization\", \"depolarization\", \"depolarize\", \"depolarize\", \"depolarize\", \"depolarize\", \"depolarize\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"diagnose\", \"diagnose\", \"diagnose\", \"diagnose\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"digital\", \"digital\", \"digital\", \"digital\", \"disease\", \"disease\", \"disease\", \"disease\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"division\", \"division\", \"division\", \"division\", \"division\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"drain\", \"drain\", \"drain\", \"drain\", \"drain\", \"ei\", \"ei\", \"ei\", \"ei\", \"eij\", \"eij\", \"eij\", \"eij\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"entity\", \"entity\", \"entity\", \"entity\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"epsp\", \"epsp\", \"epsp\", \"epsp\", \"epsp\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"exogenous_event\", \"exogenous_event\", \"exogenous_event\", \"exogenous_event\", \"expansion\", \"expansion\", \"expansion\", \"expansion\", \"expansion\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"extractor\", \"extractor\", \"extractor\", \"extractor\", \"extractor\", \"face\", \"face\", \"face\", \"face\", \"factorizability\", \"factorizability\", \"factorizability\", \"factorizability\", \"factorizable\", \"factorizable\", \"factorizable\", \"factorizable\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"filter\", \"filter\", \"filter\", \"filter\", \"finding\", \"finding\", \"finding\", \"finding\", \"finding\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fixation\", \"fixation\", \"fixation\", \"fixation\", \"fmli\", \"fmli\", \"fmli\", \"fmli\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"food\", \"food\", \"food\", \"food\", \"food\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"formalism\", \"frogger\", \"frogger\", \"frogger\", \"frogger\", \"frogger\", \"fuel\", \"fuel\", \"fuel\", \"fuel\", \"fuel\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"game\", \"game\", \"game\", \"game\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gene\", \"gene\", \"gene\", \"gene\", \"ghost\", \"ghost\", \"ghost\", \"ghost\", \"ghost\", \"gin\", \"gin\", \"gin\", \"gin\", \"gin\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"grid\", \"grid\", \"grid\", \"grid\", \"grid\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"hdl\", \"hdl\", \"hdl\", \"hdl\", \"heteroscedastic\", \"heteroscedastic\", \"heteroscedastic\", \"heteroscedastic\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"ialm\", \"ialm\", \"ialm\", \"ialm\", \"ical\", \"ical\", \"ical\", \"ical\", \"ical\", \"image\", \"image\", \"image\", \"image\", \"image\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"inconsistent\", \"inconsistent\", \"inconsistent\", \"inconsistent\", \"inconsistent\", \"inconsistent\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"injection\", \"injection\", \"injection\", \"injection\", \"injection\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"instrumental_variable\", \"instrumental_variable\", \"instrumental_variable\", \"instrumental_variable\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interference\", \"interference\", \"interference\", \"interference\", \"internist\", \"internist\", \"internist\", \"internist\", \"interrupt\", \"interrupt\", \"interrupt\", \"interrupt\", \"interrupt\", \"interruption\", \"interruption\", \"interruption\", \"interruption\", \"interruption\", \"item\", \"item\", \"item\", \"item\", \"item\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"jj\", \"jj\", \"jj\", \"jj\", \"jlog\", \"jlog\", \"jlog\", \"jlog\", \"kearn\", \"kearn\", \"kearn\", \"kearn\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"landmark\", \"landmark\", \"landmark\", \"landmark\", \"landmark\", \"large\", \"large\", \"large\", \"large\", \"large\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layered\", \"layered\", \"layered\", \"layered\", \"layered\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lio\", \"lio\", \"lio\", \"lio\", \"ljn\", \"ljn\", \"ljn\", \"ljn\", \"lmin\", \"lmin\", \"lmin\", \"lmin\", \"local\", \"local\", \"local\", \"local\", \"local\", \"localization\", \"localization\", \"localization\", \"localization\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"loglog\", \"loglog\", \"loglog\", \"loglog\", \"long_lasting\", \"long_lasting\", \"long_lasting\", \"long_lasting\", \"long_lasting\", \"loopy\", \"loopy\", \"loopy\", \"loopy\", \"loopy\", \"loss\", \"loss\", \"loss\", \"loss\", \"lowerbounde\", \"lowerbounde\", \"lowerbounde\", \"lowerbounde\", \"lprobit\", \"lprobit\", \"lprobit\", \"lprobit\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"minimization\", \"minimization\", \"minimization\", \"minimization\", \"minimization\", \"mismatch\", \"mismatch\", \"mismatch\", \"mismatch\", \"mismatch\", \"mission\", \"mission\", \"mission\", \"mission\", \"mission\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mlhgp\", \"mlhgp\", \"mlhgp\", \"mlhgp\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"moderate\", \"moderate\", \"moderate\", \"moderate\", \"moderate\", \"modular\", \"modular\", \"modular\", \"modular\", \"modulation\", \"modulation\", \"modulation\", \"modulation\", \"moral\", \"moral\", \"moral\", \"moral\", \"moralize\", \"moralize\", \"moralize\", \"moralize\", \"moreover\", \"moreover\", \"moreover\", \"moreover\", \"moreover\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"mse\", \"mse\", \"mse\", \"mse\", \"multisent\", \"multisent\", \"multisent\", \"multisent\", \"multisent\", \"mwis\", \"mwis\", \"mwis\", \"mwis\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neurocomputer\", \"neurocomputer\", \"neurocomputer\", \"neurocomputer\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron_perturbation\", \"neuron_perturbation\", \"neuron_perturbation\", \"neuron_perturbation\", \"nigp\", \"nigp\", \"nigp\", \"nigp\", \"nips_page\", \"nips_page\", \"nips_page\", \"nips_page\", \"nips_page\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"observable\", \"observable\", \"observable\", \"observable\", \"observable\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"odd\", \"odd\", \"odd\", \"odd\", \"odd\", \"offinding\", \"offinding\", \"offinding\", \"offinding\", \"ofmf\", \"ofmf\", \"ofmf\", \"ofmf\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"option\", \"option\", \"option\", \"option\", \"option\", \"ordinary_logistic\", \"ordinary_logistic\", \"ordinary_logistic\", \"ordinary_logistic\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"outlier\", \"outlier\", \"outlier\", \"outlier\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parent\", \"parent\", \"parent\", \"parent\", \"pathway\", \"pathway\", \"pathway\", \"pathway\", \"pathway\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"payment\", \"payment\", \"payment\", \"payment\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"pellet\", \"pellet\", \"pellet\", \"pellet\", \"pellet\", \"perceptual\", \"perceptual\", \"perceptual\", \"perceptual\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"perturbation\", \"perturbation\", \"perturbation\", \"perturbation\", \"perturbational\", \"perturbational\", \"perturbational\", \"perturbational\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phys\", \"phys\", \"phys\", \"phys\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"player\", \"player\", \"player\", \"player\", \"pleiotropic\", \"pleiotropic\", \"pleiotropic\", \"pleiotropic\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"pomdp\", \"pomdp\", \"pomdp\", \"pomdp\", \"pomdp\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"postsynaptic\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"private\", \"private\", \"private\", \"private\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"product\", \"product\", \"program\", \"program\", \"program\", \"program\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"psr\", \"psr\", \"psr\", \"psr\", \"psr\", \"quadrant\", \"quadrant\", \"quadrant\", \"quadrant\", \"quadruple\", \"quadruple\", \"quadruple\", \"quadruple\", \"quantitie\", \"quantitie\", \"quantitie\", \"quantitie\", \"question\", \"question\", \"question\", \"question\", \"question\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"redundancy\", \"redundancy\", \"redundancy\", \"redundancy\", \"reformulation\", \"reformulation\", \"reformulation\", \"reformulation\", \"regret\", \"regret\", \"regret\", \"regret\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"relation\", \"relation\", \"relation\", \"relation\", \"relation\", \"relational\", \"relational\", \"relational\", \"relational\", \"relational\", \"remote_sense\", \"remote_sense\", \"remote_sense\", \"remote_sense\", \"represent\", \"represent\", \"represent\", \"represent\", \"represent\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"response\", \"response\", \"response\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward_shape\", \"reward_shape\", \"reward_shape\", \"reward_shape\", \"reward_shape\", \"reward_shaping\", \"reward_shaping\", \"reward_shaping\", \"reward_shaping\", \"reward_shaping\", \"rh\", \"rh\", \"rh\", \"rh\", \"rh\", \"rmn\", \"rmn\", \"rmn\", \"rmn\", \"rod\", \"rod\", \"rod\", \"rod\", \"roi\", \"roi\", \"roi\", \"roi\", \"rois\", \"rois\", \"rois\", \"rois\", \"rpca\", \"rpca\", \"rpca\", \"rpca\", \"sage\", \"sage\", \"sage\", \"sage\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"scene\", \"scene\", \"scene\", \"scene\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"segment\", \"segment\", \"segment\", \"segment\", \"segment\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"sem\", \"sem\", \"sem\", \"sem\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"shallow\", \"shallow\", \"shallow\", \"shallow\", \"shallow\", \"shaping\", \"shaping\", \"shaping\", \"shaping\", \"shaping\", \"shock\", \"shock\", \"shock\", \"shock\", \"shock\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"simpler\", \"simpler\", \"simpler\", \"simpler\", \"simpler\", \"sin\", \"sin\", \"sin\", \"sin\", \"size\", \"size\", \"size\", \"size\", \"size\", \"smdp\", \"smdp\", \"smdp\", \"smdp\", \"smdp\", \"smo\", \"smo\", \"smo\", \"smo\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"sound\", \"sound\", \"sound\", \"sound\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spam\", \"spam\", \"spam\", \"spam\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparseness\", \"sparseness\", \"sparseness\", \"sparseness\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spin\", \"spin\", \"spin\", \"spin\", \"spiral\", \"spiral\", \"spiral\", \"spiral\", \"spiv\", \"spiv\", \"spiv\", \"spiv\", \"ssflogreg\", \"ssflogreg\", \"ssflogreg\", \"ssflogreg\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step_size\", \"step_size\", \"step_size\", \"step_size\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"string\", \"string\", \"string\", \"string\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"subject\", \"subject\", \"subject\", \"subject\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular_function\", \"submodular_function\", \"submodular_function\", \"submodular_function\", \"sum\", \"sum\", \"sum\", \"sum\", \"sum\", \"supergradient\", \"supergradient\", \"supergradient\", \"supergradient\", \"supervise\", \"supervise\", \"supervise\", \"supervise\", \"supervise\", \"svrg\", \"svrg\", \"svrg\", \"svrg\", \"swnp\", \"swnp\", \"swnp\", \"swnp\", \"symptom\", \"symptom\", \"symptom\", \"symptom\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tangent\", \"tangent\", \"tangent\", \"tangent\", \"tangent\", \"target\", \"target\", \"target\", \"target\", \"target\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task_relationship\", \"task_relationship\", \"task_relationship\", \"task_relationship\", \"taylor\", \"taylor\", \"taylor\", \"taylor\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"tile\", \"tile\", \"tile\", \"tile\", \"tile\", \"tiling\", \"tiling\", \"tiling\", \"tiling\", \"tiling\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tonga\", \"tonga\", \"tonga\", \"tonga\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trait\", \"trait\", \"trait\", \"trait\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"treat\", \"treat\", \"treat\", \"treat\", \"treat\", \"treat\", \"treermn\", \"treermn\", \"treermn\", \"treermn\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"tunnel\", \"tunnel\", \"tunnel\", \"tunnel\", \"tunnel\", \"tunneling\", \"tunneling\", \"tunneling\", \"tunneling\", \"tunneling\", \"umli\", \"umli\", \"umli\", \"umli\", \"unfaithful\", \"unfaithful\", \"unfaithful\", \"unfaithful\", \"union\", \"union\", \"union\", \"union\", \"union\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unity\", \"unity\", \"unity\", \"unity\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"unsurprise\", \"unsurprise\", \"unsurprise\", \"unsurprise\", \"untangling\", \"untangling\", \"untangling\", \"untangling\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variability\", \"variability\", \"variability\", \"variability\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variance\", \"variance\", \"variance\", \"variance\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variationally\", \"variationally\", \"variationally\", \"variationally\", \"vcal\", \"vcal\", \"vcal\", \"vcal\", \"vcal\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vlog\", \"vlog\", \"vlog\", \"vlog\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"vtun\", \"vtun\", \"vtun\", \"vtun\", \"vtun\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weather\", \"weather\", \"weather\", \"weather\", \"weather\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"worker\", \"worker\", \"worker\", \"worker\", \"wsat\", \"wsat\", \"wsat\", \"wsat\", \"zi\", \"zi\", \"zi\", \"zi\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 6, 7, 8, 1, 4, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el87021406516059978246743289617\", ldavis_el87021406516059978246743289617_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el87021406516059978246743289617\", ldavis_el87021406516059978246743289617_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el87021406516059978246743289617\", ldavis_el87021406516059978246743289617_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4      0.130422  0.032191       1        1  35.355833\n",
       "5      0.114774 -0.055321       2        1  35.193816\n",
       "6      0.080037  0.050298       3        1  14.901285\n",
       "7      0.035670 -0.027386       4        1   9.096266\n",
       "0     -0.037009 -0.012337       5        1   2.925782\n",
       "3     -0.102606 -0.019091       6        1   1.358200\n",
       "1     -0.095533  0.022049       7        1   1.168243\n",
       "2     -0.125755  0.009597       8        1   0.000576, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
       "268     model  1922.000000  1922.000000  Default  30.0000  30.0000\n",
       "1532    input   438.000000   438.000000  Default  29.0000  29.0000\n",
       "1641  network   477.000000   477.000000  Default  28.0000  28.0000\n",
       "240     learn   861.000000   861.000000  Default  27.0000  27.0000\n",
       "1099    local   228.000000   228.000000  Default  26.0000  26.0000\n",
       "...       ...          ...          ...      ...      ...      ...\n",
       "406      show     0.000248   781.159829   Topic8  -8.3030  -2.8985\n",
       "158    figure     0.000242   654.190765   Topic8  -8.3278  -2.7458\n",
       "102     datum     0.000241   691.257445   Topic8  -8.3297  -2.8028\n",
       "176      give     0.000238   661.248747   Topic8  -8.3444  -2.7732\n",
       "240     learn     0.000236   861.211269   Topic8  -8.3541  -3.0471\n",
       "\n",
       "[572 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "0         1  0.409538  abstract\n",
       "0         2  0.277429  abstract\n",
       "0         3  0.105687  abstract\n",
       "0         4  0.066054  abstract\n",
       "0         5  0.118898  abstract\n",
       "...     ...       ...       ...\n",
       "4916      4  0.040194      wsat\n",
       "3972      1  0.195076        zi\n",
       "3972      2  0.195076        zi\n",
       "3972      3  0.195076        zi\n",
       "3972      4  0.195076        zi\n",
       "\n",
       "[2070 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 6, 7, 8, 1, 4, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Closing Notes\n",
    "\n",
    "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
    "\n",
    "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
    "\n",
    "** **\n",
    "#### References:\n",
    "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
    "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
    "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
    "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
    "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "7. http://palmetto.aksw.org/palmetto-webapp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_model",
   "language": "python",
   "name": "topic_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
