{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1435703745304612870</td>\n",
       "      <td>2021-09-08 20:37:01</td>\n",
       "      <td>RT @OregonOEM: ðŸš©ðŸŒ©ðŸ”¥ Red Flag and #FireWeather w...</td>\n",
       "      <td>14838508</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1435664738353098756</td>\n",
       "      <td>2021-09-08 18:02:01</td>\n",
       "      <td>It's #NationalPreparednessMonth. Help ensure t...</td>\n",
       "      <td>14838508</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1435663883595706370</td>\n",
       "      <td>2021-09-08 17:58:37</td>\n",
       "      <td>RT @OregonOEM: Oregon is still recovering from...</td>\n",
       "      <td>14838508</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      conversation_id           created_at  \\\n",
       "0   1  1435703745304612870  2021-09-08 20:37:01   \n",
       "1   2  1435664738353098756  2021-09-08 18:02:01   \n",
       "2   3  1435663883595706370  2021-09-08 17:58:37   \n",
       "\n",
       "                                                text  author_id  \\\n",
       "0  RT @OregonOEM: ðŸš©ðŸŒ©ðŸ”¥ Red Flag and #FireWeather w...   14838508   \n",
       "1  It's #NationalPreparednessMonth. Help ensure t...   14838508   \n",
       "2  RT @OregonOEM: Oregon is still recovering from...   14838508   \n",
       "\n",
       "   in_reply_to_user_id  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Create your connection.\n",
    "from twitter.nlp_util import process_tweet\n",
    "\n",
    "cnx = sqlite3.connect('db.sqlite3')\n",
    "\n",
    "df = pd.read_sql_query(\n",
    "    \"SELECT id, conversation_id, created_at, text, author_id,in_reply_to_user_id FROM delab_timeline WHERE lang='en'\",\n",
    "    cnx)\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are importing the libraries.\n",
    "\n",
    "The first idea was to follow lda https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
    "\n",
    "but turns out it is to little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Showing some descriptive statistics for the author corpus downloaded from twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author_id.nunique()  # the number of unique authors in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_id\n",
       "16558158               478\n",
       "18616003               946\n",
       "26998226               469\n",
       "382814447              446\n",
       "1005470991668084736    445\n",
       "1106611172462219265    491\n",
       "1162371171805011968    447\n",
       "1239172010363826183    427\n",
       "1292908140975943681    414\n",
       "1402252385427222528    414\n",
       "1403930956428460035    441\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df[[\"author_id\", \"text\", \"id\"]]\n",
    "#df_reduced = df_reduced.groupby('author_id')\n",
    "# df_reduced.count()\n",
    "\n",
    "df_reshaped = df_reduced.pivot(index=\"id\", columns=\"author_id\", values=\"text\")\n",
    "mask = 400 > df_reshaped.nunique()\n",
    "mask = mask[mask == True]\n",
    "df_reshaped.drop(columns=mask.index, inplace=True)\n",
    "df_reshaped.nunique()  # the number of tweets of the authors that have more then 400 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following takes the pandas dataframe and converts it to a dictionary with the author ids as keys and the twitter\n",
    "corpora as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "8247                                   @ncreen_same ouch!\n",
       "8248    That last tweet got some responses from spambo...\n",
       "8249    'virtually' virtually means: real\\n\\nSo it sho...\n",
       "8250                               Sounds great on paper!\n",
       "8251    Any .com.au registrar recommendations? So far ...\n",
       "                              ...                        \n",
       "8742    RT @paydirtapp: Check out our Free Invoice Cre...\n",
       "8743    @taitems @mmilo yeah nice site, suggestion: ht...\n",
       "8744    RT @MichaelFHansen: Zendesk eyes Southeast Asi...\n",
       "8745    While LinkedIn has been changing drastically o...\n",
       "8746    Another LinkedIn email failâ€¦ now they want me ...\n",
       "Name: 16558158, Length: 478, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_reshaped.shape\n",
    "author_corpora = df_reshaped.to_dict(orient=\"series\")\n",
    "for author_id, tweets in author_corpora.items():\n",
    "    author_corpora[author_id] = tweets.dropna()\n",
    "\n",
    "example_corpus = author_corpora[next(iter(author_corpora))]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are ready to analyze the corpora of the authors tweets. We do one example with the first corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 9\n",
      "Number of documents: 478\n",
      "Average topic coherence: -5.7899.\n",
      "[([(0.27511057, 'work'),\n",
      "   (0.25340468, 'know'),\n",
      "   (0.23143622, 'look'),\n",
      "   (0.22061206, 'new'),\n",
      "   (0.004309774, 'like'),\n",
      "   (0.0038732383, '...'),\n",
      "   (0.003764999, 'use'),\n",
      "   (0.0037498248, 'get'),\n",
      "   (0.0037387218, 'â€¦')],\n",
      "  -5.766601811687497),\n",
      " ([(0.35383725, 'like'),\n",
      "   (0.34152696, '...'),\n",
      "   (0.2767461, 'use'),\n",
      "   (0.004997251, 'look'),\n",
      "   (0.004972638, 'new'),\n",
      "   (0.004517441, 'know'),\n",
      "   (0.004516976, 'get'),\n",
      "   (0.004463332, 'work'),\n",
      "   (0.0044220276, 'â€¦')],\n",
      "  -5.784562450219087),\n",
      " ([(0.49528044, 'â€¦'),\n",
      "   (0.45598957, 'get'),\n",
      "   (0.0073577045, 'work'),\n",
      "   (0.007170174, 'know'),\n",
      "   (0.007053388, 'like'),\n",
      "   (0.0068173804, '...'),\n",
      "   (0.0068133674, 'look'),\n",
      "   (0.006772335, 'use'),\n",
      "   (0.006745627, 'new')],\n",
      "  -5.818532690537116)]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for tweet in example_corpus:\n",
    "    docs.append(process_tweet(tweet))\n",
    "\n",
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "\n",
    "###############################################################################\n",
    "# We remove rare words and common words based on their *document frequency*.\n",
    "# Below we remove words that appear in less than 20 documents or in more than\n",
    "# 50% of the documents. Consider trying to remove words only based on their\n",
    "# frequency, or maybe combining that with this approach.\n",
    "#\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "###############################################################################\n",
    "# Finally, we transform the documents to a vectorized form. We simply compute\n",
    "# the frequency of each word, including the bigrams.\n",
    "#\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "###############################################################################\n",
    "# Let's see how many tokens and documents we have to train on.\n",
    "#\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 3\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# Note that we use the \"Umass\" topic coherence measure here (see\n",
    "# :py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
    "# obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
    "# accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
    "\n",
    "top_topics = model.top_topics(corpus)  #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As this resulted in disappointing non-nouns as topics the alternative would be to use the named entities directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% alternatively preprocess including NER\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[@rioter @alandownie @ncreen_same,\n",
       " @alandownie,\n",
       " 11000 hours,\n",
       " iOS,\n",
       " Zendesk,\n",
       " #Melbourne #,\n",
       " tomorrow,\n",
       " 10k,\n",
       " a few seconds]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_entities = set()\n",
    "\n",
    "for tweet in example_corpus:\n",
    "    doc = nlp(tweet)\n",
    "    sentences = list(doc.sents)\n",
    "    for sentence in sentences:\n",
    "        if sentence.ents:\n",
    "            found_entities.update(sentence.ents)\n",
    "\n",
    "entities = list(found_entities)[1:10]\n",
    "entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next steps would be\n",
    "- clean the NERs (one word), no numbers, no urls, no hashtags...\n",
    "- get the embeddings for the \"regular\" words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept ['iOS', 'Zendesk', '#Melbourne #', 'tomorrow', '10k', 'a few seconds', 'Lucasfilm', 'One Million', 'Rebooting Windows', 'Australia', 'SAB Miller', 'Melbourne', 'Alfa Beer', 'IPA', 'LinkedIn', '28th Mar 2013', '#', 'LinkedIn', '7pm', 'Game Changer', 'americans', 'Melbourne', 'Zendesk', 'NFP', \"Bottle Share'\", 'five', 'SQL', 'today', 'Melbourne', 'SAB Miller', 'Athenian Brewery', 'Carlsberg Group', 'this year', 'one', 'PS4', '25%', 'WebEx', 'Disney', 'Phil Schiller', 'one', 'Budapest', 'Los Angeles', 'bush', 'the next week', 'BugHerd', 'Heineken International', 'tomorrow', 'Heineken', 'the half million']\n",
      "dropped ['@rioter @alandownie @ncreen_same', '@alandownie', 'RT @merryuxmas:', '@buzzswarbrick', '@alandownie @mmilo', '3124', 'RT @MichaelFHansen', '@nikiscevak', '@mmilo', '@rioter', 'http://t.co/bB7Uzzqo', '@alandownie @ncreen_same', '@gartner', 'RT @paydirtapp:', 'RT', '234272', '@alandownie @vbrendel', '@medhved', '@alandownie @mmilo', 'http://t.co/qsvud4CHF6', 'RT @veroapp', '@jessemcnelis', '@veroapp', '@jamesarosen @therealdevgeeks', '@UXMastery', '@lukcha', '@untappd', 'RT @HeadStartAus', '@bensmithett', '@lomaxx', '@alandownie', 'RT @medhved', '@kealey', '@bigyahu', 'http://t.co/UbLUTw27', 'RT @Scobleizer', 'RT @alandownie', '@theheraldsun', 'RT @caseyjohnellis', '@bugcrowd', '3', '@GoPollGo', 'RT @bugherd', 'BugHerd https://t.co/9IK6XRd5Qw', '@paydirtapp', '@alandownie @rioter', '@alandownie', '@TomPisel', '@gpdawson @runkeeper']\n"
     ]
    }
   ],
   "source": [
    "def clean_entities(entities):\n",
    "    result = []\n",
    "    dropped = []\n",
    "    for entity in entities:\n",
    "        entity = entity.text\n",
    "        if \"@\" in entity or \"RT\" in entity or entity.isnumeric() or \"http\" in entity or \"www\" in entity:\n",
    "            dropped.append(entity)\n",
    "        else:\n",
    "            result.append(entity)\n",
    "    return result, dropped\n",
    "\n",
    "\n",
    "result, dropped = clean_entities(found_entities)\n",
    "\n",
    "print(\"kept {}\".format(result[1:50]))\n",
    "print(\"dropped {}\".format(dropped[1:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the following we are using fasttext to get vector representations of the named entities\n",
    "\n",
    "_@inproceedings{grave2018learning,\n",
    "  title={Learning Word Vectors for 157 Languages},\n",
    "  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\n",
    "  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n",
    "  year={2018}\n",
    "}_\n",
    "\n",
    "**Careful! Loading the fasttext model into memory requires a lot of memory and time**\n",
    "\n",
    "For this reason, the code is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "# ft = fasttext.load_model('/home/julian/nltk_data/fasttext/cc.de.300.bin')\n",
    "# word_vec_example = ft.get_word_vector('KÃ¶nig')\n",
    "# word_vec_example[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "from stackoverflow the following idea to store the embeddings in the db\n",
    "\n",
    "```python\n",
    "from django.db import models\n",
    "np_field = models.BinaryField()\n",
    "# transform numpy array to python byte using pickle dumps, then encoded by base64\n",
    "np_bytes = pickle.dumps(np_array)\n",
    "np_base64 = base64.b64encode(np_bytes)\n",
    "model.np_field = np_base64\n",
    "get the numpy array from django model\n",
    "np_bytes = base64.b64decode(model.np_field)\n",
    "np_array = pickle.loads(np_bytes)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}