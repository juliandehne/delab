{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyzing the topic changes and their implications\n",
    "\n",
    "First, we load the pretrained topic model and the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 308426.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['world', 'back', 'our', 'up', 'him', 'see', 'know', 'tweet', 'right', 'look']\n",
      "['br9ys2tway', 'bryrdzpsq5', 'fonts', 'fatally', 'fatal', 'fat', 'fathom', 'fathoms', 'fati', 'fatime']\n",
      "['zxxmo0yfrh', 'xsmexnxocw', 'xtiyvwtpyl', 'gc0xaznhpq', '3ds', 'rom', 'emulator', 'pokemon', 'ezxyuq40yp', 'gbcnnjptue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from bertopic import BERTopic\n",
    "from util.sql_switch import get_query_native\n",
    "\n",
    "#import delab.topic.train_topic_model as tm\n",
    "\n",
    "bertopic_model = BERTopic().load(\"BERTopic\", embedding_model=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "topic_info = bertopic_model.get_topic_info()\n",
    "\n",
    "\n",
    "def topic2wordvec(topic_model):\n",
    "    result = []\n",
    "    for t_word in topic_model:\n",
    "        str_w = t_word[0]\n",
    "        result.append(str_w)\n",
    "    return result\n",
    "\n",
    "\n",
    "# create topic-word map\n",
    "topic2word = defaultdict(list)\n",
    "for topic_id in tqdm(topic_info.Topic):\n",
    "    topic_model = bertopic_model.get_topic(topic_id)\n",
    "    words = topic2wordvec(topic_model)\n",
    "    topic2word[topic_id] = topic2word[topic_id] + words\n",
    "t2w_keys = list(topic2word.keys())[0:3]\n",
    "for key in t2w_keys:\n",
    "    print(topic2word[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we are loading the pre_saved ft vectors from the database!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using postgres\n"
     ]
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [id, text, author_id, bertopic_id, conversation_id, sentiment_value, created_at]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>author_id</th>\n      <th>bertopic_id</th>\n      <th>conversation_id</th>\n      <th>sentiment_value</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversations = get_query_native(\n",
    "    \"SELECT id, text, author_id, bertopic_id, conversation_id,sentiment_value,created_at FROM delab_tweet tw where language = 'en' and bertopic_id >= 0\")\n",
    "df_conversations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: text, dtype: object)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_6351/3820675767.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_conversations\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_conversations\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbertopic_id\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mbertopic\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbertopic_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_topic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbertopic\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    937\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    938\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_fallback_to_positional\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 939\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    940\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    941\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mkey_is_scalar\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "text = df_conversations.head(1).text\n",
    "print(text)\n",
    "id = df_conversations.head(1).bertopic_id[0]\n",
    "bertopic = bertopic_model.get_topic(id)\n",
    "print(bertopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It looks like our topic is matching the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_conversations_bad_topic = get_query_native(\n",
    "    \"SELECT COUNT(*) as n  FROM delab_tweet tw where language = 'en' and bertopic_id < 0\")\n",
    "\n",
    "df_conversations_total = get_query_native(\n",
    "    \"SELECT COUNT(*) as n  FROM delab_tweet tw where language = 'en' and bertopic_id not null\")\n",
    "\n",
    "print(df_conversations.id.size)\n",
    "print(df_conversations_bad_topic.n[0])\n",
    "print(df_conversations_total.n[0])\n",
    "\n",
    "print(\"From {} tweets {} percent could not be topic labeled\".format(df_conversations_total.n[0],\n",
    "                                                                    df_conversations_bad_topic.n[0] * 100 /\n",
    "                                                                    df_conversations_total.n[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems, that our recall is not so great. We favoured the precision over the recall, drastically removing\n",
    "topics that did not include words in the vocabulary. However, the bert models have good results on the oov-words, too.\n",
    "\n",
    "More importantly, the high precision and low recall clashes with low precision and high sensitivity from the sentiment analysis.\n",
    "However, we are continuing for the moment in order to have a look at the results as they stand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df_conversations.conversation_id.nunique()\n",
    "df_reshaped = df_conversations.pivot(index=\"id\", columns=\"conversation_id\", values=\"text\")\n",
    "mask = 10 > df_reshaped.nunique()\n",
    "mask = mask[mask == True]\n",
    "df_reshaped.drop(columns=mask.index, inplace=True)\n",
    "df_reshaped.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are only 4 conversations with 10 or more tweets that have a valid topic classification :-(.\n",
    "\n",
    "Lets' have a look at the relation between sentiment_values and topic_changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conversation_corpora_cleaned = {}\n",
    "conversation_corpora = df_reshaped.to_dict(orient=\"series\")\n",
    "for conversation_id, tweets in conversation_corpora.items():\n",
    "    conversation_corpora_cleaned[conversation_id] = tweets.dropna()\n",
    "\n",
    "useful_conversations_keys = conversation_corpora_cleaned.keys()\n",
    "\n",
    "useful_conversations = df_conversations[df_conversations[\"conversation_id\"].isin(useful_conversations_keys)]\n",
    "useful_conversations = useful_conversations.loc[:,\n",
    "                       [\"id\", \"bertopic_id\", \"conversation_id\", \"sentiment_value\", \"created_at\"]]\n",
    "\n",
    "exampleconversation = useful_conversations[useful_conversations.conversation_id == 1429364500159991808]\n",
    "exampleconversation.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This conversation seems a good candidate for inspection as it includes drastic topic and sentiment changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# in the productive setting filter this query with \"in words contained in the current conversation topics\"\n",
    "word2vec = get_query_native(\n",
    "    \"SELECT word, ft_vector from delab_topicdictionary\")\n",
    "\n",
    "def get_topic_delta(topic_id_1, topic_id_2):\n",
    "    words1 = topic2word.get(topic_id_1)\n",
    "    words2 = topic2word.get(topic_id_2)\n",
    "    if words1 is not None and words2 is not None:\n",
    "        filtered_w2v1 = word2vec[word2vec[\"word\"].isin(words1)]\n",
    "        filtered_w2v2 = word2vec[word2vec[\"word\"].isin(words2)]\n",
    "        ft_vectors_1 = filtered_w2v1.ft_vector.apply(lambda x: pd.Series(json.loads(x)))\n",
    "        ft_vectors_2 = filtered_w2v2.ft_vector.apply(lambda x: pd.Series(json.loads(x)))\n",
    "        len1 = len(ft_vectors_1)\n",
    "        len2 = len(ft_vectors_2)\n",
    "        sum_v1 = (ft_vectors_1.sum(axis=0) / len1) # we assume the vectors are embedded in a linear space\n",
    "        sum_v2 = (ft_vectors_2.sum(axis=0) / len2)\n",
    "        similarity = spatial.distance.cosine(sum_v1, sum_v2)\n",
    "        return similarity\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "exampleconversation.sort_values(by=['created_at'], ascending=True, inplace=True)\n",
    "exampleconversation.reset_index(drop=True, inplace=True)\n",
    "exampleconversation[\"bertopic_shifted\"] = exampleconversation.bertopic_id.shift()\n",
    "exampleconversation[\"topic_delta\"] = exampleconversation.apply(\n",
    "    lambda x: get_topic_delta(x.bertopic_id, x.bertopic_shifted), axis=1)\n",
    "\n",
    "exampleconversation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "print(topic2word.get(107))\n",
    "print(topic2word.get(149))\n",
    "print(\"delta is 0.23\")\n",
    "\n",
    "print(topic2word.get(149))\n",
    "print(topic2word.get(8))\n",
    "print(\"delta is 0.72\")\n",
    "\n",
    "sv = exampleconversation.sentiment_value\n",
    "exampleconversation[\"sentiment_normalized\"] = ((sv-sv.min())/(sv.max()-sv.min()))\n",
    "\n",
    "exampleconversation[\"sentiment_deltas\"] = exampleconversation[\"sentiment_normalized\"].diff().abs()\n",
    "plot = exampleconversation.plot(y=['topic_delta', \"sentiment_deltas\"], xlabel = \"Natural Order in Conversation\", ylabel=\"Normalized changes\", use_index=True)\n",
    "plot.get_figure().savefig('sentiment_topics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After normalizing, it looks like we can compare the two metrics. However, topic continuation should be\n",
    "interpreted differently then a continuous sentiment flow.\n",
    "\n",
    "In order to compute the correlation we need to compute:\n",
    "- a more precise sentiment classification\n",
    "- a wider set of conversations with existing topic classifications\n",
    "- a better metric of the topic flow that takes into account the general frequency of topic  changes and\n",
    "  can thus highlight conversations with high or low topic volatility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}