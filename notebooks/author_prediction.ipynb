{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Author Prediction\n",
    "- checking gpu stats and limiting gpu memory\n",
    "- importing dataset from pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 4104323072\n",
      "Free memory: 4099145728\n",
      "Used memory: 5177344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n",
    "\n",
    "nvidia_smi.nvmlShutdown()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_30483/3012699097.py:10: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "cuda gpu is available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 12:07:50.155690: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-08 12:07:50.257627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:50.290045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:50.290354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.158819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.159165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.159448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.159701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 2651 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-08-08 12:07:51.161824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.162097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.162357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.162934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.163198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.163446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.163713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.163966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-08 12:07:51.164182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2651 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": "(3419848, 127)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from keras import backend as K\n",
    "import pickle5 as pickle\n",
    "\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=2024)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "print(\"cuda gpu is available: {}\".format(is_cuda_gpu_available))\n",
    "\n",
    "#file_name = \"data/vision_forward_graph_data_local_05_08_22.pkl\"\n",
    "file_name = \"data/vision_forward_graph_data_08_09_22.pkl\"\n",
    "with open(file_name, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# importing utility functions\n",
    "%run author_vision_util.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen 531 conversations and gotten 541146 from twitter compared to 514793 from reddit\n"
     ]
    },
    {
     "data": {
      "text/plain": "(541091, 127)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = equalize_samples(df)\n",
    "df = df[df[\"platform\"] == \"twitter\"]\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a one hot vector representation of the possible authors\n",
    "- create an artificial user that represents a new user in a conversation up to that point\n",
    "- get a matrix with the authors as columns and a 1 if the author wrote the post\n",
    "- join it with the feature matrix\n",
    "- drop the author column\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Author_is_new\nTrue             444673\nFalse             96418\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute a fake user that symbolizes that the given user has not been seen at a given stage in the conversation\n",
    "df_conversation_authors = df[[\"conversation_id\", \"author\", \"current_time\"]]\n",
    "first_times = df_conversation_authors.groupby([\"conversation_id\", \"author\"]).min()\n",
    "\n",
    "def is_new_author(row):\n",
    "    earliest_author_post = first_times.loc[row[\"conversation_id\"],row[\"author\"]]\n",
    "    current_post_time = row[\"current_time\"]\n",
    "    return  earliest_author_post >= current_post_time\n",
    "\n",
    "new_author_column = df[[\"conversation_id\", \"author\", \"current_time\"]].apply(is_new_author, axis=1)\n",
    "new_author_column= new_author_column.rename(columns={'current_time':\"Author_is_new\"})\n",
    "#new_author_column.describe()\n",
    "# current author has not been the beam_node\n",
    "new_author_column.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_new_author_column(df):\n",
    "    author_one_hot = pd.get_dummies(df.author, prefix=\"Author\", sparse=True)\n",
    "    author_one_hot = author_one_hot.astype(bool).apply(lambda x: x & ~new_author_column.Author_is_new).astype(int)\n",
    "    labels = author_one_hot.join(new_author_column.astype(int))\n",
    "    features = take_features(df, [\"author\", \"current_time\", \"beam_node_time\"])\n",
    "    combined_set = features.join(labels)\n",
    "    return combined_set, features, labels\n",
    "\n",
    "combined_set, features, labels = compute_new_author_column(df)\n",
    "# combined_set.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training NN to predict the author that would write next\n",
    "- included a \"new author\" category to capture predicting unknown authors\n",
    "- using multi-class classification (instead of multi-label)\n",
    "- relu/sigmoid activation functions have same effect\n",
    "- precision grew significantly when adding more than 3-5 layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.optimizer_v2.rmsprop import RMSprop  # selecting train and test datasets\n",
    "train, test = train_test_split(combined_set, test_size=0.2, shuffle=True)\n",
    "print(\"split training and test set\")\n",
    "\n",
    "# train the model\n",
    "y = train.drop(features.columns, axis=1)\n",
    "x = train.drop(labels.columns, axis=1)\n",
    "print(\"seperated features and y with shapes:\")\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# import tensorflow and train the model\n",
    "# print(tf.__version__)\n",
    "input_shape = (x.shape[1],)\n",
    "output_shape = y.shape[1]\n",
    "print(\"inputshape is {}\".format(input_shape))\n",
    "model = Sequential([\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dropout(0.2),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='sigmoid', input_shape=input_shape),\n",
    "    Dense(output_shape, activation='softmax', input_shape=input_shape)\n",
    "])\n",
    "print(\"defined model as {}\".format(model.layers))\n",
    "# stochastic gradient descend as a classifier seem appropriate\n",
    "model.compile(\n",
    "    optimizer=RMSprop(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy', 'accuracy' ,'mae']\n",
    ")\n",
    "print(\"compiled model\")\n",
    "model.fit(x, y, epochs=3)\n",
    "#model.fit(x, y, epochs=10, shuffle=True)\n",
    "# evaluate the model on the test set\n",
    "test_y = test.drop(features.columns, axis=1)\n",
    "test_x = test.drop(labels.columns, axis=1)\n",
    "\n",
    "loss, cat_accuracy, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "print(\"the accuracy on the training set is cat acc {}, reg acc {} and the mae is {}\".format(cat_accuracy, accuracy, mae))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#print(labels.columns)\n",
    "# some pandas alchemy to sample  2 rows of each conversation\n",
    "sample_df = df.sample(frac=1).reset_index(drop=True).groupby('conversation_id').apply(lambda x: x.sample(n=1)).reset_index(drop = True)\n",
    "sample_features = take_features(sample_df, [\"author\", \"current_time\", \"beam_node_time\"])\n",
    "sample_prediction = model.predict(sample_features)\n",
    "np.matrix(sample_prediction)[0:5, -1] # the last row is the \"new author column\" label and should contain a high value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predicting the author presence based on prediction probabilities\n",
    "- compute predictions for the whole dataframe\n",
    "- drop features and non-features except conversation and platform\n",
    "- wide to long the authors to make them a index\n",
    "- groupby conversation and platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_features = take_features(df, [\"author\", \"current_time\", \"beam_node_time\"])\n",
    "predictions = model.predict(all_features)\n",
    "column_names = labels.columns\n",
    "predictions = pd.DataFrame(predictions, columns=column_names)\n",
    "print(type(predictions))\n",
    "print(predictions.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_non_features = take_non_features(df)\n",
    "print(type(all_non_features))\n",
    "print(all_non_features.shape)\n",
    "all_non_features.reset_index(drop=True, inplace=True)\n",
    "joined_dataframe = all_non_features.join(predictions)\n",
    "not_needed_list = [\"beam_node\", \"has_followed_path\", \"has_follow_path\", \"beam_node_author\", \"current\"]\n",
    "author_predictions = joined_dataframe.drop(not_needed_list, axis=1)\n",
    "author_predictions.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "author_predictions[\"id\"] = author_predictions.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "author_predictions.Author_is_new.describe() # no idea why that is the same prediction of all the rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "author_predictions_existing = author_predictions.drop([\"Author_is_new\"], axis=1)\n",
    "author_predictions_existing_reshaped = pd.wide_to_long(author_predictions_existing, stubnames=\"Author_\", i=[\"platform\", \"conversation_id\", \"id\"], j=\"author_id\")\n",
    "author_predictions_existing_reshaped.head(3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_author_pred = author_predictions_existing_reshaped.groupby([\"platform\", \"conversation_id\", \"author_id\"]).mean()\n",
    "avg_author_pred.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_conversation_pred  = avg_author_pred.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "avg_conversation_pred.head(3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_platform_pred = avg_conversation_pred.groupby([\"platform\"]).mean()\n",
    "avg_platform_pred # picking the correct author seems to be exceedingly difficult"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Notes\n",
    "- inserting the new author column increased precision times 10\n",
    "- categorical accuracy and regular accuracy match (which is weird)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}