{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 4627 conversations 1292 are shared in all datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": "(252329, 127)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from random import sample\n",
    "# PB: prediction based algorithm\n",
    "# RB: response based algoritm\n",
    "\n",
    "\n",
    "file_name_PB = \"data/vision_forward_graph_data_08_09_22.pkl\"\n",
    "with open(file_name_PB, 'rb') as f:\n",
    "    df_PB = pickle.load(f)\n",
    "    #df_PB.sort_values(by=\"platform\", inplace=True, ignore_index=True)\n",
    "\n",
    "file_name_RB = \"data/vision_graph_data_remote_23_08_22.pkl\"\n",
    "with open(file_name_RB, 'rb') as f:\n",
    "    df_RB = pickle.load(f)\n",
    "\n",
    "\n",
    "file_name_baseline = \"data/vision_baseline_graph_data_24_08_22.pkl\"\n",
    "with open(file_name_RB, 'rb') as f:\n",
    "    df_baseline = pickle.load(f)\n",
    "\n",
    "file_name_centrality = \"data/author_centrality_remote.pkl\"\n",
    "with open(file_name_centrality, 'rb') as f:\n",
    "    df_centrality = pickle.load(f)\n",
    "\n",
    "common_conversation_ids = set(df_PB.conversation_id).intersection(df_RB.conversation_id).intersection(\n",
    "    df_centrality.conversation_id).intersection(df_baseline.conversation_id)\n",
    "all_conversation_ids = set(df_PB.conversation_id).union(df_RB.conversation_id).union(df_centrality.conversation_id).union(df_baseline.conversation_id)\n",
    "all_conversation_count = len(all_conversation_ids)\n",
    "common_conversation_count = len(common_conversation_ids)\n",
    "# reducing the sample size for testing\n",
    "common_conversation_ids = sample(common_conversation_ids, 300)\n",
    "\n",
    "print(\"From {} conversations {} are shared in all datasets\".format(all_conversation_count, common_conversation_count))\n",
    "\n",
    "\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(common_conversation_ids)]\n",
    "df_RB = df_RB[df_RB.conversation_id.isin(common_conversation_ids)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(common_conversation_ids)]\n",
    "df_baseline = df_baseline[df_baseline.conversation_id.isin(common_conversation_ids)]\n",
    "df_PB.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility Functions\n",
    "\n",
    "The following cell contains utility functions that are needed for all the different algorithms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def is_not_reddit_or_twitter(text):\n",
    "    if text == \"reddit\" or text == \"twitter\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def equalize_samples(df):\n",
    "    \"\"\"\n",
    "    this approximates the same number of conversations for both platforms\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_conversations_twitter = set(df[df[\"platform\"] == \"twitter\"].conversation_id.tolist())\n",
    "    df_conversations_reddit = set(df[df[\"platform\"] == \"reddit\"].conversation_id.tolist())\n",
    "    reddit_data_count = df.loc[df.platform == \"reddit\", 'platform'].count()\n",
    "    twitter_data_count = df.loc[df.platform == \"twitter\", 'platform'].count()\n",
    "    # assert twitter_data_count > reddit_data_count, \"counts (reddit, twitter) are ({},{}):\".format(reddit_data_count, twitter_data_count)\n",
    "    current_count = 0\n",
    "    n = 1\n",
    "    smaller_count = reddit_data_count\n",
    "    df_conversations = df_conversations_twitter\n",
    "    if reddit_data_count > twitter_data_count:\n",
    "        smaller_count = twitter_data_count\n",
    "        df_conversations = df_conversations_reddit\n",
    "    while current_count < smaller_count:\n",
    "        chosen_conversation_ids = sample(df_conversations, n)\n",
    "        df_candidate = df[df[\"conversation_id\"].isin(chosen_conversation_ids)]\n",
    "        n = n + 1\n",
    "        current_count = df_candidate.shape[0]\n",
    "    print(\"chosen {} conversations and gotten {} from twitter compared to {} from reddit\".format(n, current_count,\n",
    "                                                                                                 reddit_data_count))\n",
    "    not_chosen_conversation_ids = set(df_conversations) - set(chosen_conversation_ids)\n",
    "    df_result = df[~df[\"conversation_id\"].isin(not_chosen_conversation_ids)]\n",
    "    return df_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-Features\n",
    "\n",
    "All the cells contain a number of columns that have a meaning in the conversation but are not features to train the NN with."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "# some utility functions to take the columns that are used as features\n",
    "non_feature_list = [\"current\", \"beam_node\", \"conversation_id\", \"platform\", \"has_followed_path\", \"has_follow_path\",\n",
    "                    \"beam_node_author\", \"author\"]\n",
    "\n",
    "\n",
    "def take_features(df, additional_non_features=[]):\n",
    "    non_feature_list2 = non_feature_list + additional_non_features\n",
    "    df = df.drop(non_feature_list2, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def take_non_features(df, additional_non_features=[]):\n",
    "    non_feature_list2 = non_feature_list + additional_non_features\n",
    "    column_names = df.columns.values\n",
    "    feature_list = [column_name for column_name in column_names if column_name not in non_feature_list2]\n",
    "    df = df.drop(feature_list, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_timedelta(df):\n",
    "    # normalize timedelta (put between 0 and 1)\n",
    "    dt = df.timedelta\n",
    "    timedelta_normalized = (dt - dt.min()) / (dt.max() - dt.min())\n",
    "    df = df.assign(timedelta=timedelta_normalized)\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Cleaning and Data preperation\n",
    "- Delete rows that are neither twitter or reddit data\n",
    "- normalize time deltas\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen 155 conversations and gotten 115721 from twitter compared to 139855 from reddit\n"
     ]
    }
   ],
   "source": [
    "# filtering data that is not twitter or reddit\n",
    "def delete_not_twitter_not_reddit(df):\n",
    "    platform = df.platform\n",
    "    to_delete_rows = platform.apply(lambda x: is_not_reddit_or_twitter(x))\n",
    "    df = df.drop(df[to_delete_rows].index)\n",
    "    return df\n",
    "\n",
    "df_RB = delete_not_twitter_not_reddit(df_RB)\n",
    "df_PB = delete_not_twitter_not_reddit(df_PB)\n",
    "df_centrality = delete_not_twitter_not_reddit(df_centrality)\n",
    "\n",
    "df_RB = equalize_samples(df_RB)\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(df_RB.conversation_id)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(df_RB.conversation_id)]\n",
    "df_baseline  = df_baseline[df_baseline.conversation_id.isin(df_RB.conversation_id)]\n",
    "\n",
    "df_RB = normalize_timedelta(df_RB)\n",
    "df_PB = normalize_timedelta(df_PB)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline for Author vision\n",
    "- uses selected values as a distance measure\n",
    "- probability of having seen a tweet is reduced by a half with each step in the reply hierachy\n",
    "- probability of having seen a tweet is reduced by a quarter for each step away from the root\n",
    "- probabiliy of having seen a tweet is increased for each path in the follower network to the tweet (forthcoming)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# in order to allow the comparison, the filter from the other notebook needs to run and the predictions, too\n",
    "reply_filter_col = [col for col in df_baseline if col.startswith('reply')]\n",
    "root_distance_filter_col = [col for col in df_baseline if col.startswith('root')]\n",
    "reply_columns = df_baseline[reply_filter_col]\n",
    "root_distance_columns = df_baseline[root_distance_filter_col]\n",
    "reply_cs = reply_columns.sum(axis=1)\n",
    "root_distance_cs = root_distance_columns.sum(axis=1)\n",
    "rcs_not_null = [i for i in reply_cs.tolist() if i != 0]\n",
    "root_reply_combined = (root_distance_cs + reply_cs)\n",
    "root_reply_combined = (root_reply_combined - root_reply_combined.min()) / (\n",
    "        root_reply_combined.max() - root_reply_combined.min())\n",
    "combined = [i for i in root_reply_combined.tolist() if i != 0]\n",
    "df_baseline = df_baseline.assign(root_reply_combined=root_reply_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  baseline\nplatform conversation_id     author                       \ndelab    1455538619804831744 2                    0.500000\n         1463938900544692229 1                    0.730769\n         1464364597192892416 2                    0.357143\nreddit   840660              538210               0.462963\n                             819234               0.462025\n...                                                    ...\ntwitter  1552230764602851329 1433445933518409730  0.500000\n                             1462782939071225858  0.500000\n                             1463651814616846336  0.476190\n                             1495016557587668994  0.416667\n                             1534920737659699201  0.428571\n\n[4883 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>baseline</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">delab</th>\n      <th>1455538619804831744</th>\n      <th>2</th>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1463938900544692229</th>\n      <th>1</th>\n      <td>0.730769</td>\n    </tr>\n    <tr>\n      <th>1464364597192892416</th>\n      <th>2</th>\n      <td>0.357143</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">reddit</th>\n      <th rowspan=\"2\" valign=\"top\">840660</th>\n      <th>538210</th>\n      <td>0.462963</td>\n    </tr>\n    <tr>\n      <th>819234</th>\n      <td>0.462025</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th rowspan=\"5\" valign=\"top\">1552230764602851329</th>\n      <th>1433445933518409730</th>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1462782939071225858</th>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1463651814616846336</th>\n      <td>0.476190</td>\n    </tr>\n    <tr>\n      <th>1495016557587668994</th>\n      <td>0.416667</td>\n    </tr>\n    <tr>\n      <th>1534920737659699201</th>\n      <td>0.428571</td>\n    </tr>\n  </tbody>\n</table>\n<p>4883 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline_with_authors = df_baseline[[\"root_reply_combined\", \"conversation_id\", \"author\", \"platform\"]]\n",
    "# df_reshaped = pd.pivot_table(df_data,index=[\"conversation_id\", \"current\"], columns=[\"root_reply_combined\"],aggfunc = np.mean)\n",
    "baseline_gpm = df_baseline_with_authors.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "baseline_predictions = baseline_gpm\n",
    "baseline_predictions.rename(columns={\"root_reply_combined\": \"baseline\"},inplace=True)\n",
    "baseline_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "          baseline\nplatform          \ndelab     0.529304\nreddit    0.490456\ntwitter   0.490994",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseline</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>delab</th>\n      <td>0.529304</td>\n    </tr>\n    <tr>\n      <th>reddit</th>\n      <td>0.490456</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.490994</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_gpm_conversation = baseline_gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "baseline_gpm = baseline_gpm_conversation.groupby(by=[\"platform\"]).mean()\n",
    "baseline_gpm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repetition Probabilities\n",
    "\n",
    "#### Analyzing the probability of an author writing repeatedly in the same conversation\n",
    "1. sum up the amounts y == 1 (because an author has answered himself)\n",
    "2. sum chances of an author seeing himself write\n",
    "3. calculate a measure of how likely it is that an author sees himself repeated as a test for the nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "platform  conversation_id    \nreddit    840660                 0.046076\n          1513432                0.104839\n          2887680                0.388889\n          4695940                0.083333\n          4947747                0.187192\n                                   ...   \ntwitter   1551870406843420675    0.428571\n          1551971727604940800    0.069697\n          1552024342661251072    0.094505\n          1552225696029855744    0.063277\n          1552230764602851329    0.136842\nLength: 251, dtype: float64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_count_columns = [\"current\", \"conversation_id\", \"platform\", \"y\"]\n",
    "author_df = df_RB[author_count_columns]\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\", \"current\"]).sum()\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "distinct_view_columns = [\"current\", \"conversation_id\", \"platform\"]\n",
    "distinct_view_df = df_RB[distinct_view_columns]\n",
    "distinct_views = distinct_view_df.groupby([\"current\", \"conversation_id\", \"platform\"]).size().to_frame('size')\n",
    "distinct_views = distinct_views.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "joined_author_stats = author_df.join(distinct_views)\n",
    "joined_author_stats = joined_author_stats[\"y\"] / joined_author_stats[\"size\"]\n",
    "\n",
    "# prepare for comparison\n",
    "import pandas as pd\n",
    "repetition_predictions = pd.DataFrame(joined_author_stats).rename(columns={0: \"repetition\"})\n",
    "\n",
    "joined_author_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The joined author stats show the repetition probabilities for each of the platforms per conversation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "          repetition_probs\nplatform                  \nreddit            0.208180\ntwitter           0.163322",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repetition_probs</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.208180</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.163322</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetition_probability = joined_author_stats.groupby(\"platform\").mean().to_frame(\"repetition_probs\")\n",
    "repetition_probability\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Response Based Author vision Algorithm (RB)\n",
    "\n",
    "The features are the distance of the author to any tweet in the conversation\n",
    "indicated by the following structures:\n",
    "- subtree to viewed tweet from a tweet the author wrote\n",
    "- root closeness of viewed tweet\n",
    "- time delta to viewed tweet from tweets the author wrote\n",
    "\n",
    "#### Loading the data from the pickled version\n",
    "1. importing libraries\n",
    "2. checking gpu support\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_13026/2951190246.py:9: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "cuda gpu is available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-25 10:21:45.923927: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-25 10:21:47.573205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:47.606765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:47.607072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:48.546465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:48.546818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:48.547113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:48.547373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 2651 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "print(\"cuda gpu is available: {}\".format(is_cuda_gpu_available))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit:\n",
      "0    102587\n",
      "1     13134\n",
      "Name: y, dtype: int64\n",
      "twitter:\n",
      "0    100854\n",
      "1     11599\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df = df[df[\"root_distance_0\"] == 0]\n",
    "# analyze the distribution of reached targets for the sample\n",
    "print(\"reddit:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"reddit\"].y.value_counts())\n",
    "print(\"twitter:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"twitter\"].y.value_counts())\n",
    "# this should be higher for reddit as the unique author / posting ratio is lower for reddit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing a nn model\n",
    "1. separate features\n",
    "2. train models for reddit and twitter\n",
    "3. inspect models for reddit and twitter\n",
    "4. predict the likelihood based on the author has seen a posting\n",
    "5. aggregate likelihoods in order to compute author vision measure\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# training functions\n",
    "def train_model(df):\n",
    "    # dropping non-reddit non-twitter data\n",
    "    df = take_features(df)\n",
    "\n",
    "    # selecting train and test datasets\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train.describe()\n",
    "\n",
    "    # train the model\n",
    "    y = train.y\n",
    "    x = train.drop(\"y\", axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "\n",
    "    print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    model = Sequential([\n",
    "        Dense(1, activation='sigmoid', input_shape=input_shape)\n",
    "    ])\n",
    "\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'mae']\n",
    "    )\n",
    "\n",
    "    # model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.y\n",
    "    test_x = test.drop(\"y\", axis=1)\n",
    "\n",
    "    loss, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is {} and the mae is {}\".format(accuracy, mae))\n",
    "\n",
    "    return x, y, test_x, test_y, model\n",
    "\n",
    "\n",
    "def inspect_model(x, y, test_x, test_y, model):\n",
    "    # have a look at some prediction\n",
    "    reply_distance_2 = test_x[test_x[\"reply_distance_2\"] == 1]\n",
    "    first_rows = reply_distance_2.head(2)\n",
    "    print(first_rows)\n",
    "    model.predict(first_rows)\n",
    "\n",
    "    # let's have a look at the weights and biases of the hidden layer\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]\n",
    "    first_layer_biases = model.layers[0].get_weights()[1]\n",
    "    # print(first_layer_weights)\n",
    "    column_names = x.columns.values\n",
    "    for i in range(len(column_names[:5])):\n",
    "        print(\"feature {} has weight {} \\n\".format(column_names[i], first_layer_weights[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89962, 81)\n",
      "(89962,)\n",
      "2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-25 10:21:49.011235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.011583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.011859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.012300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.012586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.012860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.013187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.013466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 10:21:49.013707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2651 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-08-25 10:21:49.246322: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 4s 1ms/step - loss: 0.3480 - accuracy: 0.8925 - mae: 0.2284\n",
      "703/703 [==============================] - 1s 842us/step - loss: 0.3109 - accuracy: 0.8978 - mae: 0.1821\n",
      "the accuracy on the training set is 0.8977813124656677 and the mae is 0.18212053179740906\n",
      "(92576, 81)\n",
      "(92576,)\n",
      "2.6.0\n",
      "2893/2893 [==============================] - 3s 909us/step - loss: 0.3735 - accuracy: 0.8815 - mae: 0.2433\n",
      "724/724 [==============================] - 1s 860us/step - loss: 0.3383 - accuracy: 0.8865 - mae: 0.1990\n",
      "the accuracy on the training set is 0.8865413665771484 and the mae is 0.1990499496459961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13026/2251630972.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_vision = tw_vision.append(rd_vision)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     current platform      conversation_id  \\\n95855    1468189617111322626  twitter  1468173446882336771   \n95856    1468189617111322626  twitter  1468173446882336771   \n95857    1468189617111322626  twitter  1468173446882336771   \n95858    1468189617111322626  twitter  1468173446882336771   \n95859    1468189617111322626  twitter  1468173446882336771   \n...                      ...      ...                  ...   \n3387379             61016457   reddit             93580398   \n3387380             61016457   reddit             93580398   \n3387381             61016457   reddit             93580398   \n3387382             61016457   reddit             93580398   \n3387383             61016457   reddit             93580398   \n\n                      author  predictions  \n95855    1417122930731929603     0.130517  \n95856    1417122930731929603     0.173836  \n95857    1417122930731929603     0.174965  \n95858    1417122930731929603     0.235658  \n95859    1417122930731929603     0.115358  \n...                      ...          ...  \n3387379             16263075     0.158203  \n3387380             16263075     0.098210  \n3387381             16263075     0.096595  \n3387382             16263075     0.098161  \n3387383             16263075     0.098120  \n\n[228174 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>current</th>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author</th>\n      <th>predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95855</th>\n      <td>1468189617111322626</td>\n      <td>twitter</td>\n      <td>1468173446882336771</td>\n      <td>1417122930731929603</td>\n      <td>0.130517</td>\n    </tr>\n    <tr>\n      <th>95856</th>\n      <td>1468189617111322626</td>\n      <td>twitter</td>\n      <td>1468173446882336771</td>\n      <td>1417122930731929603</td>\n      <td>0.173836</td>\n    </tr>\n    <tr>\n      <th>95857</th>\n      <td>1468189617111322626</td>\n      <td>twitter</td>\n      <td>1468173446882336771</td>\n      <td>1417122930731929603</td>\n      <td>0.174965</td>\n    </tr>\n    <tr>\n      <th>95858</th>\n      <td>1468189617111322626</td>\n      <td>twitter</td>\n      <td>1468173446882336771</td>\n      <td>1417122930731929603</td>\n      <td>0.235658</td>\n    </tr>\n    <tr>\n      <th>95859</th>\n      <td>1468189617111322626</td>\n      <td>twitter</td>\n      <td>1468173446882336771</td>\n      <td>1417122930731929603</td>\n      <td>0.115358</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3387379</th>\n      <td>61016457</td>\n      <td>reddit</td>\n      <td>93580398</td>\n      <td>16263075</td>\n      <td>0.158203</td>\n    </tr>\n    <tr>\n      <th>3387380</th>\n      <td>61016457</td>\n      <td>reddit</td>\n      <td>93580398</td>\n      <td>16263075</td>\n      <td>0.098210</td>\n    </tr>\n    <tr>\n      <th>3387381</th>\n      <td>61016457</td>\n      <td>reddit</td>\n      <td>93580398</td>\n      <td>16263075</td>\n      <td>0.096595</td>\n    </tr>\n    <tr>\n      <th>3387382</th>\n      <td>61016457</td>\n      <td>reddit</td>\n      <td>93580398</td>\n      <td>16263075</td>\n      <td>0.098161</td>\n    </tr>\n    <tr>\n      <th>3387383</th>\n      <td>61016457</td>\n      <td>reddit</td>\n      <td>93580398</td>\n      <td>16263075</td>\n      <td>0.098120</td>\n    </tr>\n  </tbody>\n</table>\n<p>228174 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look for reddit\n",
    "tw_df = df_RB[df_RB[\"platform\"] == \"twitter\"]\n",
    "tw_x, tw_y, tw_test_x, tw_test_y, tw_model = train_model(tw_df)\n",
    "\n",
    "rd_df = df_RB[df_RB[\"platform\"] == \"reddit\"]\n",
    "rd_x, rd_y, rd_test_x, rd_test_y, rd_model = train_model(rd_df)\n",
    "\n",
    "# inspect_model(tw_x, tw_y, tw_test_x, tw_test_y, tw_model)\n",
    "# inspect_model(rd_x, rd_y, rd_test_x, rd_test_y, rd_model)\n",
    "\n",
    "tw_non_features = take_non_features(tw_df)\n",
    "rd_non_features = take_non_features(rd_df)\n",
    "\n",
    "tw_features_y = take_features(tw_df)\n",
    "tw_features = tw_features_y.drop(\"y\", axis=1)\n",
    "rd_features_y = take_features(rd_df)\n",
    "rd_features = rd_features_y.drop(\"y\", axis=1)\n",
    "rd_predictions = rd_model.predict(rd_features)\n",
    "tw_predictions = tw_model.predict(tw_features)\n",
    "\n",
    "tw_vision = tw_non_features.assign(predictions=tw_predictions)\n",
    "rd_vision = rd_non_features.assign(predictions=rd_predictions)\n",
    "\n",
    "combined_vision = tw_vision.append(rd_vision)\n",
    "not_needed_list = [\"beam_node_author\", \"beam_node\", \"has_followed_path\", \"has_follow_path\"]\n",
    "combined_vision = combined_vision.drop(not_needed_list, axis=1)\n",
    "combined_vision_with_author = combined_vision\n",
    "combined_vision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "          avg_predictions\nplatform                 \nreddit           0.123729\ntwitter          0.111273",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>avg_predictions</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.123729</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.111273</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_vision_with_author2 = combined_vision_with_author.groupby(\n",
    "    [\"platform\", \"conversation_id\", \"author\", \"predictions\"]).count()\n",
    "combined_vision_with_author2 = combined_vision_with_author2.reset_index()\n",
    "combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).sum()\n",
    "combined_vision_with_author2[\n",
    "    \"avg_predictions\"] = combined_vision_with_author2.predictions / combined_vision_with_author2.current\n",
    "combined_vision_with_author2 = combined_vision_with_author2.drop([\"current\", \"predictions\"], axis=1)\n",
    "combined_vision_with_author2 = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "rb_result = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "\n",
    "# prepare for comparison\n",
    "RB_predictions= combined_vision_with_author2.rename(columns={\"avg_predictions\": \"RB\"})\n",
    "\n",
    "rb_result = rb_result.groupby([\"platform\"]).mean()\n",
    "rb_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "          predictions\nplatform             \nreddit       0.121108\ntwitter      0.113863",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predictions</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.121108</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.113863</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_vision = combined_vision.drop(\"author\", axis=1)\n",
    "gpm = combined_vision.groupby([\"platform\", \"conversation_id\", \"current\"]).mean()\n",
    "gpm_per_conversation = gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "gpm_per_platform = gpm.groupby(by=[\"platform\"]).mean()\n",
    "gpm_per_platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                  repetition_probs  predictions\nrepetition_probs               1.0          1.0\npredictions                    1.0          1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repetition_probs</th>\n      <th>predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>repetition_probs</th>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>predictions</th>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = repetition_probability.join(gpm_per_platform)\n",
    "probabilities.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpretation the correlation between probabilities and the RB-predictions\n",
    "- This means that the neural network computes a linear function of the repetition probabilities based on the computation of the y functions\n",
    "- The probabilities are very low for both reddit and twitter but in a comparable area\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Author Prediction\n",
    "\n",
    "It is possible to predict an author or \"new author\" at same time by defining categories as 1 if a author is to be predicted but\n",
    "only if it is not a new author. Because of memory, only twitter or reddit data can be predicted in one run.\n",
    "The full dataset does not fit in laptops memory and is computed on the cluster (which in turn has no gpu support)\n",
    "\n",
    "The probability of predicting an author is calculated for each relationship (root distance to another node, reply distance to other nodes, and reply distance to nodes with the same author. In future also the author follower network will be included in the feature set.\n",
    "\n",
    "The overall sum of the probability of predicting an author (in average) will be interpreted as the likelihood of any author writing in any time in the conversation (again, because it is not a new author). This will then seen as the author being present in the conversation because it is another measure of a author being available in all branches and positions in the conversation.\n",
    "\n",
    "#### Create a one hot vector representation of the possible authors\n",
    "- create an artificial user that represents a new user in a conversation up to that point\n",
    "- get a matrix with the authors as columns and a 1 if the author wrote the post\n",
    "- join it with the feature matrix\n",
    "- drop the author column\n",
    "\n",
    "\n",
    "#### Training NN to predict the author that would write next\n",
    "- included a \"new author\" category to capture predicting unknown authors\n",
    "- using multi-class classification (instead of multi-label)\n",
    "- relu/sigmoid activation functions have same effect\n",
    "- precision grew significantly when adding more than 3-5 layers\n",
    "\n",
    "#### Predicting the author presence based on prediction probabilities\n",
    "- compute predictions for the whole dataframe\n",
    "- drop features and non-features except conversation and platform\n",
    "- wide to long the authors to make them a index\n",
    "- groupby conversation and platform\n",
    "\n",
    "#### Notes\n",
    "- inserting the new author column increased precision times 10\n",
    "- categorical accuracy and regular accuracy match (which is weird)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def calculate_author_predictions(df):\n",
    "    # compute a fake user that symbolizes that the given user has not been seen at a given stage in the conversation\n",
    "    df_conversation_authors = df[[\"conversation_id\", \"author\", \"current_time\"]]\n",
    "    first_times = df_conversation_authors.groupby([\"conversation_id\", \"author\"]).min()\n",
    "\n",
    "    def is_new_author(row):\n",
    "        earliest_author_post = first_times.loc[row[\"conversation_id\"],row[\"author\"]]\n",
    "        current_post_time = row[\"current_time\"]\n",
    "        return  earliest_author_post >= current_post_time\n",
    "\n",
    "    new_author_column = df[[\"conversation_id\", \"author\", \"current_time\"]].apply(is_new_author, axis=1)\n",
    "    new_author_column= new_author_column.rename(columns={'current_time':\"Author_is_new\"})\n",
    "    new_author_column.value_counts()\n",
    "\n",
    "\n",
    "    def compute_new_author_column(df):\n",
    "        import pandas as pd\n",
    "        author_one_hot = pd.get_dummies(df.author, prefix=\"Author\", sparse=True)\n",
    "        # make author cells 0 that are now represented as \"new author\"\n",
    "        author_one_hot = author_one_hot.astype(bool).apply(lambda x: x & ~new_author_column.Author_is_new).astype(int)\n",
    "        # delete columns that are all 0\n",
    "        author_one_hot = author_one_hot.loc[:, (author_one_hot != 0).any(axis=0)]\n",
    "        # join the new author column to the labels\n",
    "        labels = author_one_hot.join(new_author_column.astype(int))\n",
    "        features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "        combined_set = features.join(labels)\n",
    "        return combined_set, features, labels\n",
    "\n",
    "    combined_set, features, labels = compute_new_author_column(df)\n",
    "\n",
    "    from keras.optimizer_v2.rmsprop import RMSprop  # selecting train and test datasets\n",
    "    train, test = train_test_split(combined_set, test_size=0.2, shuffle=False)\n",
    "    print(\"split training and test set\")\n",
    "\n",
    "    # train the model\n",
    "    y = train.drop(features.columns, axis=1)\n",
    "    x = train.drop(labels.columns, axis=1)\n",
    "    print(\"seperated features and y with shapes:\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "    # print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    output_shape = y.shape[1]\n",
    "    print(\"inputshape is {}\".format(input_shape))\n",
    "    model = Sequential([\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='softmax', input_shape=input_shape)\n",
    "    ])\n",
    "    print(\"defined model as {}\".format(model.layers))\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy', 'accuracy' ,'mae']\n",
    "    )\n",
    "    print(\"compiled model\")\n",
    "    #model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    #model.fit(x, y, epochs=10, shuffle=True)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.drop(features.columns, axis=1)\n",
    "    test_x = test.drop(labels.columns, axis=1)\n",
    "    #test_x = test_x.drop(\"timedelta\", axis=1)\n",
    "\n",
    "    loss, cat_accuracy, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is cat acc {}, reg acc {} and the mae is {}\".format(cat_accuracy, accuracy, mae))\n",
    "\n",
    "    all_features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "    print(\"start generating author predictions for the whole data set\")\n",
    "    predictions = model.predict(all_features, use_multiprocessing=True)\n",
    "    print(\"end generating author predictions for the whole data set\")\n",
    "    column_names = labels.columns\n",
    "    predictions = pd.DataFrame(predictions, columns=column_names)\n",
    "    print(type(predictions))\n",
    "    print(predictions.shape)\n",
    "\n",
    "\n",
    "    all_non_features = df[[\"conversation_id\", \"platform\"]]\n",
    "    print(type(all_non_features))\n",
    "    print(all_non_features.shape)\n",
    "    all_non_features.reset_index(drop=True, inplace=True)\n",
    "    joined_dataframe = all_non_features.join(predictions)\n",
    "    #print(joined_dataframe.Author_is_new.describe()) # no idea why that is the same prediction of all the rows\n",
    "\n",
    "    joined_dataframe = joined_dataframe.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "    author_predictions_existing = joined_dataframe.drop([\"Author_is_new\"], axis=1)\n",
    "    author_predictions_existing.reset_index(level=['platform', 'conversation_id'],inplace=True)\n",
    "    print(\"start converting author hot vectors beack to one author column\")\n",
    "    author_predictions_existing_reshaped = pd.wide_to_long(author_predictions_existing, stubnames=\"Author_\", i=['platform', 'conversation_id'], j=\"author_id\")\n",
    "    print(\"end converting author hot vectors beack to one author column\")    \n",
    "    return author_predictions_existing_reshaped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split training and test set\n",
      "seperated features and y with shapes:\n",
      "(92576, 117)\n",
      "(92576, 765)\n",
      "inputshape is (117,)\n",
      "defined model as [<keras.layers.core.Dense object at 0x7fa92cd75040>, <keras.layers.core.Dense object at 0x7fa92cd75550>, <keras.layers.core.Dense object at 0x7fa92cd75c10>, <keras.layers.core.Dense object at 0x7fa92cd7a040>]\n",
      "compiled model\n",
      "2893/2893 [==============================] - 7s 2ms/step - loss: 3.7347 - categorical_accuracy: 0.4407 - accuracy: 0.4407 - mae: 0.0021\n",
      "724/724 [==============================] - 1s 1ms/step - loss: 5.5869 - categorical_accuracy: 0.5227 - accuracy: 0.5227 - mae: 0.0020\n",
      "the accuracy on the training set is cat acc 0.5227478742599487, reg acc 0.5227478742599487 and the mae is 0.001997362356632948\n",
      "start generating author predictions for the whole data set\n",
      "end generating author predictions for the whole data set\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(115721, 765)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(115721, 2)\n",
      "start converting author hot vectors beack to one author column\n",
      "end converting author hot vectors beack to one author column\n",
      "split training and test set\n",
      "seperated features and y with shapes:\n",
      "(89962, 117)\n",
      "(89962, 532)\n",
      "inputshape is (117,)\n",
      "defined model as [<keras.layers.core.Dense object at 0x7fa927944370>, <keras.layers.core.Dense object at 0x7fa927944a00>, <keras.layers.core.Dense object at 0x7fa927944b20>, <keras.layers.core.Dense object at 0x7fa927944460>]\n",
      "compiled model\n",
      "2812/2812 [==============================] - 6s 2ms/step - loss: 2.7822 - categorical_accuracy: 0.5958 - accuracy: 0.5958 - mae: 0.0024\n",
      "703/703 [==============================] - 1s 1ms/step - loss: 5.3979 - categorical_accuracy: 0.4998 - accuracy: 0.4998 - mae: 0.0027\n",
      "the accuracy on the training set is cat acc 0.499799907207489, reg acc 0.499799907207489 and the mae is 0.002714994829148054\n",
      "start generating author predictions for the whole data set\n",
      "end generating author predictions for the whole data set\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(112453, 532)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(112453, 2)\n",
      "start converting author hot vectors beack to one author column\n",
      "end converting author hot vectors beack to one author column\n"
     ]
    }
   ],
   "source": [
    "df_PB_reddit = df_PB[df_PB[\"platform\"] == \"reddit\"]\n",
    "prediction_result_reddit = calculate_author_predictions(df_PB_reddit)\n",
    "\n",
    "df_PB_twitter = df_PB[df_PB[\"platform\"] == \"twitter\"]\n",
    "prediction_result_twitter = calculate_author_predictions(df_PB_twitter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13026/2702656795.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  PB_predictions = prediction_result_reddit.append(prediction_result_twitter)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                        PB\nplatform conversation_id     author_id                    \nreddit   840660              237258               0.000348\n                             538210               0.050401\n                             539975               0.000159\n                             615676               0.000480\n                             894509               0.000014\n...                                                    ...\ntwitter  1552230764602851329 1542227574889250816  0.000079\n                             1542398467171139584  0.000950\n                             1548805093268635650  0.000017\n                             1550245634757873668  0.000159\n                             1550780390167699456  0.002908\n\n[169163 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>PB</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">reddit</th>\n      <th rowspan=\"5\" valign=\"top\">840660</th>\n      <th>237258</th>\n      <td>0.000348</td>\n    </tr>\n    <tr>\n      <th>538210</th>\n      <td>0.050401</td>\n    </tr>\n    <tr>\n      <th>539975</th>\n      <td>0.000159</td>\n    </tr>\n    <tr>\n      <th>615676</th>\n      <td>0.000480</td>\n    </tr>\n    <tr>\n      <th>894509</th>\n      <td>0.000014</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th rowspan=\"5\" valign=\"top\">1552230764602851329</th>\n      <th>1542227574889250816</th>\n      <td>0.000079</td>\n    </tr>\n    <tr>\n      <th>1542398467171139584</th>\n      <td>0.000950</td>\n    </tr>\n    <tr>\n      <th>1548805093268635650</th>\n      <td>0.000017</td>\n    </tr>\n    <tr>\n      <th>1550245634757873668</th>\n      <td>0.000159</td>\n    </tr>\n    <tr>\n      <th>1550780390167699456</th>\n      <td>0.002908</td>\n    </tr>\n  </tbody>\n</table>\n<p>169163 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction_result_reddit\n",
    "# prediction_result_twitter\n",
    "\n",
    "PB_predictions = prediction_result_reddit.append(prediction_result_twitter)\n",
    "PB_predictions = PB_predictions.rename(columns={\"Author_\": \"PB\"})\n",
    "PB_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Author Centrality\n",
    "- The centrality was already computed when creating the dataset as it is based on graph measures primarily\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  centrality\nplatform conversation_id     author                         \nreddit   840660              538210                 0.166667\n                             819234                 0.166667\n                             1138338                0.166667\n                             4244958                0.166667\n                             7391803                0.222222\n...                                                      ...\ntwitter  1552230764602851329 1433445933518409730    0.000000\n                             1462782939071225858    0.000000\n                             1463651814616846336    1.880952\n                             1495016557587668994    0.500000\n                             1534920737659699201    0.000000\n\n[4948 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>centrality</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">reddit</th>\n      <th rowspan=\"5\" valign=\"top\">840660</th>\n      <th>538210</th>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>819234</th>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>1138338</th>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>4244958</th>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>7391803</th>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th rowspan=\"5\" valign=\"top\">1552230764602851329</th>\n      <th>1433445933518409730</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1462782939071225858</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1463651814616846336</th>\n      <td>1.880952</td>\n    </tr>\n    <tr>\n      <th>1495016557587668994</th>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1534920737659699201</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>4948 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_centrality_avg = df_centrality.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "df_centrality_avg = df_centrality_avg.assign(centrality=df_centrality_avg.centrality_score / df_centrality_avg.root_distance_avg)\n",
    "df_centrality_avg = df_centrality_avg.drop([\"centrality_score\", \"root_distance_avg\"], axis=1)\n",
    "df_centrality_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combined Analysis\n",
    "- join three author vision measures into one dataframe\n",
    "- add author centrality to the same dataframe\n",
    "- correlate the measures\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    RB        PB  centrality  baseline  \\\nplatform conversation_id                                                 \nreddit   840660               0.104544  0.000742    0.536949  0.505706   \n         1513432              0.125011  0.000772    0.375108  0.511046   \n         2887680              0.134412  0.000874    0.176215  0.638068   \n         4695940              0.105950  0.000726    0.253188  0.508409   \n         4947747              0.114354  0.000741    0.388097  0.520081   \n...                                ...       ...         ...       ...   \ntwitter  1551870406843420675  0.147236  0.000884    0.299603  0.400893   \n         1551971727604940800  0.101646  0.000820    0.345525  0.506596   \n         1552024342661251072  0.095329  0.000811    0.258068  0.484611   \n         1552225696029855744  0.094648  0.000826    0.254102  0.518544   \n         1552230764602851329  0.099305  0.000770    0.292517  0.470529   \n\n                              repetition  \nplatform conversation_id                  \nreddit   840660                 0.046076  \n         1513432                0.104839  \n         2887680                0.388889  \n         4695940                0.083333  \n         4947747                0.187192  \n...                                  ...  \ntwitter  1551870406843420675    0.428571  \n         1551971727604940800    0.069697  \n         1552024342661251072    0.094505  \n         1552225696029855744    0.063277  \n         1552230764602851329    0.136842  \n\n[251 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>RB</th>\n      <th>PB</th>\n      <th>centrality</th>\n      <th>baseline</th>\n      <th>repetition</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">reddit</th>\n      <th>840660</th>\n      <td>0.104544</td>\n      <td>0.000742</td>\n      <td>0.536949</td>\n      <td>0.505706</td>\n      <td>0.046076</td>\n    </tr>\n    <tr>\n      <th>1513432</th>\n      <td>0.125011</td>\n      <td>0.000772</td>\n      <td>0.375108</td>\n      <td>0.511046</td>\n      <td>0.104839</td>\n    </tr>\n    <tr>\n      <th>2887680</th>\n      <td>0.134412</td>\n      <td>0.000874</td>\n      <td>0.176215</td>\n      <td>0.638068</td>\n      <td>0.388889</td>\n    </tr>\n    <tr>\n      <th>4695940</th>\n      <td>0.105950</td>\n      <td>0.000726</td>\n      <td>0.253188</td>\n      <td>0.508409</td>\n      <td>0.083333</td>\n    </tr>\n    <tr>\n      <th>4947747</th>\n      <td>0.114354</td>\n      <td>0.000741</td>\n      <td>0.388097</td>\n      <td>0.520081</td>\n      <td>0.187192</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th>1551870406843420675</th>\n      <td>0.147236</td>\n      <td>0.000884</td>\n      <td>0.299603</td>\n      <td>0.400893</td>\n      <td>0.428571</td>\n    </tr>\n    <tr>\n      <th>1551971727604940800</th>\n      <td>0.101646</td>\n      <td>0.000820</td>\n      <td>0.345525</td>\n      <td>0.506596</td>\n      <td>0.069697</td>\n    </tr>\n    <tr>\n      <th>1552024342661251072</th>\n      <td>0.095329</td>\n      <td>0.000811</td>\n      <td>0.258068</td>\n      <td>0.484611</td>\n      <td>0.094505</td>\n    </tr>\n    <tr>\n      <th>1552225696029855744</th>\n      <td>0.094648</td>\n      <td>0.000826</td>\n      <td>0.254102</td>\n      <td>0.518544</td>\n      <td>0.063277</td>\n    </tr>\n    <tr>\n      <th>1552230764602851329</th>\n      <td>0.099305</td>\n      <td>0.000770</td>\n      <td>0.292517</td>\n      <td>0.470529</td>\n      <td>0.136842</td>\n    </tr>\n  </tbody>\n</table>\n<p>251 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_comparison_table = RB_predictions.join(PB_predictions).join(df_centrality_avg).join(baseline_predictions)\n",
    "\n",
    "prediction_comparison_table = prediction_comparison_table.groupby([\"platform\", \"conversation_id\"]).mean().join(repetition_predictions)\n",
    "prediction_comparison_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "corr_matrix = prediction_comparison_table.corr(method=\"pearson\")\n",
    "sn.heatmap(corr_matrix, annot=True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}