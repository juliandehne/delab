{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 4627 conversations 1292 are shared in all datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": "(36744, 127)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from random import sample\n",
    "# PB: prediction based algorithm\n",
    "# RB: response based algoritm\n",
    "\n",
    "\n",
    "file_name_PB = \"data/vision_forward_graph_data_08_09_22.pkl\"\n",
    "with open(file_name_PB, 'rb') as f:\n",
    "    df_PB = pickle.load(f)\n",
    "    #df_PB.sort_values(by=\"platform\", inplace=True, ignore_index=True)\n",
    "\n",
    "file_name_RB = \"data/vision_graph_data_remote_23_08_22.pkl\"\n",
    "with open(file_name_RB, 'rb') as f:\n",
    "    df_RB = pickle.load(f)\n",
    "\n",
    "\n",
    "file_name_baseline = \"data/vision_baseline_graph_data_24_08_22.pkl\"\n",
    "with open(file_name_RB, 'rb') as f:\n",
    "    df_baseline = pickle.load(f)\n",
    "\n",
    "file_name_centrality = \"data/author_centrality_remote.pkl\"\n",
    "with open(file_name_centrality, 'rb') as f:\n",
    "    df_centrality = pickle.load(f)\n",
    "\n",
    "common_conversation_ids = set(df_PB.conversation_id).intersection(df_RB.conversation_id).intersection(\n",
    "    df_centrality.conversation_id).intersection(df_baseline.conversation_id)\n",
    "all_conversation_ids = set(df_PB.conversation_id).union(df_RB.conversation_id).union(df_centrality.conversation_id).union(df_baseline.conversation_id)\n",
    "all_conversation_count = len(all_conversation_ids)\n",
    "common_conversation_count = len(common_conversation_ids)\n",
    "# reducing the sample size for testing\n",
    "common_conversation_ids = sample(common_conversation_ids, 50)\n",
    "\n",
    "print(\"From {} conversations {} are shared in all datasets\".format(all_conversation_count, common_conversation_count))\n",
    "\n",
    "\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(common_conversation_ids)]\n",
    "df_RB = df_RB[df_RB.conversation_id.isin(common_conversation_ids)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(common_conversation_ids)]\n",
    "df_baseline = df_baseline[df_baseline.conversation_id.isin(common_conversation_ids)]\n",
    "df_PB.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility Functions\n",
    "\n",
    "The following cell contains utility functions that are needed for all the different algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def is_not_reddit_or_twitter(text):\n",
    "    if text == \"reddit\" or text == \"twitter\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def equalize_samples(df):\n",
    "    \"\"\"\n",
    "    this approximates the same number of conversations for both platforms\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_conversations_twitter = set(df[df[\"platform\"] == \"twitter\"].conversation_id.tolist())\n",
    "    df_conversations_reddit = set(df[df[\"platform\"] == \"reddit\"].conversation_id.tolist())\n",
    "    reddit_data_count = df.loc[df.platform == \"reddit\", 'platform'].count()\n",
    "    twitter_data_count = df.loc[df.platform == \"twitter\", 'platform'].count()\n",
    "    # assert twitter_data_count > reddit_data_count, \"counts (reddit, twitter) are ({},{}):\".format(reddit_data_count, twitter_data_count)\n",
    "    current_count = 0\n",
    "    n = 1\n",
    "    smaller_count = reddit_data_count\n",
    "    df_conversations = df_conversations_twitter\n",
    "    if reddit_data_count > twitter_data_count:\n",
    "        smaller_count = twitter_data_count\n",
    "        df_conversations = df_conversations_reddit\n",
    "    while current_count < smaller_count:\n",
    "        chosen_conversation_ids = sample(df_conversations, n)\n",
    "        df_candidate = df[df[\"conversation_id\"].isin(chosen_conversation_ids)]\n",
    "        n = n + 1\n",
    "        current_count = df_candidate.shape[0]\n",
    "    print(\"chosen {} conversations and gotten {} from twitter compared to {} from reddit\".format(n, current_count,\n",
    "                                                                                                 reddit_data_count))\n",
    "    not_chosen_conversation_ids = set(df_conversations) - set(chosen_conversation_ids)\n",
    "    df_result = df[~df[\"conversation_id\"].isin(not_chosen_conversation_ids)]\n",
    "    return df_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-Features\n",
    "\n",
    "All the cells contain a number of columns that have a meaning in the conversation but are not features to train the NN with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "\n",
    "# some utility functions to take the columns that are used as features\n",
    "non_feature_list = [\"current\", \"beam_node\", \"conversation_id\", \"platform\", \"has_followed_path\", \"has_follow_path\",\n",
    "                    \"beam_node_author\", \"author\"]\n",
    "\n",
    "\n",
    "def take_features(df, additional_non_features=[]):\n",
    "    non_feature_list2 = non_feature_list + additional_non_features\n",
    "    df = df.drop(non_feature_list2, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def take_non_features(df, additional_non_features=[]):\n",
    "    non_feature_list2 = non_feature_list + additional_non_features\n",
    "    column_names = df.columns.values\n",
    "    feature_list = [column_name for column_name in column_names if column_name not in non_feature_list2]\n",
    "    df = df.drop(feature_list, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_timedelta(df):\n",
    "    # normalize timedelta (put between 0 and 1)\n",
    "    dt = df.timedelta\n",
    "    timedelta_normalized = (dt - dt.min()) / (dt.max() - dt.min())\n",
    "    df = df.assign(timedelta=timedelta_normalized)\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Cleaning and Data preperation\n",
    "- Delete rows that are neither twitter or reddit data\n",
    "- normalize time deltas\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen 32 conversations and gotten 17537 from twitter compared to 20522 from reddit\n"
     ]
    }
   ],
   "source": [
    "# filtering data that is not twitter or reddit\n",
    "def delete_not_twitter_not_reddit(df):\n",
    "    platform = df.platform\n",
    "    to_delete_rows = platform.apply(lambda x: is_not_reddit_or_twitter(x))\n",
    "    df = df.drop(df[to_delete_rows].index)\n",
    "    return df\n",
    "\n",
    "df_RB = delete_not_twitter_not_reddit(df_RB)\n",
    "df_PB = delete_not_twitter_not_reddit(df_PB)\n",
    "df_centrality = delete_not_twitter_not_reddit(df_centrality)\n",
    "\n",
    "df_RB = equalize_samples(df_RB)\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(df_RB.conversation_id)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(df_RB.conversation_id)]\n",
    "df_baseline  = df_baseline[df_baseline.conversation_id.isin(df_RB.conversation_id)]\n",
    "\n",
    "df_RB = normalize_timedelta(df_RB)\n",
    "df_PB = normalize_timedelta(df_PB)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline for Author vision\n",
    "- uses selected values as a distance measure\n",
    "- probability of having seen a tweet is reduced by a half with each step in the reply hierachy\n",
    "- probability of having seen a tweet is reduced by a quarter for each step away from the root\n",
    "- probabiliy of having seen a tweet is increased for each path in the follower network to the tweet (forthcoming)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# in order to allow the comparison, the filter from the other notebook needs to run and the predictions, too\n",
    "reply_filter_col = [col for col in df_baseline if col.startswith('reply')]\n",
    "root_distance_filter_col = [col for col in df_baseline if col.startswith('root')]\n",
    "reply_columns = df_baseline[reply_filter_col]\n",
    "root_distance_columns = df_baseline[root_distance_filter_col]\n",
    "reply_cs = reply_columns.sum(axis=1)\n",
    "root_distance_cs = root_distance_columns.sum(axis=1)\n",
    "rcs_not_null = [i for i in reply_cs.tolist() if i != 0]\n",
    "root_reply_combined = (root_distance_cs + reply_cs)\n",
    "root_reply_combined = (root_reply_combined - root_reply_combined.min()) / (\n",
    "        root_reply_combined.max() - root_reply_combined.min())\n",
    "combined = [i for i in root_reply_combined.tolist() if i != 0]\n",
    "df_baseline = df_baseline.assign(root_reply_combined=root_reply_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  baseline\nplatform conversation_id     author                       \nreddit   2087645             3853842              0.590909\n                             21221970             0.631579\n                             25043219             0.607143\n                             38759243             0.553571\n                             87768819             0.545455\n...                                                    ...\ntwitter  1552015261212954624 1418169381545259008  0.375000\n                             1430654330932236299  0.295455\n                             1440140450434740232  0.300000\n                             1450261887208153099  0.354167\n                             1513254155393114112  0.333333\n\n[725 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>baseline</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">reddit</th>\n      <th rowspan=\"5\" valign=\"top\">2087645</th>\n      <th>3853842</th>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <th>21221970</th>\n      <td>0.631579</td>\n    </tr>\n    <tr>\n      <th>25043219</th>\n      <td>0.607143</td>\n    </tr>\n    <tr>\n      <th>38759243</th>\n      <td>0.553571</td>\n    </tr>\n    <tr>\n      <th>87768819</th>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th rowspan=\"5\" valign=\"top\">1552015261212954624</th>\n      <th>1418169381545259008</th>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <th>1430654330932236299</th>\n      <td>0.295455</td>\n    </tr>\n    <tr>\n      <th>1440140450434740232</th>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <th>1450261887208153099</th>\n      <td>0.354167</td>\n    </tr>\n    <tr>\n      <th>1513254155393114112</th>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n<p>725 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline_with_authors = df_baseline[[\"root_reply_combined\", \"conversation_id\", \"author\", \"platform\"]]\n",
    "# df_reshaped = pd.pivot_table(df_data,index=[\"conversation_id\", \"current\"], columns=[\"root_reply_combined\"],aggfunc = np.mean)\n",
    "baseline_gpm = df_baseline_with_authors.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "baseline_predictions = baseline_gpm\n",
    "baseline_predictions.rename(columns={\"root_reply_combined\": \"baseline\"},inplace=True)\n",
    "baseline_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "          baseline\nplatform          \nreddit    0.478370\ntwitter   0.463158",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseline</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.478370</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.463158</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_gpm_conversation = baseline_gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "baseline_gpm = baseline_gpm_conversation.groupby(by=[\"platform\"]).mean()\n",
    "baseline_gpm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repetition Probabilities\n",
    "\n",
    "#### Analyzing the probability of an author writing repeatedly in the same conversation\n",
    "1. sum up the amounts y == 1 (because an author has answered himself)\n",
    "2. sum chances of an author seeing himself write\n",
    "3. calculate a measure of how likely it is that an author sees himself repeated as a test for the nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "platform  conversation_id    \nreddit    2087645                0.252101\n          2258028                0.464286\n          3237826                0.147465\n          4264097                0.241015\n          6879611                0.156863\n          13590426               0.384615\n          17172485               0.250000\n          19161388               0.252941\n          20520581               0.144712\n          20889898               0.090580\n          23330036               0.103333\n          31131919               0.363636\n          32545716               0.147619\n          32854345               0.107692\n          34665923               0.266667\n          35562400               0.247619\n          35824217               0.263158\n          42452689               0.116959\n          49074540               0.190476\n          66271665               0.333333\n          67066456               0.143353\n          67519271               0.333333\n          68536375               0.333333\n          71171604               0.089593\n          73497915               0.132240\n          78738990               0.326316\n          79403147               0.058733\n          80796454               0.082857\n          86717929               0.226415\n          91662800               0.358974\n          96138222               0.500000\ntwitter   1455504947999657990    0.344839\n          1457889187177410564    0.099567\n          1538176825603543041    0.036066\n          1543194290284806145    0.184783\n          1543257148565737472    0.158183\n          1543528239288786945    0.054588\n          1543910459891384326    0.047028\n          1543936160938041350    0.277607\n          1543996112536829952    0.121212\n          1544177219420839936    0.111111\n          1544672441456279552    0.294372\n          1544971258370326528    0.316667\n          1552015261212954624    0.116923\ndtype: float64"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_count_columns = [\"current\", \"conversation_id\", \"platform\", \"y\"]\n",
    "author_df = df_RB[author_count_columns]\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\", \"current\"]).sum()\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "distinct_view_columns = [\"current\", \"conversation_id\", \"platform\"]\n",
    "distinct_view_df = df_RB[distinct_view_columns]\n",
    "distinct_views = distinct_view_df.groupby([\"current\", \"conversation_id\", \"platform\"]).size().to_frame('size')\n",
    "distinct_views = distinct_views.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "joined_author_stats = author_df.join(distinct_views)\n",
    "joined_author_stats = joined_author_stats[\"y\"] / joined_author_stats[\"size\"]\n",
    "\n",
    "# prepare for comparison\n",
    "import pandas as pd\n",
    "repetition_predictions = pd.DataFrame(joined_author_stats).rename(columns={0: \"repetition\"})\n",
    "\n",
    "joined_author_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The joined author stats show the repetition probabilities for each of the platforms per conversation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "          repetition_probs\nplatform                  \nreddit            0.229362\ntwitter           0.166380",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repetition_probs</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.229362</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.166380</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetition_probability = joined_author_stats.groupby(\"platform\").mean().to_frame(\"repetition_probs\")\n",
    "repetition_probability\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Response Based Author vision Algorithm (RB)\n",
    "\n",
    "The features are the distance of the author to any tweet in the conversation\n",
    "indicated by the following structures:\n",
    "- subtree to viewed tweet from a tweet the author wrote\n",
    "- root closeness of viewed tweet\n",
    "- time delta to viewed tweet from tweets the author wrote\n",
    "\n",
    "#### Loading the data from the pickled version\n",
    "1. importing libraries\n",
    "2. checking gpu support\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda gpu is available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-25 09:41:50.598702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 09:41:50.600341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 09:41:50.600555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 09:41:50.607028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 09:41:50.607495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-25 09:41:50.610157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 2651 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "print(\"cuda gpu is available: {}\".format(is_cuda_gpu_available))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit:\n",
      "0    15228\n",
      "1     2309\n",
      "Name: y, dtype: int64\n",
      "twitter:\n",
      "0    13246\n",
      "1     2976\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df = df[df[\"root_distance_0\"] == 0]\n",
    "# analyze the distribution of reached targets for the sample\n",
    "print(\"reddit:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"reddit\"].y.value_counts())\n",
    "print(\"twitter:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"twitter\"].y.value_counts())\n",
    "# this should be higher for reddit as the unique author / posting ratio is lower for reddit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing a nn model\n",
    "1. separate features\n",
    "2. train models for reddit and twitter\n",
    "3. inspect models for reddit and twitter\n",
    "4. predict the likelihood based on the author has seen a posting\n",
    "5. aggregate likelihoods in order to compute author vision measure\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# training functions\n",
    "def train_model(df):\n",
    "    # dropping non-reddit non-twitter data\n",
    "    df = take_features(df)\n",
    "\n",
    "    # selecting train and test datasets\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train.describe()\n",
    "\n",
    "    # train the model\n",
    "    y = train.y\n",
    "    x = train.drop(\"y\", axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "\n",
    "    print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    model = Sequential([\n",
    "        Dense(1, activation='sigmoid', input_shape=input_shape)\n",
    "    ])\n",
    "\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'mae']\n",
    "    )\n",
    "\n",
    "    # model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.y\n",
    "    test_x = test.drop(\"y\", axis=1)\n",
    "\n",
    "    loss, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is {} and the mae is {}\".format(accuracy, mae))\n",
    "\n",
    "    return x, y, test_x, test_y, model\n",
    "\n",
    "\n",
    "def inspect_model(x, y, test_x, test_y, model):\n",
    "    # have a look at some prediction\n",
    "    reply_distance_2 = test_x[test_x[\"reply_distance_2\"] == 1]\n",
    "    first_rows = reply_distance_2.head(2)\n",
    "    print(first_rows)\n",
    "    model.predict(first_rows)\n",
    "\n",
    "    # let's have a look at the weights and biases of the hidden layer\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]\n",
    "    first_layer_biases = model.layers[0].get_weights()[1]\n",
    "    # print(first_layer_weights)\n",
    "    column_names = x.columns.values\n",
    "    for i in range(len(column_names[:5])):\n",
    "        print(\"feature {} has weight {} \\n\".format(column_names[i], first_layer_weights[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12977, 81)\n",
      "(12977,)\n",
      "2.6.0\n",
      "406/406 [==============================] - 1s 1ms/step - loss: 0.5723 - accuracy: 0.7856 - mae: 0.4210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "102/102 [==============================] - 0s 820us/step - loss: 0.5040 - accuracy: 0.8179 - mae: 0.3643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "the accuracy on the training set is 0.8178736567497253 and the mae is 0.3642803132534027\n",
      "(14029, 81)\n",
      "(14029,)\n",
      "2.6.0\n",
      "439/439 [==============================] - 1s 1ms/step - loss: 0.5199 - accuracy: 0.8391 - mae: 0.3867\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "110/110 [==============================] - 0s 852us/step - loss: 0.4353 - accuracy: 0.8666 - mae: 0.3162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "the accuracy on the training set is 0.866590678691864 and the mae is 0.316153883934021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8875/1732011117.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_vision = tw_vision.append(rd_vision)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     current platform      conversation_id     author  \\\n190039   1552036981517045763  twitter  1552015261212954624  266057209   \n190040   1552036981517045763  twitter  1552015261212954624  266057209   \n190041   1552036981517045763  twitter  1552015261212954624  266057209   \n190042   1552036981517045763  twitter  1552015261212954624  266057209   \n190043   1552036981517045763  twitter  1552015261212954624  266057209   \n...                      ...      ...                  ...        ...   \n3352927             87094374   reddit             73497915   66278915   \n3352928             87094374   reddit             73497915   66278915   \n3352929             87094374   reddit             73497915   66278915   \n3352930             87094374   reddit             73497915   66278915   \n3352931             87094374   reddit             73497915   66278915   \n\n         predictions  \n190039      0.268709  \n190040      0.268719  \n190041      0.309927  \n190042      0.268664  \n190043      0.315565  \n...              ...  \n3352927     0.286159  \n3352928     0.274029  \n3352929     0.281929  \n3352930     0.279803  \n3352931     0.270250  \n\n[33759 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>current</th>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author</th>\n      <th>predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>190039</th>\n      <td>1552036981517045763</td>\n      <td>twitter</td>\n      <td>1552015261212954624</td>\n      <td>266057209</td>\n      <td>0.268709</td>\n    </tr>\n    <tr>\n      <th>190040</th>\n      <td>1552036981517045763</td>\n      <td>twitter</td>\n      <td>1552015261212954624</td>\n      <td>266057209</td>\n      <td>0.268719</td>\n    </tr>\n    <tr>\n      <th>190041</th>\n      <td>1552036981517045763</td>\n      <td>twitter</td>\n      <td>1552015261212954624</td>\n      <td>266057209</td>\n      <td>0.309927</td>\n    </tr>\n    <tr>\n      <th>190042</th>\n      <td>1552036981517045763</td>\n      <td>twitter</td>\n      <td>1552015261212954624</td>\n      <td>266057209</td>\n      <td>0.268664</td>\n    </tr>\n    <tr>\n      <th>190043</th>\n      <td>1552036981517045763</td>\n      <td>twitter</td>\n      <td>1552015261212954624</td>\n      <td>266057209</td>\n      <td>0.315565</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3352927</th>\n      <td>87094374</td>\n      <td>reddit</td>\n      <td>73497915</td>\n      <td>66278915</td>\n      <td>0.286159</td>\n    </tr>\n    <tr>\n      <th>3352928</th>\n      <td>87094374</td>\n      <td>reddit</td>\n      <td>73497915</td>\n      <td>66278915</td>\n      <td>0.274029</td>\n    </tr>\n    <tr>\n      <th>3352929</th>\n      <td>87094374</td>\n      <td>reddit</td>\n      <td>73497915</td>\n      <td>66278915</td>\n      <td>0.281929</td>\n    </tr>\n    <tr>\n      <th>3352930</th>\n      <td>87094374</td>\n      <td>reddit</td>\n      <td>73497915</td>\n      <td>66278915</td>\n      <td>0.279803</td>\n    </tr>\n    <tr>\n      <th>3352931</th>\n      <td>87094374</td>\n      <td>reddit</td>\n      <td>73497915</td>\n      <td>66278915</td>\n      <td>0.270250</td>\n    </tr>\n  </tbody>\n</table>\n<p>33759 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look for reddit\n",
    "tw_df = df_RB[df_RB[\"platform\"] == \"twitter\"]\n",
    "tw_x, tw_y, tw_test_x, tw_test_y, tw_model = train_model(tw_df)\n",
    "\n",
    "rd_df = df_RB[df_RB[\"platform\"] == \"reddit\"]\n",
    "rd_x, rd_y, rd_test_x, rd_test_y, rd_model = train_model(rd_df)\n",
    "\n",
    "# inspect_model(tw_x, tw_y, tw_test_x, tw_test_y, tw_model)\n",
    "# inspect_model(rd_x, rd_y, rd_test_x, rd_test_y, rd_model)\n",
    "\n",
    "tw_non_features = take_non_features(tw_df)\n",
    "rd_non_features = take_non_features(rd_df)\n",
    "\n",
    "tw_features_y = take_features(tw_df)\n",
    "tw_features = tw_features_y.drop(\"y\", axis=1)\n",
    "rd_features_y = take_features(rd_df)\n",
    "rd_features = rd_features_y.drop(\"y\", axis=1)\n",
    "rd_predictions = rd_model.predict(rd_features)\n",
    "tw_predictions = tw_model.predict(tw_features)\n",
    "\n",
    "tw_vision = tw_non_features.assign(predictions=tw_predictions)\n",
    "rd_vision = rd_non_features.assign(predictions=rd_predictions)\n",
    "\n",
    "combined_vision = tw_vision.append(rd_vision)\n",
    "not_needed_list = [\"beam_node_author\", \"beam_node\", \"has_followed_path\", \"has_follow_path\"]\n",
    "combined_vision = combined_vision.drop(not_needed_list, axis=1)\n",
    "combined_vision_with_author = combined_vision\n",
    "combined_vision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "          avg_predictions\nplatform                 \nreddit           0.239282\ntwitter          0.285183",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>avg_predictions</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.239282</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.285183</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_vision_with_author2 = combined_vision_with_author.groupby(\n",
    "    [\"platform\", \"conversation_id\", \"author\", \"predictions\"]).count()\n",
    "combined_vision_with_author2 = combined_vision_with_author2.reset_index()\n",
    "combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).sum()\n",
    "combined_vision_with_author2[\n",
    "    \"avg_predictions\"] = combined_vision_with_author2.predictions / combined_vision_with_author2.current\n",
    "combined_vision_with_author2 = combined_vision_with_author2.drop([\"current\", \"predictions\"], axis=1)\n",
    "combined_vision_with_author2 = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "rb_result = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "\n",
    "# prepare for comparison\n",
    "RB_predictions= combined_vision_with_author2.rename(columns={\"avg_predictions\": \"RB\"})\n",
    "\n",
    "rb_result = rb_result.groupby([\"platform\"]).mean()\n",
    "rb_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "          predictions\nplatform             \nreddit       0.242621\ntwitter      0.286455",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predictions</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>reddit</th>\n      <td>0.242621</td>\n    </tr>\n    <tr>\n      <th>twitter</th>\n      <td>0.286455</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_vision = combined_vision.drop(\"author\", axis=1)\n",
    "gpm = combined_vision.groupby([\"platform\", \"conversation_id\", \"current\"]).mean()\n",
    "gpm_per_conversation = gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "gpm_per_platform = gpm.groupby(by=[\"platform\"]).mean()\n",
    "gpm_per_platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "                  repetition_probs  predictions\nrepetition_probs               1.0         -1.0\npredictions                   -1.0          1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>repetition_probs</th>\n      <th>predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>repetition_probs</th>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>predictions</th>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = repetition_probability.join(gpm_per_platform)\n",
    "probabilities.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpretation the correlation between probabilities and the RB-predictions\n",
    "- This means that the neural network computes a linear function of the repetition probabilities based on the computation of the y functions\n",
    "- The probabilities are very low for both reddit and twitter but in a comparable area\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Author Prediction\n",
    "\n",
    "It is possible to predict an author or \"new author\" at same time by defining categories as 1 if a author is to be predicted but\n",
    "only if it is not a new author. Because of memory, only twitter or reddit data can be predicted in one run.\n",
    "The full dataset does not fit in laptops memory and is computed on the cluster (which in turn has no gpu support)\n",
    "\n",
    "The probability of predicting an author is calculated for each relationship (root distance to another node, reply distance to other nodes, and reply distance to nodes with the same author. In future also the author follower network will be included in the feature set.\n",
    "\n",
    "The overall sum of the probability of predicting an author (in average) will be interpreted as the likelihood of any author writing in any time in the conversation (again, because it is not a new author). This will then seen as the author being present in the conversation because it is another measure of a author being available in all branches and positions in the conversation.\n",
    "\n",
    "#### Create a one hot vector representation of the possible authors\n",
    "- create an artificial user that represents a new user in a conversation up to that point\n",
    "- get a matrix with the authors as columns and a 1 if the author wrote the post\n",
    "- join it with the feature matrix\n",
    "- drop the author column\n",
    "\n",
    "\n",
    "#### Training NN to predict the author that would write next\n",
    "- included a \"new author\" category to capture predicting unknown authors\n",
    "- using multi-class classification (instead of multi-label)\n",
    "- relu/sigmoid activation functions have same effect\n",
    "- precision grew significantly when adding more than 3-5 layers\n",
    "\n",
    "#### Predicting the author presence based on prediction probabilities\n",
    "- compute predictions for the whole dataframe\n",
    "- drop features and non-features except conversation and platform\n",
    "- wide to long the authors to make them a index\n",
    "- groupby conversation and platform\n",
    "\n",
    "#### Notes\n",
    "- inserting the new author column increased precision times 10\n",
    "- categorical accuracy and regular accuracy match (which is weird)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def calculate_author_predictions(df):\n",
    "    # compute a fake user that symbolizes that the given user has not been seen at a given stage in the conversation\n",
    "    df_conversation_authors = df[[\"conversation_id\", \"author\", \"current_time\"]]\n",
    "    first_times = df_conversation_authors.groupby([\"conversation_id\", \"author\"]).min()\n",
    "\n",
    "    def is_new_author(row):\n",
    "        earliest_author_post = first_times.loc[row[\"conversation_id\"],row[\"author\"]]\n",
    "        current_post_time = row[\"current_time\"]\n",
    "        return  earliest_author_post >= current_post_time\n",
    "\n",
    "    new_author_column = df[[\"conversation_id\", \"author\", \"current_time\"]].apply(is_new_author, axis=1)\n",
    "    new_author_column= new_author_column.rename(columns={'current_time':\"Author_is_new\"})\n",
    "    new_author_column.value_counts()\n",
    "\n",
    "\n",
    "    def compute_new_author_column(df):\n",
    "        import pandas as pd\n",
    "        author_one_hot = pd.get_dummies(df.author, prefix=\"Author\", sparse=True)\n",
    "        # make author cells 0 that are now represented as \"new author\"\n",
    "        author_one_hot = author_one_hot.astype(bool).apply(lambda x: x & ~new_author_column.Author_is_new).astype(int)\n",
    "        # delete columns that are all 0\n",
    "        author_one_hot = author_one_hot.loc[:, (author_one_hot != 0).any(axis=0)]\n",
    "        # join the new author column to the labels\n",
    "        labels = author_one_hot.join(new_author_column.astype(int))\n",
    "        features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "        combined_set = features.join(labels)\n",
    "        return combined_set, features, labels\n",
    "\n",
    "    combined_set, features, labels = compute_new_author_column(df)\n",
    "\n",
    "    from keras.optimizer_v2.rmsprop import RMSprop  # selecting train and test datasets\n",
    "    train, test = train_test_split(combined_set, test_size=0.2, shuffle=False)\n",
    "    print(\"split training and test set\")\n",
    "\n",
    "    # train the model\n",
    "    y = train.drop(features.columns, axis=1)\n",
    "    x = train.drop(labels.columns, axis=1)\n",
    "    print(\"seperated features and y with shapes:\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "    # print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    output_shape = y.shape[1]\n",
    "    print(\"inputshape is {}\".format(input_shape))\n",
    "    model = Sequential([\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='softmax', input_shape=input_shape)\n",
    "    ])\n",
    "    print(\"defined model as {}\".format(model.layers))\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy', 'accuracy' ,'mae']\n",
    "    )\n",
    "    print(\"compiled model\")\n",
    "    #model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    #model.fit(x, y, epochs=10, shuffle=True)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.drop(features.columns, axis=1)\n",
    "    test_x = test.drop(labels.columns, axis=1)\n",
    "    #test_x = test_x.drop(\"timedelta\", axis=1)\n",
    "\n",
    "    loss, cat_accuracy, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is cat acc {}, reg acc {} and the mae is {}\".format(cat_accuracy, accuracy, mae))\n",
    "\n",
    "    all_features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "    print(\"start generating author predictions for the whole data set\")\n",
    "    predictions = model.predict(all_features, use_multiprocessing=True)\n",
    "    print(\"end generating author predictions for the whole data set\")\n",
    "    column_names = labels.columns\n",
    "    predictions = pd.DataFrame(predictions, columns=column_names)\n",
    "    print(type(predictions))\n",
    "    print(predictions.shape)\n",
    "\n",
    "\n",
    "    all_non_features = df[[\"conversation_id\", \"platform\"]]\n",
    "    print(type(all_non_features))\n",
    "    print(all_non_features.shape)\n",
    "    all_non_features.reset_index(drop=True, inplace=True)\n",
    "    joined_dataframe = all_non_features.join(predictions)\n",
    "    #print(joined_dataframe.Author_is_new.describe()) # no idea why that is the same prediction of all the rows\n",
    "\n",
    "    joined_dataframe = joined_dataframe.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "    author_predictions_existing = joined_dataframe.drop([\"Author_is_new\"], axis=1)\n",
    "    author_predictions_existing.reset_index(level=['platform', 'conversation_id'],inplace=True)\n",
    "    print(\"start converting author hot vectors beack to one author column\")\n",
    "    author_predictions_existing_reshaped = pd.wide_to_long(author_predictions_existing, stubnames=\"Author_\", i=['platform', 'conversation_id'], j=\"author_id\")\n",
    "    print(\"end converting author hot vectors beack to one author column\")    \n",
    "    return author_predictions_existing_reshaped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split training and test set\n",
      "seperated features and y with shapes:\n",
      "(14029, 117)\n",
      "(14029, 124)\n",
      "inputshape is (117,)\n",
      "defined model as [<keras.layers.core.Dense object at 0x7f125df58df0>, <keras.layers.core.Dense object at 0x7f125df58f40>, <keras.layers.core.Dense object at 0x7f125df533a0>, <keras.layers.core.Dense object at 0x7f1264329cd0>]\n",
      "compiled model\n",
      "439/439 [==============================] - 1s 2ms/step - loss: 2.9180 - categorical_accuracy: 0.4238 - accuracy: 0.4238 - mae: 0.0131      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 6.6310 - categorical_accuracy: 0.4113 - accuracy: 0.4113 - mae: 0.0129     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "the accuracy on the training set is cat acc 0.4113454818725586, reg acc 0.4113454818725586 and the mae is 0.01286711823195219\n",
      "start generating author predictions for the whole data set\n",
      "end generating author predictions for the whole data set\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(17537, 124)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(17537, 2)\n",
      "start converting author hot vectors beack to one author column\n",
      "end converting author hot vectors beack to one author column\n",
      "split training and test set\n",
      "seperated features and y with shapes:\n",
      "(12977, 117)\n",
      "(12977, 60)\n",
      "inputshape is (117,)\n",
      "defined model as [<keras.layers.core.Dense object at 0x7f1324075bb0>, <keras.layers.core.Dense object at 0x7f1324075160>, <keras.layers.core.Dense object at 0x7f1324075460>, <keras.layers.core.Dense object at 0x7f1324075760>]\n",
      "compiled model\n",
      "406/406 [==============================] - 1s 2ms/step - loss: 2.3058 - categorical_accuracy: 0.4594 - accuracy: 0.4594 - mae: 0.0250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.3571 - categorical_accuracy: 0.4552 - accuracy: 0.4552 - mae: 0.0235\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "the accuracy on the training set is cat acc 0.45516178011894226, reg acc 0.45516178011894226 and the mae is 0.02349778264760971\n",
      "start generating author predictions for the whole data set\n",
      "end generating author predictions for the whole data set\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(16222, 60)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(16222, 2)\n",
      "start converting author hot vectors beack to one author column\n",
      "end converting author hot vectors beack to one author column\n"
     ]
    }
   ],
   "source": [
    "df_PB_reddit = df_PB[df_PB[\"platform\"] == \"reddit\"]\n",
    "prediction_result_reddit = calculate_author_predictions(df_PB_reddit)\n",
    "\n",
    "df_PB_twitter = df_PB[df_PB[\"platform\"] == \"twitter\"]\n",
    "prediction_result_twitter = calculate_author_predictions(df_PB_twitter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8875/3022830577.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  PB_predictions = prediction_result_reddit.append(prediction_result_twitter)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                        PB\nplatform conversation_id     author_id                    \nreddit   2087645             139565               0.001642\n                             538210               0.085935\n                             1305120              0.028978\n                             2542784              0.002728\n                             3382544              0.005785\n...                                                    ...\ntwitter  1552015261212954624 1506453497578934275  0.000334\n                             1514906890442055682  0.000016\n                             1520788067224670213  0.011693\n                             1520820316431335424  0.001790\n                             1528344937258962946  0.001544\n\n[4580 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>PB</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">reddit</th>\n      <th rowspan=\"5\" valign=\"top\">2087645</th>\n      <th>139565</th>\n      <td>0.001642</td>\n    </tr>\n    <tr>\n      <th>538210</th>\n      <td>0.085935</td>\n    </tr>\n    <tr>\n      <th>1305120</th>\n      <td>0.028978</td>\n    </tr>\n    <tr>\n      <th>2542784</th>\n      <td>0.002728</td>\n    </tr>\n    <tr>\n      <th>3382544</th>\n      <td>0.005785</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th rowspan=\"5\" valign=\"top\">1552015261212954624</th>\n      <th>1506453497578934275</th>\n      <td>0.000334</td>\n    </tr>\n    <tr>\n      <th>1514906890442055682</th>\n      <td>0.000016</td>\n    </tr>\n    <tr>\n      <th>1520788067224670213</th>\n      <td>0.011693</td>\n    </tr>\n    <tr>\n      <th>1520820316431335424</th>\n      <td>0.001790</td>\n    </tr>\n    <tr>\n      <th>1528344937258962946</th>\n      <td>0.001544</td>\n    </tr>\n  </tbody>\n</table>\n<p>4580 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction_result_reddit\n",
    "# prediction_result_twitter\n",
    "\n",
    "PB_predictions = prediction_result_reddit.append(prediction_result_twitter)\n",
    "PB_predictions = PB_predictions.rename(columns={\"Author_\": \"PB\"})\n",
    "PB_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Author Centrality\n",
    "- The centrality was already computed when creating the dataset as it is based on graph measures primarily\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  centrality\nplatform conversation_id     author                         \nreddit   2087645             3853842                0.222222\n                             21221970               0.166667\n                             21259329               0.000000\n                             25043219               0.391111\n                             38759243               0.111111\n...                                                      ...\ntwitter  1552015261212954624 1418169381545259008    0.000000\n                             1430654330932236299    0.166667\n                             1440140450434740232    0.000000\n                             1450261887208153099    2.185185\n                             1513254155393114112    0.000000\n\n[737 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>centrality</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th>author</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">reddit</th>\n      <th rowspan=\"5\" valign=\"top\">2087645</th>\n      <th>3853842</th>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>21221970</th>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>21259329</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25043219</th>\n      <td>0.391111</td>\n    </tr>\n    <tr>\n      <th>38759243</th>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">twitter</th>\n      <th rowspan=\"5\" valign=\"top\">1552015261212954624</th>\n      <th>1418169381545259008</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1430654330932236299</th>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>1440140450434740232</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1450261887208153099</th>\n      <td>2.185185</td>\n    </tr>\n    <tr>\n      <th>1513254155393114112</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>737 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_centrality_avg = df_centrality.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "df_centrality_avg = df_centrality_avg.assign(centrality=df_centrality_avg.centrality_score / df_centrality_avg.root_distance_avg)\n",
    "df_centrality_avg = df_centrality_avg.drop([\"centrality_score\", \"root_distance_avg\"], axis=1)\n",
    "df_centrality_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combined Analysis\n",
    "- join three author vision measures into one dataframe\n",
    "- add author centrality to the same dataframe\n",
    "- correlate the measures\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    RB        PB  centrality  baseline  \\\nplatform conversation_id                                                 \nreddit   2087645              0.244923  0.005002    0.217111  0.585731   \n         2258028              0.254613  0.005340    0.311111  0.525641   \n         3237826              0.256694  0.004868    0.129628  0.526959   \n         4264097              0.248729  0.005030    0.355831  0.528471   \n         6879611              0.244134  0.004649    0.102607  0.422637   \n         13590426             0.237511  0.004478    0.272046  0.464835   \n         17172485             0.224595  0.003693    0.315476  0.467687   \n         19161388             0.240262  0.004689    0.283967  0.483850   \n         20520581             0.245881  0.004802    0.291901  0.503891   \n         20889898             0.217086  0.003850    0.091270  0.445809   \n         23330036             0.243446  0.004629    0.217544  0.400155   \n         31131919             0.228004  0.004804    0.191930  0.535818   \n         32545716             0.242345  0.004900    0.239296  0.528751   \n         32854345             0.222579  0.004203    0.170915  0.504873   \n         34665923             0.252468  0.005131    0.364815  0.337147   \n         35562400             0.249329  0.004740    0.275463  0.406987   \n         35824217             0.254794  0.005043    0.327020  0.511591   \n         42452689             0.221925  0.003985    0.265625  0.418014   \n         49074540             0.237651  0.004172    0.320539  0.347186   \n         66271665             0.224993  0.003761    0.261111  0.490000   \n         67066456             0.256989  0.005013    0.220630  0.524677   \n         67519271             0.220946  0.003840    0.088889  0.485714   \n         68536375             0.230487  0.003966    0.261111  0.454167   \n         71171604             0.250456  0.004812    0.273714  0.527994   \n         73497915             0.250278  0.004712    0.158094  0.526111   \n         78738990             0.241673  0.004736    0.439429  0.481732   \n         79403147             0.238671  0.004488    0.146247  0.509736   \n         80796454             0.231578  0.004194    0.376449  0.342875   \n         86717929             0.237362  0.005245    0.345569  0.514583   \n         91662800             0.229447  0.004343    0.308793  0.513333   \n         96138222             0.237880  0.004588    0.276786  0.512500   \ntwitter  1455504947999657990  0.272486  0.007509    0.350530  0.497211   \n         1457889187177410564  0.285048  0.005539    0.263889  0.400265   \n         1538176825603543041  0.275103  0.005813    0.069187  0.436561   \n         1543194290284806145  0.291730  0.008483    0.188985  0.444759   \n         1543257148565737472  0.289601  0.010940    0.179141  0.518299   \n         1543528239288786945  0.277327  0.005781    0.194224  0.454231   \n         1543910459891384326  0.279208  0.006014    0.118687  0.416770   \n         1543936160938041350  0.287178  0.011565    0.296784  0.542260   \n         1543996112536829952  0.280298  0.006066    0.229001  0.436198   \n         1544177219420839936  0.292587  0.010055    0.099023  0.512149   \n         1544672441456279552  0.297450  0.012220    0.236319  0.658999   \n         1544971258370326528  0.290334  0.006458    0.351202  0.380444   \n         1552015261212954624  0.289023  0.005990    0.273965  0.322907   \n\n                              repetition  \nplatform conversation_id                  \nreddit   2087645                0.252101  \n         2258028                0.464286  \n         3237826                0.147465  \n         4264097                0.241015  \n         6879611                0.156863  \n         13590426               0.384615  \n         17172485               0.250000  \n         19161388               0.252941  \n         20520581               0.144712  \n         20889898               0.090580  \n         23330036               0.103333  \n         31131919               0.363636  \n         32545716               0.147619  \n         32854345               0.107692  \n         34665923               0.266667  \n         35562400               0.247619  \n         35824217               0.263158  \n         42452689               0.116959  \n         49074540               0.190476  \n         66271665               0.333333  \n         67066456               0.143353  \n         67519271               0.333333  \n         68536375               0.333333  \n         71171604               0.089593  \n         73497915               0.132240  \n         78738990               0.326316  \n         79403147               0.058733  \n         80796454               0.082857  \n         86717929               0.226415  \n         91662800               0.358974  \n         96138222               0.500000  \ntwitter  1455504947999657990    0.344839  \n         1457889187177410564    0.099567  \n         1538176825603543041    0.036066  \n         1543194290284806145    0.184783  \n         1543257148565737472    0.158183  \n         1543528239288786945    0.054588  \n         1543910459891384326    0.047028  \n         1543936160938041350    0.277607  \n         1543996112536829952    0.121212  \n         1544177219420839936    0.111111  \n         1544672441456279552    0.294372  \n         1544971258370326528    0.316667  \n         1552015261212954624    0.116923  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>RB</th>\n      <th>PB</th>\n      <th>centrality</th>\n      <th>baseline</th>\n      <th>repetition</th>\n    </tr>\n    <tr>\n      <th>platform</th>\n      <th>conversation_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"31\" valign=\"top\">reddit</th>\n      <th>2087645</th>\n      <td>0.244923</td>\n      <td>0.005002</td>\n      <td>0.217111</td>\n      <td>0.585731</td>\n      <td>0.252101</td>\n    </tr>\n    <tr>\n      <th>2258028</th>\n      <td>0.254613</td>\n      <td>0.005340</td>\n      <td>0.311111</td>\n      <td>0.525641</td>\n      <td>0.464286</td>\n    </tr>\n    <tr>\n      <th>3237826</th>\n      <td>0.256694</td>\n      <td>0.004868</td>\n      <td>0.129628</td>\n      <td>0.526959</td>\n      <td>0.147465</td>\n    </tr>\n    <tr>\n      <th>4264097</th>\n      <td>0.248729</td>\n      <td>0.005030</td>\n      <td>0.355831</td>\n      <td>0.528471</td>\n      <td>0.241015</td>\n    </tr>\n    <tr>\n      <th>6879611</th>\n      <td>0.244134</td>\n      <td>0.004649</td>\n      <td>0.102607</td>\n      <td>0.422637</td>\n      <td>0.156863</td>\n    </tr>\n    <tr>\n      <th>13590426</th>\n      <td>0.237511</td>\n      <td>0.004478</td>\n      <td>0.272046</td>\n      <td>0.464835</td>\n      <td>0.384615</td>\n    </tr>\n    <tr>\n      <th>17172485</th>\n      <td>0.224595</td>\n      <td>0.003693</td>\n      <td>0.315476</td>\n      <td>0.467687</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>19161388</th>\n      <td>0.240262</td>\n      <td>0.004689</td>\n      <td>0.283967</td>\n      <td>0.483850</td>\n      <td>0.252941</td>\n    </tr>\n    <tr>\n      <th>20520581</th>\n      <td>0.245881</td>\n      <td>0.004802</td>\n      <td>0.291901</td>\n      <td>0.503891</td>\n      <td>0.144712</td>\n    </tr>\n    <tr>\n      <th>20889898</th>\n      <td>0.217086</td>\n      <td>0.003850</td>\n      <td>0.091270</td>\n      <td>0.445809</td>\n      <td>0.090580</td>\n    </tr>\n    <tr>\n      <th>23330036</th>\n      <td>0.243446</td>\n      <td>0.004629</td>\n      <td>0.217544</td>\n      <td>0.400155</td>\n      <td>0.103333</td>\n    </tr>\n    <tr>\n      <th>31131919</th>\n      <td>0.228004</td>\n      <td>0.004804</td>\n      <td>0.191930</td>\n      <td>0.535818</td>\n      <td>0.363636</td>\n    </tr>\n    <tr>\n      <th>32545716</th>\n      <td>0.242345</td>\n      <td>0.004900</td>\n      <td>0.239296</td>\n      <td>0.528751</td>\n      <td>0.147619</td>\n    </tr>\n    <tr>\n      <th>32854345</th>\n      <td>0.222579</td>\n      <td>0.004203</td>\n      <td>0.170915</td>\n      <td>0.504873</td>\n      <td>0.107692</td>\n    </tr>\n    <tr>\n      <th>34665923</th>\n      <td>0.252468</td>\n      <td>0.005131</td>\n      <td>0.364815</td>\n      <td>0.337147</td>\n      <td>0.266667</td>\n    </tr>\n    <tr>\n      <th>35562400</th>\n      <td>0.249329</td>\n      <td>0.004740</td>\n      <td>0.275463</td>\n      <td>0.406987</td>\n      <td>0.247619</td>\n    </tr>\n    <tr>\n      <th>35824217</th>\n      <td>0.254794</td>\n      <td>0.005043</td>\n      <td>0.327020</td>\n      <td>0.511591</td>\n      <td>0.263158</td>\n    </tr>\n    <tr>\n      <th>42452689</th>\n      <td>0.221925</td>\n      <td>0.003985</td>\n      <td>0.265625</td>\n      <td>0.418014</td>\n      <td>0.116959</td>\n    </tr>\n    <tr>\n      <th>49074540</th>\n      <td>0.237651</td>\n      <td>0.004172</td>\n      <td>0.320539</td>\n      <td>0.347186</td>\n      <td>0.190476</td>\n    </tr>\n    <tr>\n      <th>66271665</th>\n      <td>0.224993</td>\n      <td>0.003761</td>\n      <td>0.261111</td>\n      <td>0.490000</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>67066456</th>\n      <td>0.256989</td>\n      <td>0.005013</td>\n      <td>0.220630</td>\n      <td>0.524677</td>\n      <td>0.143353</td>\n    </tr>\n    <tr>\n      <th>67519271</th>\n      <td>0.220946</td>\n      <td>0.003840</td>\n      <td>0.088889</td>\n      <td>0.485714</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>68536375</th>\n      <td>0.230487</td>\n      <td>0.003966</td>\n      <td>0.261111</td>\n      <td>0.454167</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>71171604</th>\n      <td>0.250456</td>\n      <td>0.004812</td>\n      <td>0.273714</td>\n      <td>0.527994</td>\n      <td>0.089593</td>\n    </tr>\n    <tr>\n      <th>73497915</th>\n      <td>0.250278</td>\n      <td>0.004712</td>\n      <td>0.158094</td>\n      <td>0.526111</td>\n      <td>0.132240</td>\n    </tr>\n    <tr>\n      <th>78738990</th>\n      <td>0.241673</td>\n      <td>0.004736</td>\n      <td>0.439429</td>\n      <td>0.481732</td>\n      <td>0.326316</td>\n    </tr>\n    <tr>\n      <th>79403147</th>\n      <td>0.238671</td>\n      <td>0.004488</td>\n      <td>0.146247</td>\n      <td>0.509736</td>\n      <td>0.058733</td>\n    </tr>\n    <tr>\n      <th>80796454</th>\n      <td>0.231578</td>\n      <td>0.004194</td>\n      <td>0.376449</td>\n      <td>0.342875</td>\n      <td>0.082857</td>\n    </tr>\n    <tr>\n      <th>86717929</th>\n      <td>0.237362</td>\n      <td>0.005245</td>\n      <td>0.345569</td>\n      <td>0.514583</td>\n      <td>0.226415</td>\n    </tr>\n    <tr>\n      <th>91662800</th>\n      <td>0.229447</td>\n      <td>0.004343</td>\n      <td>0.308793</td>\n      <td>0.513333</td>\n      <td>0.358974</td>\n    </tr>\n    <tr>\n      <th>96138222</th>\n      <td>0.237880</td>\n      <td>0.004588</td>\n      <td>0.276786</td>\n      <td>0.512500</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"13\" valign=\"top\">twitter</th>\n      <th>1455504947999657990</th>\n      <td>0.272486</td>\n      <td>0.007509</td>\n      <td>0.350530</td>\n      <td>0.497211</td>\n      <td>0.344839</td>\n    </tr>\n    <tr>\n      <th>1457889187177410564</th>\n      <td>0.285048</td>\n      <td>0.005539</td>\n      <td>0.263889</td>\n      <td>0.400265</td>\n      <td>0.099567</td>\n    </tr>\n    <tr>\n      <th>1538176825603543041</th>\n      <td>0.275103</td>\n      <td>0.005813</td>\n      <td>0.069187</td>\n      <td>0.436561</td>\n      <td>0.036066</td>\n    </tr>\n    <tr>\n      <th>1543194290284806145</th>\n      <td>0.291730</td>\n      <td>0.008483</td>\n      <td>0.188985</td>\n      <td>0.444759</td>\n      <td>0.184783</td>\n    </tr>\n    <tr>\n      <th>1543257148565737472</th>\n      <td>0.289601</td>\n      <td>0.010940</td>\n      <td>0.179141</td>\n      <td>0.518299</td>\n      <td>0.158183</td>\n    </tr>\n    <tr>\n      <th>1543528239288786945</th>\n      <td>0.277327</td>\n      <td>0.005781</td>\n      <td>0.194224</td>\n      <td>0.454231</td>\n      <td>0.054588</td>\n    </tr>\n    <tr>\n      <th>1543910459891384326</th>\n      <td>0.279208</td>\n      <td>0.006014</td>\n      <td>0.118687</td>\n      <td>0.416770</td>\n      <td>0.047028</td>\n    </tr>\n    <tr>\n      <th>1543936160938041350</th>\n      <td>0.287178</td>\n      <td>0.011565</td>\n      <td>0.296784</td>\n      <td>0.542260</td>\n      <td>0.277607</td>\n    </tr>\n    <tr>\n      <th>1543996112536829952</th>\n      <td>0.280298</td>\n      <td>0.006066</td>\n      <td>0.229001</td>\n      <td>0.436198</td>\n      <td>0.121212</td>\n    </tr>\n    <tr>\n      <th>1544177219420839936</th>\n      <td>0.292587</td>\n      <td>0.010055</td>\n      <td>0.099023</td>\n      <td>0.512149</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>1544672441456279552</th>\n      <td>0.297450</td>\n      <td>0.012220</td>\n      <td>0.236319</td>\n      <td>0.658999</td>\n      <td>0.294372</td>\n    </tr>\n    <tr>\n      <th>1544971258370326528</th>\n      <td>0.290334</td>\n      <td>0.006458</td>\n      <td>0.351202</td>\n      <td>0.380444</td>\n      <td>0.316667</td>\n    </tr>\n    <tr>\n      <th>1552015261212954624</th>\n      <td>0.289023</td>\n      <td>0.005990</td>\n      <td>0.273965</td>\n      <td>0.322907</td>\n      <td>0.116923</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_comparison_table = RB_predictions.join(PB_predictions).join(df_centrality_avg).join(baseline_predictions)\n",
    "\n",
    "prediction_comparison_table = prediction_comparison_table.groupby([\"platform\", \"conversation_id\"]).mean().join(repetition_predictions)\n",
    "prediction_comparison_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "corr_matrix = prediction_comparison_table.corr(method=\"pearson\")\n",
    "sn.heatmap(corr_matrix, annot=True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}