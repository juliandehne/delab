{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from random import sample\n",
    "# PB: prediction based algorithm\n",
    "# RB: response based algoritm\n",
    "\n",
    "\n",
    "file_name_PB = \"data/vision_forward_graph_data_08_09_22.pkl\"\n",
    "with open(file_name_PB, 'rb') as f:\n",
    "    df_PB = pickle.load(f)\n",
    "    #df_PB.sort_values(by=\"platform\", inplace=True, ignore_index=True)\n",
    "\n",
    "file_name_RB = \"data/vision_graph_data_remote_23_08_22.pkl\"\n",
    "with open(file_name_RB, 'rb') as f:\n",
    "    df_RB = pickle.load(f)\n",
    "\n",
    "file_name_centrality = \"data/author_centrality_remote.pkl\"\n",
    "with open(file_name_centrality, 'rb') as f:\n",
    "    df_centrality = pickle.load(f)\n",
    "\n",
    "common_conversation_ids = set(df_PB.conversation_id).intersection(df_RB.conversation_id).intersection(\n",
    "    df_centrality.conversation_id)\n",
    "all_conversation_ids = set(df_PB.conversation_id).union(df_RB.conversation_id).union(df_centrality.conversation_id)\n",
    "all_conversation_count = len(all_conversation_ids)\n",
    "common_conversation_count = len(common_conversation_ids)\n",
    "# reducing the sample size for testing\n",
    "common_conversation_ids = sample(common_conversation_ids, 700)\n",
    "\n",
    "print(\"From {} conversations {} are shared in all datasets\".format(all_conversation_count, common_conversation_count))\n",
    "\n",
    "\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(common_conversation_ids)]\n",
    "df_RB = df_RB[df_RB.conversation_id.isin(common_conversation_ids)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(common_conversation_ids)]\n",
    "df_PB.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility Functions\n",
    "\n",
    "The following cell contains utility functions that are needed for all the different algorithms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def is_not_reddit_or_twitter(text):\n",
    "    if text == \"reddit\" or text == \"twitter\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def equalize_samples(df):\n",
    "    \"\"\"\n",
    "    this approximates the same number of conversations for both platforms\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_conversations_twitter = set(df[df[\"platform\"] == \"twitter\"].conversation_id.tolist())\n",
    "    df_conversations_reddit = set(df[df[\"platform\"] == \"reddit\"].conversation_id.tolist())\n",
    "    reddit_data_count = df.loc[df.platform == \"reddit\", 'platform'].count()\n",
    "    twitter_data_count = df.loc[df.platform == \"twitter\", 'platform'].count()\n",
    "    # assert twitter_data_count > reddit_data_count, \"counts (reddit, twitter) are ({},{}):\".format(reddit_data_count, twitter_data_count)\n",
    "    current_count = 0\n",
    "    n = 1\n",
    "    smaller_count = reddit_data_count\n",
    "    df_conversations = df_conversations_twitter\n",
    "    if reddit_data_count > twitter_data_count:\n",
    "        smaller_count = twitter_data_count\n",
    "        df_conversations = df_conversations_reddit\n",
    "    while current_count < smaller_count:\n",
    "        chosen_conversation_ids = sample(df_conversations, n)\n",
    "        df_candidate = df[df[\"conversation_id\"].isin(chosen_conversation_ids)]\n",
    "        n = n + 1\n",
    "        current_count = df_candidate.shape[0]\n",
    "    print(\"chosen {} conversations and gotten {} from twitter compared to {} from reddit\".format(n, current_count,\n",
    "                                                                                                 reddit_data_count))\n",
    "    not_chosen_conversation_ids = set(df_conversations) - set(chosen_conversation_ids)\n",
    "    df_result = df[~df[\"conversation_id\"].isin(not_chosen_conversation_ids)]\n",
    "    return df_result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-Features\n",
    "\n",
    "All the cells contain a number of columns that have a meaning in the conversation but are not features to train the NN with."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# some utility functions to take the columns that are used as features\n",
    "non_feature_list = [\"current\", \"beam_node\", \"conversation_id\", \"platform\", \"has_followed_path\", \"has_follow_path\",\n",
    "                    \"beam_node_author\", \"author\"]\n",
    "\n",
    "\n",
    "def take_features(df, additional_non_features=[]):\n",
    "    non_feature_list2 = non_feature_list + additional_non_features\n",
    "    df = df.drop(non_feature_list2, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def take_non_features(df, additional_non_features=[]):\n",
    "    non_feature_list2 = non_feature_list + additional_non_features\n",
    "    column_names = df.columns.values\n",
    "    feature_list = [column_name for column_name in column_names if column_name not in non_feature_list2]\n",
    "    df = df.drop(feature_list, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_timedelta(df):\n",
    "    # normalize timedelta (put between 0 and 1)\n",
    "    dt = df.timedelta\n",
    "    timedelta_normalized = (dt - dt.min()) / (dt.max() - dt.min())\n",
    "    df = df.assign(timedelta=timedelta_normalized)\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Cleaning and Data preperation\n",
    "- Delete rows that are neither twitter or reddit data\n",
    "- normalize time deltas\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filtering data that is not twitter or reddit\n",
    "def delete_not_twitter_not_reddit(df):\n",
    "    platform = df.platform\n",
    "    to_delete_rows = platform.apply(lambda x: is_not_reddit_or_twitter(x))\n",
    "    df = df.drop(df[to_delete_rows].index)\n",
    "    return df\n",
    "\n",
    "df_RB = delete_not_twitter_not_reddit(df_RB)\n",
    "df_PB = delete_not_twitter_not_reddit(df_PB)\n",
    "df_centrality = delete_not_twitter_not_reddit(df_centrality)\n",
    "\n",
    "df_RB = equalize_samples(df_RB)\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(df_RB.conversation_id)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(df_RB.conversation_id)]\n",
    "\n",
    "df_RB = normalize_timedelta(df_RB)\n",
    "df_PB = normalize_timedelta(df_PB)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline for Author vision\n",
    "- uses selected values as a distance measure\n",
    "- probability of having seen a tweet is reduced by a half with each step in the reply hierachy\n",
    "- probability of having seen a tweet is reduced by a quarter for each step away from the root\n",
    "- probabiliy of having seen a tweet is increased for each path in the follower network to the tweet (forthcoming)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in order to allow the comparison, the filter from the other notebook needs to run and the predictions, too\n",
    "df_baseline = df_RB[df_RB[\"conversation_id\"].isin(common_conversation_ids)]\n",
    "\n",
    "reply_filter_col = [col for col in df_baseline if col.startswith('reply')]\n",
    "root_distance_filter_col = [col for col in df_baseline if col.startswith('root')]\n",
    "reply_columns = df_baseline[reply_filter_col]\n",
    "root_distance_columns = df_baseline[root_distance_filter_col]\n",
    "reply_cs = reply_columns.sum(axis=1)\n",
    "root_distance_cs = root_distance_columns.sum(axis=1)\n",
    "rcs_not_null = [i for i in reply_cs.tolist() if i != 0]\n",
    "root_reply_combined = (root_distance_cs + reply_cs)\n",
    "root_reply_combined = (root_reply_combined - root_reply_combined.min()) / (\n",
    "        root_reply_combined.max() - root_reply_combined.min())\n",
    "combined = [i for i in root_reply_combined.tolist() if i != 0]\n",
    "df_baseline = df_baseline.assign(root_reply_combined=root_reply_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_baseline_with_authors = df_baseline[[\"root_reply_combined\", \"conversation_id\", \"author\", \"platform\"]]\n",
    "# df_reshaped = pd.pivot_table(df_data,index=[\"conversation_id\", \"current\"], columns=[\"root_reply_combined\"],aggfunc = np.mean)\n",
    "baseline_gpm = df_baseline_with_authors.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "baseline_predictions = baseline_gpm\n",
    "baseline_predictions.rename(columns={\"root_reply_combined\": \"baseline\"},inplace=True)\n",
    "baseline_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_gpm_conversation = baseline_gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "baseline_gpm = baseline_gpm_conversation.groupby(by=[\"platform\"]).mean()\n",
    "baseline_gpm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repetition Probabilities\n",
    "\n",
    "#### Analyzing the probability of an author writing repeatedly in the same conversation\n",
    "1. sum up the amounts y == 1 (because an author has answered himself)\n",
    "2. sum chances of an author seeing himself write\n",
    "3. calculate a measure of how likely it is that an author sees himself repeated as a test for the nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "author_count_columns = [\"current\", \"conversation_id\", \"platform\", \"y\"]\n",
    "author_df = df_RB[author_count_columns]\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\", \"current\"]).sum()\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "distinct_view_columns = [\"current\", \"conversation_id\", \"platform\"]\n",
    "distinct_view_df = df_RB[distinct_view_columns]\n",
    "distinct_views = distinct_view_df.groupby([\"current\", \"conversation_id\", \"platform\"]).size().to_frame('size')\n",
    "distinct_views = distinct_views.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "joined_author_stats = author_df.join(distinct_views)\n",
    "joined_author_stats = joined_author_stats[\"y\"] / joined_author_stats[\"size\"]\n",
    "\n",
    "# prepare for comparison\n",
    "import pandas as pd\n",
    "repetition_predictions = pd.DataFrame(joined_author_stats).rename(columns={0: \"repetition\"})\n",
    "\n",
    "joined_author_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The joined author stats show the repetition probabilities for each of the platforms per conversation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "repetition_probability = joined_author_stats.groupby(\"platform\").mean().to_frame(\"repetition_probs\")\n",
    "repetition_probability\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Response Based Author vision Algorithm (RB)\n",
    "\n",
    "The features are the distance of the author to any tweet in the conversation\n",
    "indicated by the following structures:\n",
    "- subtree to viewed tweet from a tweet the author wrote\n",
    "- root closeness of viewed tweet\n",
    "- time delta to viewed tweet from tweets the author wrote\n",
    "\n",
    "#### Loading the data from the pickled version\n",
    "1. importing libraries\n",
    "2. checking gpu support\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "print(\"cuda gpu is available: {}\".format(is_cuda_gpu_available))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df[df[\"root_distance_0\"] == 0]\n",
    "# analyze the distribution of reached targets for the sample\n",
    "print(\"reddit:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"reddit\"].y.value_counts())\n",
    "print(\"twitter:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"twitter\"].y.value_counts())\n",
    "# this should be higher for reddit as the unique author / posting ratio is lower for reddit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing a nn model\n",
    "1. separate features\n",
    "2. train models for reddit and twitter\n",
    "3. inspect models for reddit and twitter\n",
    "4. predict the likelihood based on the author has seen a posting\n",
    "5. aggregate likelihoods in order to compute author vision measure\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training functions\n",
    "def train_model(df):\n",
    "    # dropping non-reddit non-twitter data\n",
    "    df = take_features(df)\n",
    "\n",
    "    # selecting train and test datasets\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train.describe()\n",
    "\n",
    "    # train the model\n",
    "    y = train.y\n",
    "    x = train.drop(\"y\", axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "\n",
    "    print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    model = Sequential([\n",
    "        Dense(1, activation='sigmoid', input_shape=input_shape)\n",
    "    ])\n",
    "\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'mae']\n",
    "    )\n",
    "\n",
    "    # model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.y\n",
    "    test_x = test.drop(\"y\", axis=1)\n",
    "\n",
    "    loss, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is {} and the mae is {}\".format(accuracy, mae))\n",
    "\n",
    "    return x, y, test_x, test_y, model\n",
    "\n",
    "\n",
    "def inspect_model(x, y, test_x, test_y, model):\n",
    "    # have a look at some prediction\n",
    "    reply_distance_2 = test_x[test_x[\"reply_distance_2\"] == 1]\n",
    "    first_rows = reply_distance_2.head(2)\n",
    "    print(first_rows)\n",
    "    model.predict(first_rows)\n",
    "\n",
    "    # let's have a look at the weights and biases of the hidden layer\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]\n",
    "    first_layer_biases = model.layers[0].get_weights()[1]\n",
    "    # print(first_layer_weights)\n",
    "    column_names = x.columns.values\n",
    "    for i in range(len(column_names[:5])):\n",
    "        print(\"feature {} has weight {} \\n\".format(column_names[i], first_layer_weights[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# have a look for reddit\n",
    "tw_df = df_RB[df_RB[\"platform\"] == \"twitter\"]\n",
    "tw_x, tw_y, tw_test_x, tw_test_y, tw_model = train_model(tw_df)\n",
    "\n",
    "rd_df = df_RB[df_RB[\"platform\"] == \"reddit\"]\n",
    "rd_x, rd_y, rd_test_x, rd_test_y, rd_model = train_model(rd_df)\n",
    "\n",
    "# inspect_model(tw_x, tw_y, tw_test_x, tw_test_y, tw_model)\n",
    "# inspect_model(rd_x, rd_y, rd_test_x, rd_test_y, rd_model)\n",
    "\n",
    "tw_non_features = take_non_features(tw_df)\n",
    "rd_non_features = take_non_features(rd_df)\n",
    "\n",
    "tw_features_y = take_features(tw_df)\n",
    "tw_features = tw_features_y.drop(\"y\", axis=1)\n",
    "rd_features_y = take_features(rd_df)\n",
    "rd_features = rd_features_y.drop(\"y\", axis=1)\n",
    "rd_predictions = rd_model.predict(rd_features)\n",
    "tw_predictions = tw_model.predict(tw_features)\n",
    "\n",
    "tw_vision = tw_non_features.assign(predictions=tw_predictions)\n",
    "rd_vision = rd_non_features.assign(predictions=rd_predictions)\n",
    "\n",
    "combined_vision = tw_vision.append(rd_vision)\n",
    "not_needed_list = [\"beam_node_author\", \"beam_node\", \"has_followed_path\", \"has_follow_path\"]\n",
    "combined_vision = combined_vision.drop(not_needed_list, axis=1)\n",
    "combined_vision_with_author = combined_vision\n",
    "combined_vision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_vision_with_author2 = combined_vision_with_author.groupby(\n",
    "    [\"platform\", \"conversation_id\", \"author\", \"predictions\"]).count()\n",
    "combined_vision_with_author2 = combined_vision_with_author2.reset_index()\n",
    "combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).sum()\n",
    "combined_vision_with_author2[\n",
    "    \"avg_predictions\"] = combined_vision_with_author2.predictions / combined_vision_with_author2.current\n",
    "combined_vision_with_author2 = combined_vision_with_author2.drop([\"current\", \"predictions\"], axis=1)\n",
    "combined_vision_with_author2 = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "rb_result = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "\n",
    "# prepare for comparison\n",
    "RB_predictions= combined_vision_with_author2.rename(columns={\"avg_predictions\": \"RB\"})\n",
    "\n",
    "rb_result = rb_result.groupby([\"platform\"]).mean()\n",
    "rb_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_vision = combined_vision.drop(\"author\", axis=1)\n",
    "gpm = combined_vision.groupby([\"platform\", \"conversation_id\", \"current\"]).mean()\n",
    "gpm_per_conversation = gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "gpm_per_platform = gpm.groupby(by=[\"platform\"]).mean()\n",
    "gpm_per_platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probabilities = repetition_probability.join(gpm_per_platform)\n",
    "probabilities.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpretation the correlation between probabilities and the RB-predictions\n",
    "- This means that the neural network computes a linear function of the repetition probabilities based on the computation of the y functions\n",
    "- The probabilities are very low for both reddit and twitter but in a comparable area\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Author Prediction\n",
    "\n",
    "It is possible to predict an author or \"new author\" at same time by defining categories as 1 if a author is to be predicted but\n",
    "only if it is not a new author. Because of memory, only twitter or reddit data can be predicted in one run.\n",
    "The full dataset does not fit in laptops memory and is computed on the cluster (which in turn has no gpu support)\n",
    "\n",
    "The probability of predicting an author is calculated for each relationship (root distance to another node, reply distance to other nodes, and reply distance to nodes with the same author. In future also the author follower network will be included in the feature set.\n",
    "\n",
    "The overall sum of the probability of predicting an author (in average) will be interpreted as the likelihood of any author writing in any time in the conversation (again, because it is not a new author). This will then seen as the author being present in the conversation because it is another measure of a author being available in all branches and positions in the conversation.\n",
    "\n",
    "#### Create a one hot vector representation of the possible authors\n",
    "- create an artificial user that represents a new user in a conversation up to that point\n",
    "- get a matrix with the authors as columns and a 1 if the author wrote the post\n",
    "- join it with the feature matrix\n",
    "- drop the author column\n",
    "\n",
    "\n",
    "#### Training NN to predict the author that would write next\n",
    "- included a \"new author\" category to capture predicting unknown authors\n",
    "- using multi-class classification (instead of multi-label)\n",
    "- relu/sigmoid activation functions have same effect\n",
    "- precision grew significantly when adding more than 3-5 layers\n",
    "\n",
    "#### Predicting the author presence based on prediction probabilities\n",
    "- compute predictions for the whole dataframe\n",
    "- drop features and non-features except conversation and platform\n",
    "- wide to long the authors to make them a index\n",
    "- groupby conversation and platform\n",
    "\n",
    "#### Notes\n",
    "- inserting the new author column increased precision times 10\n",
    "- categorical accuracy and regular accuracy match (which is weird)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def calculate_author_predictions(df):\n",
    "    # compute a fake user that symbolizes that the given user has not been seen at a given stage in the conversation\n",
    "    df_conversation_authors = df[[\"conversation_id\", \"author\", \"current_time\"]]\n",
    "    first_times = df_conversation_authors.groupby([\"conversation_id\", \"author\"]).min()\n",
    "\n",
    "    def is_new_author(row):\n",
    "        earliest_author_post = first_times.loc[row[\"conversation_id\"],row[\"author\"]]\n",
    "        current_post_time = row[\"current_time\"]\n",
    "        return  earliest_author_post >= current_post_time\n",
    "\n",
    "    new_author_column = df[[\"conversation_id\", \"author\", \"current_time\"]].apply(is_new_author, axis=1)\n",
    "    new_author_column= new_author_column.rename(columns={'current_time':\"Author_is_new\"})\n",
    "    new_author_column.value_counts()\n",
    "\n",
    "\n",
    "    def compute_new_author_column(df):\n",
    "        import pandas as pd\n",
    "        author_one_hot = pd.get_dummies(df.author, prefix=\"Author\", sparse=True)\n",
    "        # make author cells 0 that are now represented as \"new author\"\n",
    "        author_one_hot = author_one_hot.astype(bool).apply(lambda x: x & ~new_author_column.Author_is_new).astype(int)\n",
    "        # delete columns that are all 0\n",
    "        author_one_hot = author_one_hot.loc[:, (author_one_hot != 0).any(axis=0)]\n",
    "        # join the new author column to the labels\n",
    "        labels = author_one_hot.join(new_author_column.astype(int))\n",
    "        features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "        combined_set = features.join(labels)\n",
    "        return combined_set, features, labels\n",
    "\n",
    "    combined_set, features, labels = compute_new_author_column(df)\n",
    "\n",
    "    from keras.optimizer_v2.rmsprop import RMSprop  # selecting train and test datasets\n",
    "    train, test = train_test_split(combined_set, test_size=0.2, shuffle=False)\n",
    "    print(\"split training and test set\")\n",
    "\n",
    "    # train the model\n",
    "    y = train.drop(features.columns, axis=1)\n",
    "    x = train.drop(labels.columns, axis=1)\n",
    "    print(\"seperated features and y with shapes:\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "    # print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    output_shape = y.shape[1]\n",
    "    print(\"inputshape is {}\".format(input_shape))\n",
    "    model = Sequential([\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='softmax', input_shape=input_shape)\n",
    "    ])\n",
    "    print(\"defined model as {}\".format(model.layers))\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy', 'accuracy' ,'mae']\n",
    "    )\n",
    "    print(\"compiled model\")\n",
    "    #model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    #model.fit(x, y, epochs=10, shuffle=True)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.drop(features.columns, axis=1)\n",
    "    test_x = test.drop(labels.columns, axis=1)\n",
    "    #test_x = test_x.drop(\"timedelta\", axis=1)\n",
    "\n",
    "    loss, cat_accuracy, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is cat acc {}, reg acc {} and the mae is {}\".format(cat_accuracy, accuracy, mae))\n",
    "\n",
    "    all_features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "    print(\"start generating author predictions for the whole data set\")\n",
    "    predictions = model.predict(all_features, use_multiprocessing=True)\n",
    "    print(\"end generating author predictions for the whole data set\")\n",
    "    column_names = labels.columns\n",
    "    predictions = pd.DataFrame(predictions, columns=column_names)\n",
    "    print(type(predictions))\n",
    "    print(predictions.shape)\n",
    "\n",
    "\n",
    "    all_non_features = df[[\"conversation_id\", \"platform\"]]\n",
    "    print(type(all_non_features))\n",
    "    print(all_non_features.shape)\n",
    "    all_non_features.reset_index(drop=True, inplace=True)\n",
    "    joined_dataframe = all_non_features.join(predictions)\n",
    "    #print(joined_dataframe.Author_is_new.describe()) # no idea why that is the same prediction of all the rows\n",
    "\n",
    "    joined_dataframe = joined_dataframe.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "    author_predictions_existing = joined_dataframe.drop([\"Author_is_new\"], axis=1)\n",
    "    author_predictions_existing.reset_index(level=['platform', 'conversation_id'],inplace=True)\n",
    "    print(\"start converting author hot vectors beack to one author column\")\n",
    "    author_predictions_existing_reshaped = pd.wide_to_long(author_predictions_existing, stubnames=\"Author_\", i=['platform', 'conversation_id'], j=\"author_id\")\n",
    "    print(\"end converting author hot vectors beack to one author column\")    \n",
    "    return author_predictions_existing_reshaped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_PB_reddit = df_PB[df_PB[\"platform\"] == \"reddit\"]\n",
    "prediction_result_reddit = calculate_author_predictions(df_PB_reddit)\n",
    "\n",
    "df_PB_twitter = df_PB[df_PB[\"platform\"] == \"twitter\"]\n",
    "prediction_result_twitter = calculate_author_predictions(df_PB_twitter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prediction_result_reddit\n",
    "# prediction_result_twitter\n",
    "\n",
    "PB_predictions = prediction_result_reddit.append(prediction_result_twitter)\n",
    "PB_predictions = PB_predictions.rename(columns={\"Author_\": \"PB\"})\n",
    "PB_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Author Centrality\n",
    "- The centrality was already computed when creating the dataset as it is based on graph measures primarily\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_centrality_avg = df_centrality.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "df_centrality_avg = df_centrality_avg.assign(centrality=df_centrality_avg.centrality_score / df_centrality_avg.root_distance_avg)\n",
    "df_centrality_avg = df_centrality_avg.drop([\"centrality_score\", \"root_distance_avg\"], axis=1)\n",
    "df_centrality_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combined Analysis\n",
    "- join three author vision measures into one dataframe\n",
    "- add author centrality to the same dataframe\n",
    "- correlate the measures\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_comparison_table = RB_predictions.join(PB_predictions).join(df_centrality_avg).join(baseline_predictions)\n",
    "\n",
    "prediction_comparison_table = prediction_comparison_table.groupby([\"platform\", \"conversation_id\"]).mean().join(repetition_predictions)\n",
    "prediction_comparison_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "corr_matrix = prediction_comparison_table.corr(method=\"pearson\")\n",
    "sn.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Data preperation\n",
    "- Delete rows that are neither twitter or reddit data\n",
    "- normalize time deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# filtering data that is not twitter or reddit\n",
    "def delete_not_twitter_not_reddit(df):\n",
    "    platform = df.platform\n",
    "    to_delete_rows = platform.apply(lambda x: is_not_reddit_or_twitter(x))\n",
    "    df = df.drop(df[to_delete_rows].index)\n",
    "    return df\n",
    "\n",
    "df_RB = delete_not_twitter_not_reddit(df_RB)\n",
    "df_PB = delete_not_twitter_not_reddit(df_PB)\n",
    "df_centrality = delete_not_twitter_not_reddit(df_centrality)\n",
    "\n",
    "df_RB = equalize_samples(df_RB)\n",
    "df_PB = df_PB[df_PB.conversation_id.isin(df_RB.conversation_id)]\n",
    "df_centrality = df_centrality[df_centrality.conversation_id.isin(df_RB.conversation_id)]\n",
    "\n",
    "df_RB = normalize_timedelta(df_RB)\n",
    "df_PB = normalize_timedelta(df_PB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline for Author vision\n",
    "- uses selected values as a distance measure\n",
    "- probability of having seen a tweet is reduced by a half with each step in the reply hierachy\n",
    "- probability of having seen a tweet is reduced by a quarter for each step away from the root\n",
    "- probabiliy of having seen a tweet is increased for each path in the follower network to the tweet (forthcoming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# in order to allow the comparison, the filter from the other notebook needs to run and the predictions, too\n",
    "df_baseline = df_RB[df_RB[\"conversation_id\"].isin(common_conversation_ids)]\n",
    "\n",
    "reply_filter_col = [col for col in df_baseline if col.startswith('reply')]\n",
    "root_distance_filter_col = [col for col in df_baseline if col.startswith('root')]\n",
    "reply_columns = df_baseline[reply_filter_col]\n",
    "root_distance_columns = df_baseline[root_distance_filter_col]\n",
    "reply_cs = reply_columns.sum(axis=1)\n",
    "root_distance_cs = root_distance_columns.sum(axis=1)\n",
    "rcs_not_null = [i for i in reply_cs.tolist() if i != 0]\n",
    "root_reply_combined = (root_distance_cs + reply_cs)\n",
    "root_reply_combined = (root_reply_combined - root_reply_combined.min()) / (\n",
    "        root_reply_combined.max() - root_reply_combined.min())\n",
    "combined = [i for i in root_reply_combined.tolist() if i != 0]\n",
    "df_baseline = df_baseline.assign(root_reply_combined=root_reply_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_baseline_with_authors = df_baseline[[\"root_reply_combined\", \"conversation_id\", \"author\", \"platform\"]]\n",
    "# df_reshaped = pd.pivot_table(df_data,index=[\"conversation_id\", \"current\"], columns=[\"root_reply_combined\"],aggfunc = np.mean)\n",
    "baseline_gpm = df_baseline_with_authors.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "baseline_gpm_conversation = baseline_gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "baseline_gpm = baseline_gpm_conversation.groupby(by=[\"platform\"]).mean()\n",
    "baseline_gpm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition Probabilities\n",
    "\n",
    "#### Analyzing the probability of an author writing repeatedly in the same conversation\n",
    "1. sum up the amounts y == 1 (because an author has answered himself)\n",
    "2. sum chances of an author seeing himself write\n",
    "3. calculate a measure of how likely it is that an author sees himself repeated as a test for the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "author_count_columns = [\"current\", \"conversation_id\", \"platform\", \"y\"]\n",
    "author_df = df_RB[author_count_columns]\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\", \"current\"]).sum()\n",
    "author_df = author_df.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "distinct_view_columns = [\"current\", \"conversation_id\", \"platform\"]\n",
    "distinct_view_df = df_RB[distinct_view_columns]\n",
    "distinct_views = distinct_view_df.groupby([\"current\", \"conversation_id\", \"platform\"]).size().to_frame('size')\n",
    "distinct_views = distinct_views.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "joined_author_stats = author_df.join(distinct_views)\n",
    "joined_author_stats = joined_author_stats[\"y\"] / joined_author_stats[\"size\"]\n",
    "joined_author_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joined author stats show the repetition probabilities for each of the platforms per conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repetition_probability = joined_author_stats.groupby(\"platform\").mean().to_frame(\"repetition_probs\")\n",
    "repetition_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Response Based Author vision Algorithm (RB)\n",
    "\n",
    "The features are the distance of the author to any tweet in the conversation\n",
    "indicated by the following structures:\n",
    "- subtree to viewed tweet from a tweet the author wrote\n",
    "- root closeness of viewed tweet\n",
    "- time delta to viewed tweet from tweets the author wrote\n",
    "\n",
    "#### Loading the data from the pickled version\n",
    "1. importing libraries\n",
    "2. checking gpu support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "print(\"cuda gpu is available: {}\".format(is_cuda_gpu_available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df = df[df[\"root_distance_0\"] == 0]\n",
    "# analyze the distribution of reached targets for the sample\n",
    "print(\"reddit:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"reddit\"].y.value_counts())\n",
    "print(\"twitter:\")\n",
    "print(df_RB[df_RB[\"platform\"] == \"twitter\"].y.value_counts())\n",
    "# this should be higher for reddit as the unique author / posting ratio is lower for reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing a nn model\n",
    "1. separate features\n",
    "2. train models for reddit and twitter\n",
    "3. inspect models for reddit and twitter\n",
    "4. predict the likelihood based on the author has seen a posting\n",
    "5. aggregate likelihoods in order to compute author vision measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training functions\n",
    "def train_model(df):\n",
    "    # dropping non-reddit non-twitter data\n",
    "    df = take_features(df)\n",
    "\n",
    "    # selecting train and test datasets\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train.describe()\n",
    "\n",
    "    # train the model\n",
    "    y = train.y\n",
    "    x = train.drop(\"y\", axis=1)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "\n",
    "    print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    model = Sequential([\n",
    "        Dense(1, activation='sigmoid', input_shape=input_shape)\n",
    "    ])\n",
    "\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'mae']\n",
    "    )\n",
    "\n",
    "    # model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.y\n",
    "    test_x = test.drop(\"y\", axis=1)\n",
    "\n",
    "    loss, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is {} and the mae is {}\".format(accuracy, mae))\n",
    "\n",
    "    return x, y, test_x, test_y, model\n",
    "\n",
    "\n",
    "def inspect_model(x, y, test_x, test_y, model):\n",
    "    # have a look at some prediction\n",
    "    reply_distance_2 = test_x[test_x[\"reply_distance_2\"] == 1]\n",
    "    first_rows = reply_distance_2.head(2)\n",
    "    print(first_rows)\n",
    "    model.predict(first_rows)\n",
    "\n",
    "    # let's have a look at the weights and biases of the hidden layer\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]\n",
    "    first_layer_biases = model.layers[0].get_weights()[1]\n",
    "    # print(first_layer_weights)\n",
    "    column_names = x.columns.values\n",
    "    for i in range(len(column_names[:5])):\n",
    "        print(\"feature {} has weight {} \\n\".format(column_names[i], first_layer_weights[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# have a look for reddit\n",
    "tw_df = df_RB[df_RB[\"platform\"] == \"twitter\"]\n",
    "tw_x, tw_y, tw_test_x, tw_test_y, tw_model = train_model(tw_df)\n",
    "\n",
    "rd_df = df_RB[df_RB[\"platform\"] == \"reddit\"]\n",
    "rd_x, rd_y, rd_test_x, rd_test_y, rd_model = train_model(rd_df)\n",
    "\n",
    "# inspect_model(tw_x, tw_y, tw_test_x, tw_test_y, tw_model)\n",
    "# inspect_model(rd_x, rd_y, rd_test_x, rd_test_y, rd_model)\n",
    "\n",
    "tw_non_features = take_non_features(tw_df)\n",
    "rd_non_features = take_non_features(rd_df)\n",
    "\n",
    "tw_features_y = take_features(tw_df)\n",
    "tw_features = tw_features_y.drop(\"y\", axis=1)\n",
    "rd_features_y = take_features(rd_df)\n",
    "rd_features = rd_features_y.drop(\"y\", axis=1)\n",
    "rd_predictions = rd_model.predict(rd_features)\n",
    "tw_predictions = tw_model.predict(tw_features)\n",
    "\n",
    "tw_vision = tw_non_features.assign(predictions=tw_predictions)\n",
    "rd_vision = rd_non_features.assign(predictions=rd_predictions)\n",
    "\n",
    "combined_vision = tw_vision.append(rd_vision)\n",
    "not_needed_list = [\"beam_node_author\", \"beam_node\", \"has_followed_path\", \"has_follow_path\"]\n",
    "combined_vision = combined_vision.drop(not_needed_list, axis=1)\n",
    "combined_vision_with_author = combined_vision\n",
    "combined_vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "combined_vision_with_author2 = combined_vision_with_author.groupby(\n",
    "    [\"platform\", \"conversation_id\", \"author\", \"predictions\"]).count()\n",
    "combined_vision_with_author2 = combined_vision_with_author2.reset_index()\n",
    "combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).sum()\n",
    "combined_vision_with_author2[\n",
    "    \"avg_predictions\"] = combined_vision_with_author2.predictions / combined_vision_with_author2.current\n",
    "combined_vision_with_author2 = combined_vision_with_author2.drop([\"current\", \"predictions\"], axis=1)\n",
    "combined_vision_with_author2 = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\", \"author\"]).mean()\n",
    "combined_vision_with_author2 = combined_vision_with_author2.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "combined_vision_with_author2 = combined_vision_with_author2.groupby([\"platform\"]).mean()\n",
    "rb_result = combined_vision_with_author2\n",
    "rb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "combined_vision = combined_vision.drop(\"author\", axis=1)\n",
    "gpm = combined_vision.groupby([\"platform\", \"conversation_id\", \"current\"]).mean()\n",
    "gpm_per_conversation = gpm.groupby(by=[\"platform\", \"conversation_id\"]).mean()\n",
    "gpm_per_platform = gpm.groupby(by=[\"platform\"]).mean()\n",
    "gpm_per_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = repetition_probability.join(gpm_per_platform)\n",
    "probabilities.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation the correlation between probabilities and the RB-predictions\n",
    "- This means that the neural network computes a linear function of the repetition probabilities based on the computation of the y functions\n",
    "- The probabilities are very low for both reddit and twitter but in a comparable area\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Prediction\n",
    "\n",
    "It is possible to predict an author or \"new author\" at same time by defining categories as 1 if a author is to be predicted but\n",
    "only if it is not a new author. Because of memory, only twitter or reddit data can be predicted in one run.\n",
    "The full dataset does not fit in laptops memory and is computed on the cluster (which in turn has no gpu support)\n",
    "\n",
    "The probability of predicting an author is calculated for each relationship (root distance to another node, reply distance to other nodes, and reply distance to nodes with the same author. In future also the author follower network will be included in the feature set.\n",
    "\n",
    "The overall sum of the probability of predicting an author (in average) will be interpreted as the likelihood of any author writing in any time in the conversation (again, because it is not a new author). This will then seen as the author being present in the conversation because it is another measure of a author being available in all branches and positions in the conversation.\n",
    "\n",
    "#### Create a one hot vector representation of the possible authors\n",
    "- create an artificial user that represents a new user in a conversation up to that point\n",
    "- get a matrix with the authors as columns and a 1 if the author wrote the post\n",
    "- join it with the feature matrix\n",
    "- drop the author column\n",
    "\n",
    "\n",
    "#### Training NN to predict the author that would write next\n",
    "- included a \"new author\" category to capture predicting unknown authors\n",
    "- using multi-class classification (instead of multi-label)\n",
    "- relu/sigmoid activation functions have same effect\n",
    "- precision grew significantly when adding more than 3-5 layers\n",
    "\n",
    "#### Predicting the author presence based on prediction probabilities\n",
    "- compute predictions for the whole dataframe\n",
    "- drop features and non-features except conversation and platform\n",
    "- wide to long the authors to make them a index\n",
    "- groupby conversation and platform\n",
    "\n",
    "#### Notes\n",
    "- inserting the new author column increased precision times 10\n",
    "- categorical accuracy and regular accuracy match (which is weird)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def calculate_author_predictions(df):\n",
    "    # compute a fake user that symbolizes that the given user has not been seen at a given stage in the conversation\n",
    "    df_conversation_authors = df[[\"conversation_id\", \"author\", \"current_time\"]]\n",
    "    first_times = df_conversation_authors.groupby([\"conversation_id\", \"author\"]).min()\n",
    "\n",
    "    def is_new_author(row):\n",
    "        earliest_author_post = first_times.loc[row[\"conversation_id\"],row[\"author\"]]\n",
    "        current_post_time = row[\"current_time\"]\n",
    "        return  earliest_author_post >= current_post_time\n",
    "\n",
    "    new_author_column = df[[\"conversation_id\", \"author\", \"current_time\"]].apply(is_new_author, axis=1)\n",
    "    new_author_column= new_author_column.rename(columns={'current_time':\"Author_is_new\"})\n",
    "    new_author_column.value_counts()\n",
    "\n",
    "\n",
    "    def compute_new_author_column(df):\n",
    "        import pandas as pd\n",
    "        author_one_hot = pd.get_dummies(df.author, prefix=\"Author\", sparse=True)\n",
    "        # make author cells 0 that are now represented as \"new author\"\n",
    "        author_one_hot = author_one_hot.astype(bool).apply(lambda x: x & ~new_author_column.Author_is_new).astype(int)\n",
    "        # delete columns that are all 0\n",
    "        author_one_hot = author_one_hot.loc[:, (author_one_hot != 0).any(axis=0)]\n",
    "        # join the new author column to the labels\n",
    "        labels = author_one_hot.join(new_author_column.astype(int))\n",
    "        features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "        combined_set = features.join(labels)\n",
    "        return combined_set, features, labels\n",
    "\n",
    "    combined_set, features, labels = compute_new_author_column(df)\n",
    "\n",
    "    from keras.optimizer_v2.rmsprop import RMSprop  # selecting train and test datasets\n",
    "    train, test = train_test_split(combined_set, test_size=0.2, shuffle=False)\n",
    "    print(\"split training and test set\")\n",
    "\n",
    "    # train the model\n",
    "    y = train.drop(features.columns, axis=1)\n",
    "    x = train.drop(labels.columns, axis=1)\n",
    "    print(\"seperated features and y with shapes:\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # import tensorflow and train the model\n",
    "    # print(tf.__version__)\n",
    "    input_shape = (x.shape[1],)\n",
    "    output_shape = y.shape[1]\n",
    "    print(\"inputshape is {}\".format(input_shape))\n",
    "    model = Sequential([\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='softmax', input_shape=input_shape)\n",
    "    ])\n",
    "    print(\"defined model as {}\".format(model.layers))\n",
    "    # stochastic gradient descend as a classifier seem appropriate\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy', 'accuracy' ,'mae']\n",
    "    )\n",
    "    print(\"compiled model\")\n",
    "    #model.fit(x, y, epochs=3)\n",
    "    model.fit(x, y)\n",
    "    #model.fit(x, y, epochs=10, shuffle=True)\n",
    "    # evaluate the model on the test set\n",
    "    test_y = test.drop(features.columns, axis=1)\n",
    "    test_x = test.drop(labels.columns, axis=1)\n",
    "    #test_x = test_x.drop(\"timedelta\", axis=1)\n",
    "\n",
    "    loss, cat_accuracy, accuracy, mae = model.evaluate(test_x, test_y)\n",
    "    print(\"the accuracy on the training set is cat acc {}, reg acc {} and the mae is {}\".format(cat_accuracy, accuracy, mae))\n",
    "\n",
    "    all_features = take_features(df, [\"current_time\", \"beam_node_time\"])\n",
    "    print(\"start generating author predictions for the whole data set\")\n",
    "    predictions = model.predict(all_features, use_multiprocessing=True)\n",
    "    print(\"end generating author predictions for the whole data set\")\n",
    "    column_names = labels.columns\n",
    "    predictions = pd.DataFrame(predictions, columns=column_names)\n",
    "    print(type(predictions))\n",
    "    print(predictions.shape)\n",
    "\n",
    "\n",
    "    all_non_features = df[[\"conversation_id\", \"platform\"]]\n",
    "    print(type(all_non_features))\n",
    "    print(all_non_features.shape)\n",
    "    all_non_features.reset_index(drop=True, inplace=True)\n",
    "    joined_dataframe = all_non_features.join(predictions)\n",
    "    #print(joined_dataframe.Author_is_new.describe()) # no idea why that is the same prediction of all the rows\n",
    "\n",
    "    joined_dataframe = joined_dataframe.groupby([\"platform\", \"conversation_id\"]).mean()\n",
    "    author_predictions_existing = joined_dataframe.drop([\"Author_is_new\"], axis=1)\n",
    "    author_predictions_existing.reset_index(level=['platform', 'conversation_id'],inplace=True)\n",
    "    print(\"start converting author hot vectors beack to one author column\")\n",
    "    author_predictions_existing_reshaped = pd.wide_to_long(author_predictions_existing, stubnames=\"Author_\", i=['platform', 'conversation_id'], j=\"author_id\")\n",
    "    print(\"end converting author hot vectors beack to one author column\")    \n",
    "    return joined_dataframe, author_predictions_existing_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_PB_reddit = df_PB[df_PB[\"platform\"] == \"reddit\"]\n",
    "prediction_result_reddit, prediction_result_reddit_existing = calculate_author_predictions(df_PB_reddit)\n",
    "\n",
    "df_PB_twitter = df_PB[df_PB[\"platform\"] == \"twitter\"]\n",
    "prediction_result_twitter, prediction_result_twitter_existing = calculate_author_predictions(df_PB_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}